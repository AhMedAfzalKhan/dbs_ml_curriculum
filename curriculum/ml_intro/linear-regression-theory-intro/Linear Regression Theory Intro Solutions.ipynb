{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "! wget -O rise.css -nc https://soph.info/metis/nb.css  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-16T23:58:20.802189Z",
     "start_time": "2018-11-16T23:58:20.783524Z"
    },
    "keep_output": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"https://soph.info/metis/nb.css\" type=\"text/css\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"\"\"<link rel=\"stylesheet\" href=\"https://soph.info/metis/nb.css\" type=\"text/css\"/>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Regression Theory Lab\n",
    "\n",
    "In this lab, we cover the basics of linear regression, with examples of Python implementation in [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and [Statsmodels](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on the [Anscombe dataset](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) and [FRED House Price Index](https://fred.stlouisfed.org/series/USSTHPI) data. This is a primer for the `linear-regression-code-intro` module."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives:\n",
    "Students will be able to:\n",
    "1. Apply Linear Regression to the framework taught in the Machine Learning intro lesson.\n",
    "2. Interpret the coefficients of a linear regression model.\n",
    "3. Fit linear regression models using either scikit-learn or statsmodels.\n",
    "4. Predict out-of-sample using a model.\n",
    "5. Recognize and explain limits of models:\n",
    "  * Models do not determine causation.\n",
    "  * Models cannot substitute for understanding data."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import sklearn\n",
    "\n",
    "sns.set_style(\"whitegrid\", {\"font.family\": [\"serif\"]})\n",
    "sns.set_context(\"talk\") # talk context is good for RISE presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load anscombe data\n",
    "anscombe = pd.read_csv('./data/anscombe.csv')\n",
    "anscombe.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The anscombe data includes four datasets that are useful for instruction. Right now, we'll focus on dataset I."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot with matplotlib\n",
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", fit_reg=False,\n",
    "           ci=None, data=anscombe, height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    x=\"x\", y=\"y\", ci=None, fit_reg=False, col=\"dataset\", data=anscombe.query(\"dataset=='I'\"), height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We note a pattern - a correlation - between `x` and `y` that can be used to build a linear model. As x increases, y tends to increase by a fixed amnount. In its simplest sense, a linear model is a straight line relationship between a:\n",
    " - feature and target\n",
    " - independent variable and dependent variable\n",
    " - predictor and response"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All these terms are interchangable and come from different sub-domains of science, but data science tends to use _feature/target_ naming."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using seaborn, we can quickly plot our data along with a regression model."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot with visualized linear model\n",
    "sns.lmplot(x=\"x\", y=\"y\", ci=None, col=\"dataset\", data=anscombe.query(\"dataset=='I'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We see the line crosses (6,6) and (10,8), giving us a slope of 2/4: there is a change in `y` of .5 for every change of 1 in `x`. We could write the equation for this line as $y = .5x + 3$. Is this line a good fit for our data?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Okay, that's what our finished product looks like. But how does it work? How does something find the best line for a given dataset?\n",
    "\n",
    "First some math terms we'll use: \n",
    "- $x$: input features of data\n",
    "- $y$: target value of data\n",
    "- $\\hat{y}$: predicted output from the model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/e/ed/Residuals_for_Linear_Regression_Fit.png)\n",
    "\n",
    "We'll start by quantifying how well a line fits a dataset. On the figure, the dots are the points in our dataset and the red line is what we want to evaluate. \n",
    "\n",
    "The distance from each point to the line is called the error. These errors are also called residuals: they are the difference between observed $y$ and predicted $\\widehat{y}$."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/e/ed/Residuals_for_Linear_Regression_Fit.png)\n",
    "\n",
    "Ordinary Least Squares is an approach to fitting a model/line that seeks to minimize the sum of squared errors. \n",
    "\n",
    "That means OLS is taking each error, squaring it, and then trying to minimize that length. One consenquence is that the bottom and top points in the figure contribute greatly to our model's error and have strong influence. Outliers are especially dangerous!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can define this mathematically too.\n",
    "\n",
    "OLS Cost Function:\n",
    "\n",
    "$$\\frac{1}{2m} \\sum _{i=1}^m \\left(\\hat{Y^{(i)}}-Y^{(i)}\\right)^2$$"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note: one key reason to use squared distances is that it creates a convex optimization problem, which has only one solution and is very fast to solve!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling with sklearn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start by training a model and then we'll talk about what it does."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import model and fit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression  \n",
    "\n",
    "# grab the x column and y column\n",
    "\n",
    "X1 = anscombe.query(\"dataset=='I'\").loc[:,'x'].values.reshape(-1,1)\n",
    "y1 = anscombe.query(\"dataset=='I'\").loc[:,'y']\n",
    "\n",
    "linreg_model = LinearRegression()\n",
    "linreg_model.fit(X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There! That's all there is to training a model in sci-kit learn."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can use the stored model object (`linreg_model`) to find coefficients."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# show coefficients\n",
    "linreg_model.coef_\n",
    "\n",
    "# this should match our 2/4 spot-check on the slope!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And the intercept."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get y-intercept\n",
    "linreg_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we can make predictions on new data. Let's try $x=0$"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use our fitted model to predict y when x=0\n",
    "\n",
    "linreg_model.predict([[0]])\n",
    "\n",
    "# yes, we get our intercept!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In Linear Regression, the model learns a set of parameters, $\\beta$. $\\beta$ is a **vector** of parameters, which includes $\\beta_0$, $\\beta_1$, $\\beta_2$, etc."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These are combined using the formula\n",
    "$$\\hat{y} = \\beta^Tx + \\epsilon.$$ \n",
    "We can expand the matrix multiplication to get\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\beta_4x_4 ... + \\epsilon.$$"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, $x_1$, $x_2$, etc. are the features or columns in our data.\n",
    "\n",
    "$\\beta_0$ is our intercept, and the other betas correspond, left to right, to our features. We only have one in this case, but next we'll look at a dataset with more features!\n",
    "\n",
    "$\\epsilon$ stands for the error. Our model starts by assuming that it can't be perfectly right and that each point of data might have some error. This error is assumed to be generated from a Gaussian (aka Normal) distribution independently for each point."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling Housing Data with Statsmodels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You've just seen how to use `sklearn` to fit a linear regression model. Now let's look at a different tool, `statsmodels`. You'll see more on this later but for now, both methods are equally good. \n",
    "- `sklearn` is designed more for machine learning.\n",
    "- `statsmodels` is designed more for statisticians. "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# housing price index by date\n",
    "housing_price_index = pd.read_csv('data/monthly-hpi.csv')\n",
    "\n",
    "# inflation, S&P500 and interest rates by date\n",
    "shiller = pd.read_csv('data/shiller.csv')\n",
    "\n",
    "# GDP by date\n",
    "gross_domestic_product = pd.read_csv('data/gdp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our data comes in different time blocks, so we can `pandas` merge on date to match up our data."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# merge to single dataframe\n",
    "housing_df = shiller.merge(gross_domestic_product, on='date').merge(housing_price_index, on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# scope out our data\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# describe it\n",
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Per the [FHFA site](https://www.fhfa.gov/DataTools/Downloads/Pages/House-Price-Index.aspx), House Price Index (HPI) is a broad measure of the movement of single-family house prices. The HPI is a weighted, repeat-sales index, meaning that it measures average price changes in repeat sales or refinancings on the same properties. This information is obtained by reviewing repeat mortgage transactions on single-family properties whose mortgages have been purchased or securitized by Fannie Mae or Freddie Mac since January 1975.\n",
    "\n",
    ">The prices are normalized to 1980:Q1 = 100."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# slice data into features and target\n",
    "X = housing_df.drop(columns=[\"housing_price_index\", \"date\"]).astype(float)\n",
    "y = housing_df.loc[:,\"housing_price_index\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit model with target as housing price index\n",
    "housing_model = sm.OLS(y, X, data=housing_df)\n",
    "\n",
    "results = housing_model.fit()\n",
    "\n",
    "# summarize our model\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/results.png)\n",
    "\n",
    "\n",
    "There's a lot going on here! We'll break the summary table down in a future lesson.\n",
    "\n",
    "For now, let's find the coefficients in the second table:\n",
    "\n",
    "- `sp500` has a coefficient of 0.0266\n",
    "- This means when the S&P 500 goes up by one dollar, the HPI goes up by 0.0266"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Does this mean the stock market **causes** an increase in housing prices??"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- NOT necessarily! There could be something we aren't measuring affecting both S&P and HPI, causality could be going the other direction, or it could be a much more complicated system."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/results.png)\n",
    "\n",
    "Are there any coefficients that surprise you?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Possibilities:\n",
    "- Interest rates: Higher interest rates should make it more difficult to purchase a house, but interest rates are positively related to HPI.\n",
    "- Labor force participation, GDP, Price index: all of these would seem to increase HPI but are negatively related."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Are Models Everything We Need?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# describe first set\n",
    "anscombe.query(\"dataset=='I'\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# STUDENTS: describe second, third, and fourth sets\n",
    "\n",
    "## What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# the summary stats are all the same!!! Spooky!\n",
    "anscombe.query(\"dataset=='II'\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Incredibly, the mean, variance (deviation squared), and other key metrics are identical! This is the famous Anscombe dataset, created to show that it takes more than a quick summary to understand our data well."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# STUDENTS: use statsmodels or sklearn to fit linear models to these datasets and compare\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "x = anscombe.query(\"dataset=='II'\").loc[:,\"x\"]\n",
    "y = anscombe.query(\"dataset=='II'\").loc[:,\"y\"]\n",
    "\n",
    "# fit model with target as housing price index\n",
    "housing_model = sm.OLS(y, x)\n",
    "\n",
    "results = housing_model.fit()\n",
    "\n",
    "# summarize our model\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=data/anscombe.jpg style=\"float:right; max-width:50%\">\n",
    "\n",
    "Even more incredibly, the models that best fit our data according to OLS is the same for all our datasets!\n",
    "\n",
    "We note in this Anscombe exercise that statistics aren't enough for us to understand our data. Even models don't give us full insight; we must combine these approaches with visualization of our data!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With the help of seaborn we can quickly plot each dataset along with its linear regression fit to confirm that these wildly different datasets do infact lead to the same predictions."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", col_wrap=2, data=anscombe, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img border=\"0\" src=\"https://blog.revolutionanalytics.com/downloads/DataSaurus%20Dozen.gif\" title=\"DataSaurus Dozen\" width=\"500\">\n",
    "\n",
    "Another even more extreme example of this: the [Dinosaur Dozen](https://blog.revolutionanalytics.com/2017/05/the-datasaurus-dozen.html). 12 plots including a T-Rex that have the same mean, std, and correlation.\n",
    "\n",
    "What lesson can we learn from the Anscome Dataset and the Dinosaur Dozen?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Models are very useful tools but they can never be a substitute for understanding (and at minimum, **plotting**) your data."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
