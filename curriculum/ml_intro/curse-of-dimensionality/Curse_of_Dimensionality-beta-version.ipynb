{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis in high dimensions:\n",
    "# The Curse of Dimensionality\n",
    "\n",
    "### In high-dimensional space, points tend to be far apart.\n",
    "\n",
    "This impacts data analysis. Intuitively, clustering is difficult when points are scattered. This notebook will show why high-dimensional space is like this.\n",
    "\n",
    "Think about the unit cube in $n$-dimensions. This is hard to visualize for $n>3$. The point of this code is to try to say something about this crazy, hard to visualize high-dimensional space thing. Algebraically, this is not too difficult to think about. It's just all the $n$-tuples of numbers between 0 and 1.\n",
    "\n",
    "#### How big is the unit cube?\n",
    "\n",
    "What is the length of the main-diagonal of the $n$-cube? In 2 dimensions, it's $\\sqrt 2$ and in 3 dimensions it's $\\sqrt 3$. In 100-dimensional space, the main diagonal is 10 units long. In 1M-dimensional space, the main diagonal is 1,000 units long. The 1M-dimensional unit cube has points that are pretty far apart from one another.\n",
    "\n",
    "#### What percentage of the cube is in the ball?\n",
    "\n",
    "Consider an n-dimensional ball inscribed in an n-dimensional cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def make_circle():\n",
    "    fig = plt.gcf()\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    fig.gca().add_artist(plt.Circle((0,0),1,alpha=.5))\n",
    "    ax.scatter(0,0,s=10,color=\"black\")\n",
    "    ax.plot(np.linspace(0,1,100),np.zeros(100),color=\"black\")\n",
    "    ax.text(.4,.1,\"r\",size=48)\n",
    "    ax.set_xlim(left=-1,right=1)\n",
    "    ax.set_ylim(bottom=-1,top=1)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "make_circle()\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import product, combinations\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "#draw cube\n",
    "r = [-1, 1]\n",
    "for s, e in combinations(np.array(list(product(r,r,r))), 2):\n",
    "    if np.sum(np.abs(s-e)) == r[1]-r[0]:\n",
    "        ax.plot3D(*zip(s,e))\n",
    "\n",
    "#draw sphere\n",
    "u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n",
    "x=np.cos(u)*np.sin(v)\n",
    "y=np.sin(u)*np.sin(v)\n",
    "z=np.cos(v)\n",
    "ax.plot_wireframe(x, y, z, color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2 dimensions: $\\frac {\\pi r^2} {(2r)^2} = \\frac \\pi 4$ ~ 79%\n",
    "\n",
    "In 3 dimensions: $\\frac {\\frac 4 3 \\pi r^3} {(2r)^3} = \\frac \\pi 6$ ~ 52%\n",
    "\n",
    "What is the trend?\n",
    "\n",
    "We can use a random variable drawn from a uniform distribution to estimate the percentage for $n$ dimensions.\n",
    "\n",
    "Strategy: draw a bunch of random points in the cube (each dimension value between 0 and 1) and calculate the percentage within 1 unit from the origin. \n",
    "\n",
    "Run this experiment 100 times to estimate the error in the estimate.\n",
    "\n",
    ">Note: this is an example of a Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to draw a point?\n",
    "print(\"random number:\", np.random.sample())\n",
    "\n",
    "# how to draw 5 points in 2 dimensions?\n",
    "sample_data = np.random.sample((5,2))\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how far are these points from origin?\n",
    "#print \"distances: \",(sample_data**2).sum(1)\n",
    "def norm(x): \n",
    "    # input: a bunch of points -- one vector per row\n",
    "    # output: the distance from the origin to each point\n",
    "    return np.sqrt( (x**2).sum(1) )\n",
    "def in_the_ball(x): \n",
    "    # input: a bunch of points -- one vector per row\n",
    "    # output: a boolean array -- if the point is in the ball\n",
    "    return norm(x)<1\n",
    "\n",
    "print(norm(sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Monte Carlo with increasing dimensions..  \n",
    "def what_percent_of_the_ncube_is_in_the_nball(d_dim,\n",
    "                                              sample_size=10**4):\n",
    "    shape = sample_size,d_dim\n",
    "    data = np.array([in_the_ball(np.random.sample(shape)).mean()\n",
    "                     for iteration in range(100)])\n",
    "    return data.mean(), data.min(), data.max()\n",
    "\n",
    "dims = range(2,15)\n",
    "data = np.array(list(map(what_percent_of_the_ncube_is_in_the_nball,dims)))\n",
    "\n",
    "\n",
    "what_percent_of_the_ncube_is_in_the_nball(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "\n",
    "# NOTE percentage drop for higher dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(dims, data[:,1], data[:,2], facecolor='blue',alpha=.5)\n",
    "plt.xlabel(\"number of dimensions\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.title(\"What percentage of the cube is in the ball?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that most of the cube is not in the ball -- in high enough dimensions almost none of it is.\n",
    "\n",
    "Why does this matter? If we were to try to cluster points in high-dimensional space or estimate density -- the more dimensions, the more difficult.\n",
    "\n",
    "On average, points are much further apart in high dimensions.\n",
    "\n",
    "Consider a bunch of points uniformly distributed in a $n$-dimensional unit cube. Suppose we'd like to obtain a kNN estimator for the center of the cube.\n",
    "\n",
    "#### Let R be the distance from the center to the closest data point. How does R change with increasing dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_distance(dimension, sample_size=10**3):\n",
    "    points = np.random.sample((sample_size,dimension))-.5   # centering our data.. \n",
    "    # returning the closest point. . \n",
    "    return np.min(norm(points))\n",
    "\n",
    "def estimate_closest(dimension):\n",
    "    data = np.array([get_min_distance(dimension) for _ in range(100)])\n",
    "    return data.mean(), data.min(), data.max()\n",
    "\n",
    "dims = range(2,100)\n",
    "min_distance_data = np.array(list(map(estimate_closest,dims)))\n",
    "\n",
    "# test on a dimension.. \n",
    "print(estimate_closest(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dims,min_distance_data[:,0], color='red')\n",
    "plt.fill_between(dims, min_distance_data[:,1], min_distance_data[:,2],alpha=.5)\n",
    "plt.xlabel(\"N dimensions\")\n",
    "plt.ylabel(\"Estimated min distance from origin\")\n",
    "plt.title(\"How far away are the points from the origin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does the main-diagonal of the cube get big, but most points are scattered about far away from the center. With 1,000 random points, the closest point is about 2 units away in 80 dimensions. In 2 and 3 dimensions -- there are always points within .1 from the origin.\n",
    "\n",
    "What if we wanted to increase our sample so that points were as dense in high-dimensions as they are in lower-dimensions? We'd have to increase our sample size exponentially in the number of dimensions. That's a lot. It means that most of the time, with very high dimensional data sets the data is often very sparsely distributed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
