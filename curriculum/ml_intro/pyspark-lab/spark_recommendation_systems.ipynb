{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems with PySpark\n",
    "\n",
    "Inspired by this common Spark [tutorial](https://github.com/jadianes/spark-movie-lens/blob/master/notebooks/building-recommender.ipynb) but updated to work with dataframes.\n",
    "\n",
    "We will be using MovieLens data to create a recommender system with PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, os, sys, zipfile, io\n",
    "\n",
    "# pyspark imports\n",
    "import pyspark\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_formats = ['retina']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Spark server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data\n",
    "\n",
    "We begin by downloading the smaller movie ratings data set and writing it to a directory. If the larger dataset is desired, use the alternative value for `filepath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'ml-latest-small'  # the smaller dataset\n",
    "# filepath = 'ml-latest'      # the full dataset\n",
    "\n",
    "url = 'http://files.grouplens.org/datasets/movielens/{}.zip'.format(filepath)\n",
    "\n",
    "filename = 'movies.csv'\n",
    "fullpath = os.sep.join([filepath, filename])\n",
    "\n",
    "if not os.path.exists(fullpath):    \n",
    "    sess = requests.session()\n",
    "    resp = sess.get(url)\n",
    "    zipdata = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    zipdata.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the files from the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first lines of included files\n",
    "! head -n 3 {filepath}/movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 3 {filepath}/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 3 {filepath}/links.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 3 {filepath}/tags.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the ratings\n",
    "\n",
    "We begin by loading the ratings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_path = os.sep.join([filepath, 'ratings.csv'])\n",
    "ratings = spark.read.csv(ratings_path,\n",
    "                        sep=',',\n",
    "                        inferSchema=True,\n",
    "                        header=True)\n",
    "\n",
    "for column in ['userId','movieId','timestamp']:\n",
    "    ratings = ratings.withColumn(column, ratings[column].cast('int'))\n",
    "    \n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select a few rows of data and convert to a Pandas dataframe\n",
    "def preview(df, n=3):\n",
    "    return pd.DataFrame(df.take(n), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "Let's split the data into train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ratings.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recommender\n",
    "\n",
    "Let's build our first reocmmendation system using PySpark's alternative least squares (ALS) matrix factorization. We will begin by using rank 4 matrices in the factorization. The rank (as well as many other factors), can be updated later using a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# create the model\n",
    "als = ALS(userCol='userId',\n",
    "          itemCol='movieId',\n",
    "          ratingCol='rating',\n",
    "          rank=4,\n",
    "          seed=42)\n",
    "\n",
    "# fit the model\n",
    "als_model = als.fit(train)\n",
    "\n",
    "# make predictions on the test set\n",
    "als_pred = als_model.transform(test)\n",
    "\n",
    "preview(als_pred[['userId', 'movieId', 'rating', 'prediction']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the error metric on this model. Since rating is a continuous value, it probably makes sense to use something like RMSE.\n",
    "\n",
    "Let's begin by calculating the difference between the rating and prediction and then squaring it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_pred = als_pred.withColumn('diff_sq', (als_pred['rating'] - als_pred['prediction'])**2)\n",
    "\n",
    "preview(als_pred[['rating','prediction','diff_sq']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the RMSE over the entire data set using functions found in PySpark's SQL context (`pyspark.sql.functions`). In general, PySpark's functions **DO NOT** ignore NaN's, so we will have to drop them before doing the calculation otherwise it will return NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as func\n",
    "\n",
    "# the aggregate function can be created outside of the dataframe if desired\n",
    "rms_calc = func.sqrt(func.mean('diff_sq'))\n",
    "\n",
    "\n",
    "# the aggregate function is then called using select\n",
    "# this is similar syntax-wise to a SQL select\n",
    "# and `alias` is used to set the column name\n",
    "# note that a list of aggregate functions can be passed as well,\n",
    "# which will return mulitple columns\n",
    "als_pred.dropna().select( rms_calc.alias('rmse') ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with grid search\n",
    "\n",
    "We can use a parameter grid to identify the best ALS model, much like we did with the supervised machine learning model. We will use RMSE as our error metric again, but we can simplify things by using the RMSE function from the `RegressionEvaluator` module.\n",
    "\n",
    "See the note in the code about shortening the grid search time if it takes too long on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# The ALS instance\n",
    "als = ALS(userCol='userId',\n",
    "          itemCol='movieId',\n",
    "          ratingCol='rating',\n",
    "          seed=42)\n",
    "\n",
    "# The parameter grid to search\n",
    "# NOTE: the parmeter lists can be reduced to two or even \n",
    "# one item if the grid search takes too long\n",
    "als_paramgrid = (ParamGridBuilder()\n",
    "                 .addGrid(als.rank, [2, 4])\n",
    "                 .addGrid(als.maxIter, [10])\n",
    "                 .addGrid(als.regParam, [0.1])\n",
    "                 .addGrid(als.alpha, [2.0, 3.0])\n",
    "                 .build())\n",
    "\n",
    "# The evaluation function for determining the best model\n",
    "rmse_eval = RegressionEvaluator(labelCol='rating',\n",
    "                                predictionCol='prediction', \n",
    "                                metricName='rmse')\n",
    "\n",
    "# The cross validation instance\n",
    "als_cv = CrossValidator(estimator=als,\n",
    "                        estimatorParamMaps=als_paramgrid,\n",
    "                        evaluator=rmse_eval,\n",
    "                        numFolds=3, \n",
    "                        seed=42)\n",
    "\n",
    "# Fit the models and find the best one!\n",
    "als_cv = als_cv.fit(train.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the supervised machine learning cross-validated model, we can explore the best cross-validated ALS model to a limited extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_best = als_cv.bestModel\n",
    "\n",
    "print(als_best.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make predictions using the best model and calculate the RMSE. Based on the gridsearch, the RMSE was reduced slightly from 0.935 to 0.928."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_eval.evaluate(als_pred.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_pred_best = als_best.transform(test)\n",
    "\n",
    "als_best_rmse = pd.DataFrame([ (rmse_eval.evaluate(als_pred.dropna()), \n",
    "                                        rmse_eval.evaluate(als_pred_best.dropna())) ],\n",
    "                                     columns=['rmse_original', 'rmse_crossval'])\n",
    "\n",
    "als_best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predicted vs actual ratings\n",
    "\n",
    "We can use a scatter plot to visualize how well our predictions for the test data match the actual ones.  As can be seen below, there is some room for improvement in our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a Pandas dataframe for plotting\n",
    "als_pred_best_df = als_pred_best.dropna().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('dark')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = als_pred_best_df.plot('rating','prediction', \n",
    "                           marker='o', ls='', ms=3.0,\n",
    "                           alpha=0.5, legend=False)\n",
    "\n",
    "_ = ax.set(xlabel='Actual Rating', ylabel='Predicted Rating',\n",
    "           title='Movie Rating Predictions')\n",
    "ax.set_xlim(0, 5.5)\n",
    "\n",
    "# save the figure in notebooks folder so it's accessible on AWS\n",
    "fig = plt.gcf()\n",
    "fig.savefig('img/movie_rating_predictions.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save recommendation model for later use\n",
    "\n",
    "As with the gradient boosted tree model (GBT) in the supervised machine learning notebook, we can save our alternating least squares (ALS) model and load/use it elsewhere.\n",
    "\n",
    "This section is essentially identical to the supervised machine learning example from yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_name = 'als_recommendation_model_pipeline_crossval'\n",
    "\n",
    "# models will not overwrite existing ones of the same name\n",
    "import shutil\n",
    "if os.path.exists(model_output_name):\n",
    "    shutil.rmtree(model_output_name)\n",
    "    \n",
    "als_best.save(model_output_name)\n",
    "\n",
    "! ls {model_output_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading is also similar--here we have to import the ALS model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "reloaded_model = ALSModel.load(model_output_name)\n",
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
