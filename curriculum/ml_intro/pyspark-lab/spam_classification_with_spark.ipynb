{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification with PySpark\n",
    "\n",
    "Let's use PySpark to classify SMS texts as either spam or ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we need to download the stopwords for nltk which we will be using later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we begin by creating the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "Next we load the train and test spam email data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 4 data/spam_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "train = spark.read.csv('data/spam_train.txt', \n",
    "                       sep='\\t',\n",
    "                       header=True,\n",
    "                       inferSchema=True)\n",
    "\n",
    "test = spark.read.csv('data/spam_test.txt', \n",
    "                       sep='\\t',\n",
    "                       header=True,\n",
    "                       inferSchema=True)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text\n",
    "\n",
    "To begin, the text will be processed to remove punctuation and alphanumerical words. We will slso perform stemming. These tasks don't have a PySpark-specific implementation, so we will be using a standard Python function that will be converted to one that acts on PySpark dataframes. To do this, we will use something called a \"user defined function\" (UDF) in PySpark.\n",
    "\n",
    "Note that stopwords were not removed here, although they could have been. Instead, we will remove them later using a PySpark function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Create the function that performs the text conversion\n",
    "\n",
    "sn = SnowballStemmer('english')\n",
    "\n",
    "def clean_text(text, sn=sn):\n",
    " \n",
    "    # Regular expression substitutions\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation + 'Â£'))\n",
    "    text = punc_re.sub(' ', ' '+text.lower()+' ') # Pad with spaces for easier stopword removal\n",
    "\n",
    "    # Remove numbers\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    text = num_re.sub(' ', text)\n",
    "    \n",
    "    # Remove alphanumerical words\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    text = alpha_num_re.sub(' ', text)\n",
    "    \n",
    "    # Stemming\n",
    "    text = sn.stem(text)\n",
    "    \n",
    "    # Regex for multiple spaces\n",
    "    spaces_re = re.compile('\\s+')\n",
    "    text = spaces_re.sub(' ', text.strip())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Convert the function to a UDF\n",
    "clean_text = udf(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the text in the train and test data by applying the UDF to each row of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn('text', clean_text(train['text']))\n",
    "test  = test.withColumn('text', clean_text(test['text']))\n",
    "\n",
    "train.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert the ham/spam label column (what we are predicting) into a numerical index. We will use PySpark's `StringIndexer` for this. It works much like Scikit-learn's `LabelEncoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convert the column labels\n",
    "si = StringIndexer(inputCol='label', outputCol='label_float')\n",
    "si_model = si.fit(train)\n",
    "\n",
    "train = si_model.transform(train)\n",
    "test = si_model.transform(test)\n",
    "\n",
    "# Drop the string column and rename the numerical one to 'label'\n",
    "train = train.drop('label')\n",
    "train = train.withColumnRenamed('label_float', 'label')\n",
    "\n",
    "test = test.drop('label')\n",
    "test = test.withColumnRenamed('label_float', 'label')\n",
    "\n",
    "train.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding and model building\n",
    "\n",
    "Now we will train some models and compare the results! We will be using several different feature engineering methods for natural language processing. Naive Bayes will be used to predict if the message is spam or ham.\n",
    "\n",
    "We will be using PySpark's [`Pipeline`](https://spark.apache.org/docs/latest/ml-pipeline.html) to create processing pipelines. We will also be using [`ParamGridBuilder`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to create a grid of parameters to search during cross-validation (done later).\n",
    "\n",
    "Create four pipelines with the following features:\n",
    "\n",
    "    1. TF-IDF and Naive Bayes\n",
    "    2. CountVectorizer and Naive Bayes\n",
    "    3. BiGrams, CountVectorizer, and Naive Bayes\n",
    "    4. Word2Vec, CountVectorizer, and Naive Bayes\n",
    "    \n",
    "Note that the output of PySpark's Word2Vec model already has vector averaging performed, so we don't need to do so in this case.\n",
    "\n",
    "First, let's do some pre-processing on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, CountVectorizer, NGram, Word2Vec\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "####### More Preprocessing #######\n",
    "# These steps are done for all pipelines\n",
    "\n",
    "# Tokenize (split) words\n",
    "tk = Tokenizer(inputCol=\"text\", outputCol='tokens')\n",
    "\n",
    "# The stopword removal function\n",
    "sw = StopWordsRemover(inputCol=tk.getOutputCol(),\n",
    "                             outputCol='stopwords')\n",
    "sw = sw.setStopWords(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the first pipeline with TF-IDF and Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### The first ML pipeline #######\n",
    "# TF-IDF and Naive Bayes\n",
    "\n",
    "htf = HashingTF(inputCol=sw.getOutputCol(),\n",
    "                      outputCol='hashing')\n",
    "idf = IDF(inputCol=htf.getOutputCol(),\n",
    "          outputCol='idf')\n",
    "\n",
    "va = VectorAssembler(inputCols=[idf.getOutputCol()], outputCol='features')\n",
    "\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Assemble the pipeline\n",
    "pipeline1 = Pipeline(stages=[tk, sw, htf, idf, va, nb])\n",
    "\n",
    "# Setup parameters to optimize\n",
    "paramgrid1 = (ParamGridBuilder().addGrid(htf.numFeatures, [20])\n",
    "                                .addGrid(idf.minDocFreq, [1])              \n",
    "                                .addGrid(nb.smoothing, [0.0, 1.0])\n",
    "                                .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the second pipeline with Count Vectorizer and Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### The second ML pipeline #######\n",
    "# CountVectorizer and Naive Bayes\n",
    "cntv = CountVectorizer(inputCol=sw.getOutputCol(),\n",
    "                       outputCol='cntv')\n",
    "\n",
    "va = VectorAssembler(inputCols=[cntv.getOutputCol()], outputCol='features')\n",
    "\n",
    "# Assemble the pipeline and set parameters to optimize in gridsearch\n",
    "pipeline2 = Pipeline(stages=[tk, sw, cntv, va, nb])\n",
    "\n",
    "# Setup parameters to optimize\n",
    "paramgrid2 = (ParamGridBuilder().addGrid(cntv.minTF, [1.0, 3.0])\n",
    "                                .addGrid(cntv.minDF, [1.0])              \n",
    "                                .addGrid(nb.smoothing, [0.0, 1.0])\n",
    "                                .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add bigrams into the mix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### The third ML pipeline #######\n",
    "# BiGrams, CountVectorizer, and Naive Bayes\n",
    "\n",
    "ng2 = NGram(n=2, inputCol=sw.getOutputCol(),\n",
    "                  outputCol='ngrams2')\n",
    "\n",
    "# Need separate count vectorizer for single words and bigrams\n",
    "cntv1 = CountVectorizer(inputCol=sw.getOutputCol(),\n",
    "                       outputCol='cntv1')\n",
    "cntv2 = CountVectorizer(inputCol=ng2.getOutputCol(),\n",
    "                       outputCol='cntv2')\n",
    "\n",
    "va = VectorAssembler(inputCols=[cntv1.getOutputCol(),\n",
    "                                cntv2.getOutputCol()], outputCol='features')\n",
    "\n",
    "# Assemble the pipeline and set parameters to optimize in gridsearch\n",
    "pipeline3 = Pipeline(stages=[tk, sw, cntv1, ng2, cntv2, va, nb])\n",
    "\n",
    "# Setup parameters to optimize\n",
    "paramgrid3 = (ParamGridBuilder().addGrid(cntv1.minTF, [1.0])\n",
    "                                .addGrid(cntv1.minDF, [1.0])\n",
    "                                .addGrid(cntv2.minTF, [1.0])\n",
    "                                .addGrid(cntv2.minDF, [1.0, 3.0])\n",
    "                                .addGrid(nb.smoothing, [0.0, 1.0])\n",
    "                                .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally add word2vec as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### The fourth ML pipeline #######\n",
    "# CountVectorizer, Word2Vec, MinMaxScaler, and Naive Bayes\n",
    "\n",
    "wv = Word2Vec(inputCol=tk.getOutputCol(),\n",
    "              outputCol='w2v')\n",
    "\n",
    "# Naive Bayes requires positive features and \n",
    "# Word2Vec features are sometimes negative\n",
    "# So the values are rescaled to between [0.0, 1.0]\n",
    "mm = MinMaxScaler(inputCol=wv.getOutputCol(),\n",
    "                  outputCol='mm')\n",
    "\n",
    "cntv = CountVectorizer(inputCol=sw.getOutputCol(),\n",
    "                       outputCol='cntv')\n",
    "\n",
    "va = VectorAssembler(inputCols=[mm.getOutputCol(), \n",
    "                                cntv.getOutputCol()], outputCol='features')\n",
    "\n",
    "# Assemble the pipeline and set parameters to optimize in gridsearch\n",
    "pipeline4 = Pipeline(stages=[tk, sw, wv, mm, cntv, va, nb])\n",
    "\n",
    "# Setup parameters to optimize\n",
    "paramgrid4 = (ParamGridBuilder().addGrid(cntv.minTF, [1.0])\n",
    "                                .addGrid(cntv.minDF, [1.0, 3.0])              \n",
    "                                .addGrid(nb.smoothing, [0.0, 1.0])\n",
    "                                .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train each of these models using cross validation and output the one with the best f1 score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', \n",
    "                                              labelCol='label', \n",
    "                                              metricName='f1')\n",
    "\n",
    "# List to store the fit model in\n",
    "cvModel_list = list()\n",
    "\n",
    "for lab,pipeline,paramgrid in zip(['TF-IDF', 'CountVectorizer',\n",
    "                                   'CountVectorizer+BiGrams', 'Word2Vec'],\n",
    "                                  [pipeline1, pipeline2, pipeline3, pipeline4],\n",
    "                                  [paramgrid1, paramgrid2, paramgrid3, paramgrid4]):\n",
    "\n",
    "    \n",
    "    # Columns to remove in between fits\n",
    "    drop_cols = [x for x in test.columns if x not in ['text', 'label']]\n",
    "\n",
    "    for col in drop_cols:\n",
    "        train = train.drop(col)\n",
    "        test  = test.drop(col)\n",
    "    \n",
    "    cv = CrossValidator(estimator=pipeline, \n",
    "                        estimatorParamMaps=paramgrid, \n",
    "                        evaluator=evaluator, \n",
    "                        numFolds=4)\n",
    "    \n",
    "    cvModel = cv.fit(train)\n",
    "    test = cvModel.transform(test)\n",
    "    print(lab, evaluator.evaluate(test))\n",
    "    \n",
    "    cvModel_list.append(cvModel)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use the best model to make other predictions, we could extract and serialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_spam_model = cvModel_list[2].bestModel\n",
    "\n",
    "model_output_name = 'spam_model_pipeline_crossval'\n",
    "\n",
    "# models will not overwrite existing ones of the same name\n",
    "import shutil, os\n",
    "if os.path.exists(model_output_name):\n",
    "    shutil.rmtree(model_output_name)\n",
    "\n",
    "best_spam_model.save(model_output_name)\n",
    "\n",
    "! ls {model_output_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
