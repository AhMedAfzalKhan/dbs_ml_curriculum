{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning with PySpark\n",
    "\n",
    "We will be using the Orange Telecom Churn Dataset for this tutorial. This is a relatively well-known data set describing customer turnover in the telecommunications industry. The data we use are being hosted at [https://www.sgi.com/tech/mlc/db/churn.all](https://www.sgi.com/tech/mlc/db/churn.all)."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import requests, os, sys\n",
    "\n",
    "if sys.version_info.major == 3:\n",
    "    from io import StringIO\n",
    "else:\n",
    "    from StringIO import StringIO\n",
    "\n",
    "import pyspark\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_formats = ['retina']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Spark server\n",
    "\n",
    "We will be using the SparkSession (`spark`) to access our spark cluster.."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data\n",
    "\n",
    "The data was originally obtained from https://www.sgi.com/tech/mlc/db/churn.all, but has been made available to us in the `data` directory"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullpath = 'data/churn.txt'\n",
    "    \n",
    "# first line of file\n",
    "! head -n 5 {fullpath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the data do not contain column headings so we will have to add those. \n",
    "\n",
    "When we import the data below, the schema (which is similar to a schema in SQL or dtypes in Pandas) has been inferred based on the data."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read.csv is very similar to the Pandas version\n",
    "data = spark.read.csv(fullpath,\n",
    "                     sep=',',\n",
    "                     inferSchema=True,\n",
    "                     header=True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no column headers, we can see the spark gives default names to all the columns. We will want to change that."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"state\", \"account_length\", \"area_code\", \"phone_number\", \"intl_plan\", \n",
    "           \"voice_mail_plan\", \"number_vmail_messages\", \n",
    "           \"total_day_minutes\", \"total_day_calls\", \"total_day_charge\", \n",
    "           \"total_eve_minutes\", \"total_eve_calls\", \"total_eve_charge\", \n",
    "           \"total_night_minutes\", \"total_night_calls\", \"total_night_charge\", \n",
    "           \"total_intl_minutes\", \"total_intl_calls\", \"total_intl_charge\", \n",
    "           \"number_customer_service_calls\", \"churned\"]\n",
    "\n",
    "old_columns = data.schema.names\n",
    "\n",
    "# renaming columns is a little different with PySpark\n",
    "for old,new in zip(old_columns,columns):\n",
    "    data = data.withColumnRenamed(old, new)\n",
    "\n",
    "# view dtypes\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we inferred the schema and then set the column names above. However, both of these things can also be explicitly created and then provided to the `read.csv` command.\n",
    "\n",
    "An example of a defined schema is below, where the text is the column name, then the type (string, double, boolean, double, etc.) is entered. The final `True` indicates that null values can exist.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"state\", StringType(), True), \n",
    "    StructField(\"account_length\", DoubleType(), True), \n",
    "    StructField(\"area_code\", StringType(), True), \n",
    "    ...\n",
    "    ...\n",
    "    StructField(\"churned\", StringType(), True)])\n",
    "```"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's preview the data as a Pandas dataframe."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to select a few rows of data, convert to a Pandas dataframe, and transpose\n",
    "def preview(df, n=3):\n",
    "    return pd.DataFrame(df.take(n), columns=df.columns).T\n",
    "\n",
    "preview(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Spark SQL allows us to explore data using SQL syntax. Let's examing the breakdown of churned customers. To perform SQL queries, the tables have to be registered using the `registerTempTable` method."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.registerTempTable('data')\n",
    "\n",
    "churn_counts = spark.sql(r\"\"\"SELECT churned, COUNT(churned) AS total \n",
    "                            FROM data\n",
    "                            GROUP BY churned\"\"\")\n",
    "churn_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that there is a period at the end of the True/False values in the churned column. (There is also a space at the beginning, but that is less obvious.) We will clean all of these issues up later.\n",
    "\n",
    "Let's examine the churn in the states with the most subscribers. First, determine the 10 states with the most subscribers."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_customer_counts = spark.sql(r\"\"\"SELECT state, COUNT(state) AS total \n",
    "                                     FROM data \n",
    "                                     GROUP BY state \n",
    "                                     ORDER BY total desc \n",
    "                                     LIMIT 10\"\"\")\n",
    "state_customer_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_churn_counts = spark.sql(r\"\"\"SELECT state, COUNT(churned) as churned\n",
    "                                  FROM data\n",
    "                                  WHERE churned LIKE '%True%'\n",
    "                                  GROUP BY state\"\"\")\n",
    "\n",
    "state_churn_counts.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are actual PySpark dataframes and they can be joined using either PySpark or SQL. First the join using SQL."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the tables with SQL\n",
    "state_customer_counts.registerTempTable('state_customer_counts')\n",
    "state_churn_counts.registerTempTable('state_churn_counts')\n",
    "\n",
    "# join using SQL\n",
    "combined_state_data = spark.sql(r\"\"\"SELECT cust_cts.state, churned, total\n",
    "                                   FROM state_customer_counts AS cust_cts\n",
    "                                   INNER JOIN state_churn_counts AS churn_cts\n",
    "                                   ON cust_cts.state = churn_cts.state\"\"\")\n",
    "combined_state_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark also allows dataframes to be joined and sorted in a fashion similar to Pandas."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_state_data2 = state_churn_counts.join(state_customer_counts, \n",
    "                                               on='state', \n",
    "                                               how='inner')\n",
    "\n",
    "# sort inversely by total column\n",
    "combined_state_data2 = combined_state_data2.sort(combined_state_data2.total.desc())\n",
    "\n",
    "combined_state_data2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "The columns that are supposed to be boolean (`intl_plan`, `voice_mail_plan`, and `churned`) need some cleaning and their types need to be converted from string to boolean."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# columns not used in analysis\n",
    "for field in ['state', 'area_code', 'phone_number']:\n",
    "    data = data.drop(field)\n",
    "\n",
    "def trim(string):\n",
    "    return (string\n",
    "            .strip()                # string columns have spaces due to csv file format\n",
    "            .replace('.','')        # remove period at end of each line\n",
    "            .replace('yes','True')  # convert yes/no to True/False\n",
    "            .replace('no','False'))\n",
    "\n",
    "# udf converts a function into one that can be applied over\n",
    "# a dataframe column. This is kind of like Pandas apply/map functionality.\n",
    "trim = udf(trim)\n",
    "    \n",
    "# apply the function to every string column\n",
    "for dtype in data.dtypes:\n",
    "    column = dtype[0]\n",
    "    if dtype[1] == 'string':\n",
    "        data = data.withColumn(column, trim(data[column]))\n",
    "\n",
    "# boolean types converted to integers\n",
    "for column in ['intl_plan', 'voice_mail_plan']:\n",
    "    data = data.withColumn(column, data[column].cast('boolean').cast('int'))\n",
    "    \n",
    "# predictors have to be of type double for many of Spark's ML models\n",
    "data = data.withColumn('churned', data['churned'].cast('boolean').cast('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned the data, let's make correlation plots of variables."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('dark')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "# get the names of columns that are integers or doubles\n",
    "numeric_features = [t[0] for t in data.dtypes if t[1] in ['int', 'double']]\n",
    "\n",
    "# sample 10% of this data and convert to a Pandas dataframe\n",
    "sampled_data = data.select(numeric_features).sample(False, 0.1).toPandas()\n",
    "\n",
    "# make the scatter plot\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(12, 12));\n",
    "\n",
    "# Rotate axis labels and remove axis ticks\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, there are four pairs of variables that are highly correlated with each other, corresponding to charge and minutes for day, night, evening, and international. This makes sense since the charge like reflects a per-minute rate. We will remove the charge column for each of these."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove one of each pair of highly correlated columns\n",
    "for field in ['total_day_charge', 'total_night_charge',\n",
    "              'total_eve_charge', 'total_intl_charge']:\n",
    "    data = data.drop(field)\n",
    "    \n",
    "preview(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "Before modelling, the data are split into train and test data sets. \n",
    "\n",
    "We could also do a three-way split for train, validation, and test data using the `randomSplit` method:\n",
    "\n",
    "```python\n",
    "train, valid, test = data.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "```\n",
    "\n",
    "Finally, it is also possible to do a stratified split using the `sampleBy` method of the dataframe."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine feature columns\n",
    "\n",
    "Inputs to Scikit-learn models have the data split into X-arrays/dataframes (features) and Y-arrays/dataframes (predictors). PySpark expects both the features and the predictor to be in a single dataframe. However, all the features have to be combined into a single column containing a list of the feature values. To accomplish this, we use [`VectorAssembler`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n",
    "\n",
    "Note that VectorAssembler uses transform. This is just like the second step of Scikit-learn's feature transformers for normalization, etc.\n",
    "\n",
    "We will also be using [`MinMaxScaler`](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.MinMaxScaler) to scale the features for the logistic regression model. (Note that scaling isn't strictly required with logistic regression, but it doesn't hurt and it's good to get experience with how these feature modifications work.\n",
    "\n",
    "In the cells below, use VectorAssembler to create a packed feature column and then use MinMaxScaler to scale this packed column."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# the feature columns\n",
    "features = data.schema.names[:-1]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "\n",
    "train_pack = assembler.transform(train)\n",
    "test_pack = assembler.transform(test)\n",
    "\n",
    "# (optional) remove the columns packed into the feature vector\n",
    "for field in features:\n",
    "    train_pack = train_pack.drop(field)\n",
    "    test_pack  = test_pack.drop(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "minmaxscale = MinMaxScaler(inputCol='features', outputCol='features_scaled')\n",
    "minmaxscale = minmaxscale.fit(train_pack)\n",
    "\n",
    "train_pack = minmaxscale.transform(train_pack)\n",
    "test_pack  = minmaxscale.transform(test_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model building\n",
    "\n",
    "Now we get to the fun part--machine learning! \n",
    "\n",
    "PySpark has two machine learning libraries:\n",
    "\n",
    "1. MLlib: older, based on RDDs, more complete, but no additional features are being added as of Spark 2.0\n",
    "2. ML: the *future*, based on DataFrames, less feature complete than MLlib but this is improving\n",
    "\n",
    "Often the name MLlib is used to refer to both libraries, so context is important and this can be confusing. We will be using ML whenever possible."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, \\\n",
    "                                      GBTClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by training a number of models and evaluating the results. Notice that the `fit` method is similar to Scikit-learn. However, `transform` is used with PySpark instead of `predict`. The syntax is similar among different PySpark ML models, though.\n",
    "\n",
    "Try to train the logistic regression model with the scaled features column. For the other models, just use the unscaled features."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "lr = LogisticRegression(labelCol='churned', \n",
    "                        featuresCol='features_scaled',\n",
    "                        predictionCol='prediction')\n",
    "\n",
    "lr_model = lr.fit(train_pack)\n",
    "lr_pred = lr_model.transform(test_pack)\n",
    "\n",
    "# Decision Tree (depth = 2)\n",
    "dt = DecisionTreeClassifier(maxDepth=2, \n",
    "                            labelCol='churned', \n",
    "                            featuresCol='features',\n",
    "                            predictionCol='prediction')\n",
    "\n",
    "dt_model = dt.fit(train_pack)\n",
    "dt_pred = dt_model.transform(test_pack)\n",
    "\n",
    "# Random Forest (also of depth = 2)\n",
    "rf = RandomForestClassifier(maxDepth=2, \n",
    "                            labelCol='churned',\n",
    "                            featuresCol='features',\n",
    "                            predictionCol='prediction')\n",
    "\n",
    "rf_model = rf.fit(train_pack)\n",
    "rf_pred = rf_model.transform(test_pack)\n",
    "\n",
    "# Gradient Boosted Tree (depth = 2)\n",
    "gbt = GBTClassifier(maxDepth=2, \n",
    "                    labelCol='churned', \n",
    "                    featuresCol='features',\n",
    "                    predictionCol='prediction')\n",
    "\n",
    "gbt_model = gbt.fit(train_pack)\n",
    "gbt_pred = gbt_model.transform(test_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "PySpark also has error metrics that are similar to those found in Scikit-learn. They are divided into metrics appropriate only for binary classifications (ROC and Precision-Recall curve related metrics) and those that also apply to multi-class classifications."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, \\\n",
    "                                  MulticlassClassificationEvaluator\n",
    "\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol='churned', \n",
    "                                             predictionCol='prediction',\n",
    "                                             metricName='accuracy')\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(labelCol='churned', \n",
    "                                              predictionCol='prediction',\n",
    "                                              metricName='weightedPrecision')\n",
    "\n",
    "recall = MulticlassClassificationEvaluator(labelCol='churned', \n",
    "                                           predictionCol='prediction',\n",
    "                                           metricName='weightedRecall')\n",
    "\n",
    "f1 = MulticlassClassificationEvaluator(labelCol='churned', \n",
    "                                       predictionCol='prediction',\n",
    "                                       metricName='f1')\n",
    "\n",
    "areaROC = BinaryClassificationEvaluator(labelCol='churned',\n",
    "                                        rawPredictionCol='prediction',\n",
    "                                        metricName='areaUnderROC')\n",
    "\n",
    "areaPR = BinaryClassificationEvaluator(labelCol='churned',\n",
    "                                       rawPredictionCol='prediction',\n",
    "                                       metricName='areaUnderPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics can be used to create a table of the values for each model's predictions."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the error metrics\n",
    "metrics = [accuracy, precision, recall, f1, areaROC, areaPR]\n",
    "metric_labels = ['accuracy', 'precision', 'recall', 'f1', 'areaROC', 'areaPR']\n",
    "\n",
    "# the predictions from each model\n",
    "predictions = [lr_pred, dt_pred, rf_pred, gbt_pred]\n",
    "predict_labels = ['LR', 'DT', 'RF', 'GBT']\n",
    "\n",
    "eval_list = list()\n",
    "\n",
    "# for each model's predictions, calculate error metrics\n",
    "# and add to a Pandas series\n",
    "for pred in zip(predict_labels, predictions):\n",
    "    name = pred[0]\n",
    "    predict = pred[1]\n",
    "    \n",
    "    metric_vals = pd.Series(dict([(x[0], x[1].evaluate(predict)) \n",
    "                                 for x in zip(metric_labels, metrics)]),\n",
    "                            name=name)\n",
    "    eval_list.append(metric_vals)\n",
    "    \n",
    "# combine all the series into a dataframe\n",
    "eval_df = pd.concat(eval_list, axis=1).T\n",
    "eval_df = eval_df[metric_labels]\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar metrics (plus a confusion matrix) are also available in MLlib, the older library. We will have to convert the dataframe to an RDD to use these metrics. These metrics have the advantage that they are a little easier (and less verbose) to implement. There is also a confusion matrix function."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, \\\n",
    "                                     MulticlassMetrics\n",
    "\n",
    "# let's make a function this time so it's reuseable\n",
    "\n",
    "def evaluate_model_predictions(prediction_list, prediction_labels):\n",
    "    \n",
    "    eval_list = list()\n",
    "\n",
    "    # for each model's predictions, calculate error metrics\n",
    "    # and add to a Pandas series\n",
    "    for pred in zip(prediction_labels, prediction_list):\n",
    "        name = pred[0]\n",
    "        predict = pred[1]\n",
    "\n",
    "        # here is where the prediction dataframe is converted to an rdd\n",
    "        predict_rdd = predict[['prediction', 'churned']].rdd\n",
    "        b_metrics = BinaryClassificationMetrics(predict_rdd)\n",
    "        m_metrics = MulticlassMetrics(predict_rdd)\n",
    "\n",
    "        metric_vals = pd.Series({'accuracy'        : m_metrics.accuracy,\n",
    "                                 'precision'       : m_metrics.precision(0),\n",
    "                                 'recall'          : m_metrics.recall(0),\n",
    "                                 'f1'              : m_metrics.fMeasure(),\n",
    "                                 'areaROC'         : b_metrics.areaUnderROC,\n",
    "                                 'areaPR'          : b_metrics.areaUnderPR,\n",
    "                                 'confusion matrix': (m_metrics.confusionMatrix()\n",
    "                                                      .toArray().astype('int'))},\n",
    "                                name=name)\n",
    "        eval_list.append(metric_vals)\n",
    "\n",
    "    # combine all the series into a dataframe\n",
    "    eval_df = pd.concat(eval_list, axis=1).T\n",
    "    \n",
    "    metric_labels = ['accuracy', 'precision', 'recall', 'f1', 'areaROC', 'areaPR', 'confusion matrix']\n",
    "    eval_df = eval_df[metric_labels]\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "\n",
    "# the predictions from each model\n",
    "predictions = [lr_pred, dt_pred, rf_pred, gbt_pred]\n",
    "predict_labels = ['LR', 'DT', 'RF', 'GBT']\n",
    "\n",
    "eval_df2 = evaluate_model_predictions(predictions, predict_labels)\n",
    "eval_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the calculations for precision, recall, and f1 are a little different from the first set. This is because the dataframe calculations use weighted precision and recall."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine into pipeline\n",
    "\n",
    "Any feature transformations and model fitting can be combined into a pipeline. Pipelines are useful because:\n",
    "\n",
    "1. The data can be split  *before* any transformations that may lead to leakage of the test data.\n",
    "2. Simultaneously ensure that the split data sets are treated identically.\n",
    "\n",
    "We don't do much in this example for feature transformation other than assembling all the features into one column and scaling, but let's use that to create a pipeline for the logistic regression model we used above.\n",
    "\n",
    "If we *did* do extensive feature engineering, then having a pipeline would be extremely beneificial. (Just like in scikit-learn.)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# the pipline\n",
    "pipeline = Pipeline(stages=[assembler, minmaxscale, lr]) \n",
    "\n",
    "# fit and transform\n",
    "lr_model2 = pipeline.fit(train)\n",
    "lr_pred2 = lr_model2.transform(test)\n",
    "\n",
    "# the results are the same for GBT as the second table above\n",
    "evaluate_model_predictions([lr_pred2], ['LR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search and cross validation\n",
    "\n",
    "PySpark also has gridsearch and crossvalidation functionalities, similar to Scikit-learn's. Let's do a quick example with GradientBoostedTrees."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take ~5min to run this cell\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# the GradientBoostedTree model\n",
    "gbt2 = GBTClassifier(featuresCol='features',\n",
    "                     labelCol='churned', \n",
    "                     predictionCol='prediction')\n",
    " \n",
    "# the pipline\n",
    "pipeline = Pipeline(stages=[assembler, gbt2]) \n",
    "\n",
    "# the parameter grid--we'll optimize maxDepth and stepSize\n",
    "paramgrid = (ParamGridBuilder().addGrid(gbt2.maxDepth, [4,6])\n",
    "                               .addGrid(gbt2.stepSize, [0.001, 0.01, 0.1, 1.0]).build())\n",
    "\n",
    "# use f1 score as the evaluation metric for best model \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='churned', \n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='f1') \n",
    "\n",
    "# use 3-fold cross validation \n",
    "crossval = CrossValidator(estimator=pipeline, \n",
    "                          estimatorParamMaps=paramgrid, \n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=3) \n",
    "\n",
    "gbt2_model = crossval.fit(train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like with Scikit-learn's gridsearch function, we can probe for the best model and view its attributes. \n",
    "\n",
    "First, we have to select the best model, though."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the best pipeline based on f1 score\n",
    "best_pipeline = gbt2_model.bestModel\n",
    "\n",
    "# return the best GBT model, which is the second step of the pipeline\n",
    "best_gbt_model = best_pipeline.stages[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all model parameters can be extracted with PySpark, but we can extract the individual trees for GradientBoostedTrees and then examine them."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many trees were there?\n",
    "len(best_gbt_model.trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list some of the trees\n",
    "best_gbt_model.trees[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what were their weights in the GBT model?\n",
    "best_gbt_model.treeWeights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what was the best depth?\n",
    "[x.depth for x in best_gbt_model.trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine feature importances, much like Scikit-learn."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature importances\n",
    "feature_importances = best_gbt_model.featureImportances.toArray()\n",
    "\n",
    "# extract feature names, except for the predictor\n",
    "feature_names = train.columns[:-1]\n",
    "\n",
    "feature_series = (pd.Series(dict(zip(feature_names, feature_importances)))\n",
    "                  .sort_values(ascending=True))\n",
    "\n",
    "feature_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot them in a bar graph. Total minutes (day, evening, night, or international) are the biggest predictor of churn."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('dark')\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "ax = feature_series.plot(kind='barh')\n",
    "_ = ax.set(xlabel='Relative Importance', \n",
    "           ylabel='Features', \n",
    "           title='Feature Importances for Best GradientBoostedTree Model')\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "# save to the folder shared with the AWS instance\n",
    "# this is the same folder as the notebook is executed from\n",
    "# so a path is not required\n",
    "fig.savefig('img/feature_importances.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure error on test data set\n",
    "\n",
    "Let's use the best model to measure the error on the test data set. We used f1 score for the gridsearch metric, and as can be seen below, this improved from 0.916 to 0.943 with the gridsearch."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pred_test = best_pipeline.transform(test)\n",
    "\n",
    "evaluate_model_predictions([gbt_pred_test], ['GBT_GridSearch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for later use\n",
    "\n",
    "PySpark trained models can be saved for later use and then reloaded. Conceptually, this is much like pickling, except the output is a directory."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_name = 'gbt_model_pipeline_crossval'\n",
    "\n",
    "# models will not overwrite existing ones of the same name\n",
    "import shutil\n",
    "if os.path.exists(model_output_name):\n",
    "    shutil.rmtree(model_output_name)\n",
    "\n",
    "best_gbt_model.save(model_output_name)\n",
    "\n",
    "! ls {model_output_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model can be loaded here or in other analysis. Note that we have to import the *model* instance to load the file. Previously, we had only imported the classifier."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "\n",
    "reloaded_model = GBTClassificationModel.load(model_output_name)\n",
    "reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
