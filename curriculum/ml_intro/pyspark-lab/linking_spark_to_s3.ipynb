{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Spark to S3\n",
    "\n",
    "Writing out from Spark directly to S3 can be a very handy thing to do. However, Spark doesn't naturally play well with S3... it borrows that capability from Hadoop. Which means that we need to tell Spark and Hadoop how to play together with S3. This notebook demonstrates some of the setup for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up my AWS connection and the Spark Arguments\n",
    "\n",
    "First, we need to get our AWS Keys in order and then tell Spark that whenever it's asked to make a context, it should build itself with a certain set of packages activated. In particular, we want the AWS connectors active from Java and Hadoop. So we'll get all of that in order using the OS environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "aak = os.environ['AWS_ACCESS_KEY']\n",
    "ask = os.environ['AWS_SECRET_KEY']\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and configuring Spark Context\n",
    "\n",
    "We'll now build our spark context, then tell hadoop that we want it to know about the s3 file system. We'll that and then tell it how to connect by providing the keys we have. The first config line creates the filesystem as something accessible. The second and third line tells hadoop how to act like me when talking to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "hadoopConf = spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoopConf.set(\"fs.s3.awsAccessKeyId\", aak)\n",
    "hadoopConf.set(\"fs.s3.awsSecretAccessKey\", ask)\n",
    "\n",
    "df = spark.read.csv(\"/Users/zachariahmiller/Documents/Metis/chi18_ds7/class_lectures/week04-mcnulty1/02-logistic_sql_load/data/all_state_1950.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have that set up and some data, I can just treat `s3://bucket_name` as a location that I can write to or read from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('s3://whynotwork/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet('s3://whynotwork/test')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
