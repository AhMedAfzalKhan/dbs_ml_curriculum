{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:59:44.006489Z",
     "start_time": "2018-11-28T03:59:41.486469Z"
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import arrow\n",
    "\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "for lib in (sklearn, np, sns): # seaborn should be .9+ and sklearn .20+\n",
    "    print(f'{lib.__name__} Version: {lib.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretical complexity is one thing, but what about empirical complexity? Let's look at the models we might use for classification and see how they do on large datasets.\n",
    "\n",
    "This is a short lesson meant to get you thinking about *real world, practical* complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:59:49.493941Z",
     "start_time": "2018-11-28T03:59:45.867547Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras # we're just using this to grab the dataset for now\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're going to need bigger data!\n",
    "\n",
    "In order to see important differences between models, we'll want to look at them as they handle datasets that are much bigger than what we've used so far in class.\n",
    "\n",
    "In order to do that, we'll use the full MNIST dataset ([more info](http://yann.lecun.com/exdb/mnist/)). This classic dataset contains 70,000 images of hand written digits. Each image is 28x28 pixels. Just like we saw in the KNN digits demo, we'll perform image recognition by flattening each image so that each of the 784 pixels are treated like a feature.\n",
    "\n",
    "In total, this gives us about 60MB of data. It may not sound like much, but you'll see this amount cause trouble for several of the algorithms we've discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:59:49.995039Z",
     "start_time": "2018-11-28T03:59:49.497293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the data and measure its size \n",
    "\n",
    "(x, y), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "mb = (x.size + x_test.size + y.size + y_test.size)/1e6\n",
    "\n",
    "print(f\"dataset size: {mb}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T03:59:51.536498Z",
     "start_time": "2018-11-28T03:59:49.998545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape each observation to be 784 features instead of a 28x28 matrix \n",
    "x = (x.reshape(-1,784)/255  - .5).astype(\"float64\")\n",
    "x_test = (x_test.reshape(-1,784)/255  - .5).astype(\"float64\")\n",
    "\n",
    "# MNIST isn't shuffled by default so let's shuffle x and y\n",
    "indices = np.random.choice(len(y), size=len(y), replace=False)\n",
    "x = x[indices, :]; y = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "What's the time complexity of K-nearest neighbors?\n",
    "\n",
    "Specifically, given the number of samples ($n$) and the number of features for each sample ($f$), find \n",
    "1. the big-O time complexity for **training** KNN given a set of samples `x` and their labels `y`\n",
    "2. the big-O time complexity for using KNN to **predict** a class `y` given a single sample `x_i`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "The complexity for **training** KNN is trivial: We're not learning any new parameters. In fact, the parameters are just our training data!\n",
    "\n",
    "The complexity of **prediction**, however, is nontrivial. Given a candidate `x_i`, we have to find the `k` nearest neighbors out of the $n$-sized dataset. In the simplest case, this means measuring the distance between `x_i` and every single sample in our training data. That's `O(n)` distance measurements. Additionally, each distance measurement is done across $f$ dimensions, so ultimately we're looking at `O(kn)` complexity.\n",
    "\n",
    "How do you think KNN's complexity of `O(1)` for train and `O(kn)` for prediction compares to other models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:17:22.468263Z",
     "start_time": "2018-11-28T17:17:22.461365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import a few more algorithms so we can time time on different sized samples\n",
    "from sklearn import svm, ensemble, linear_model, neighbors, naive_bayes, kernel_approximation, pipeline, exceptions, model_selection\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=exceptions.ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:17:23.008175Z",
     "start_time": "2018-11-28T17:17:23.002283Z"
    }
   },
   "outputs": [],
   "source": [
    "# What's the time right now? \n",
    "lap_time = arrow.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:17:23.382451Z",
     "start_time": "2018-11-28T17:17:23.374579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Has it been 30 seconds since we ran the last cell? This is how we'll keep track of how much time has passed\n",
    "lap_time.shift(seconds=30) < arrow.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:20:09.060517Z",
     "start_time": "2018-11-28T17:17:24.053305Z"
    }
   },
   "outputs": [],
   "source": [
    "import timeit  # We'll use this package to run each simulation multiple times and take the fastest runtime\n",
    "\n",
    "num_loops = 5\n",
    "num_repeats = 1\n",
    "num_tests = 10\n",
    "max_samples = 2000\n",
    "max_features = 100\n",
    "max_sec = 30\n",
    "\n",
    "# my favorite way to build a datafrom on the fly is\n",
    "# to start with a list of dictionaries\n",
    "complexity_lod = []\n",
    "\n",
    "# Each time we run our algorithm, we'll want to use a larger sample size to test the effect of sample size on runtime\n",
    "# Set up the list of sample sizes we'll use\n",
    "samples = np.linspace(100, max_samples, num=num_tests, dtype=int)\n",
    "\n",
    "# approx rbf svm\n",
    "approx_rbf_svm = pipeline.make_pipeline(\n",
    "    kernel_approximation.Nystroem(kernel=\"rbf\", gamma=0.01, n_components=400),\n",
    "    linear_model.SGDClassifier(max_iter=500, tol=1e-3, eta0=0.01, learning_rate=\"adaptive\"))\n",
    "approx_poly_svm = pipeline.make_pipeline(\n",
    "    kernel_approximation.Nystroem(kernel=\"poly\", gamma=0.01, n_components=400, kernel_params={\"degree\": 5}),\n",
    "    linear_model.SGDClassifier(max_iter=500, tol=1e-3, eta0=0.01, learning_rate=\"adaptive\"))\n",
    "\n",
    "# We'll want to test multiple algorithms\n",
    "model_dict = {\n",
    "    \"svm-linearSVC\":\n",
    "    svm.LinearSVC(),\n",
    "    \"svm-rbf\":\n",
    "    svm.SVC(gamma=\"auto\"),\n",
    "    \"svm-poly3\":\n",
    "    svm.SVC(kernel=\"poly\", gamma=\"auto\"),\n",
    "    \"svm-rbf-sgd\":\n",
    "    approx_rbf_svm,\n",
    "    \"svm-poly-sgd\":\n",
    "    approx_poly_svm,\n",
    "    \"random-forest\":\n",
    "    ensemble.RandomForestClassifier(n_estimators=100),\n",
    "    \"logistic\":\n",
    "    linear_model.LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        multi_class=\"auto\",\n",
    "    ),\n",
    "    \"knn\":\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    \"nb\":\n",
    "    naive_bayes.GaussianNB(),\n",
    "}\n",
    "\n",
    "# Run all of our models, increasing the sample size each time (from samples list we made above)\n",
    "for n_samp in samples:\n",
    "    print(f\"Running for {n_samp} samples. \", end=\"\")\n",
    "    lap_time = arrow.now()\n",
    "\n",
    "    # Loop through each algorithm\n",
    "    for model_key in model_dict:\n",
    "\n",
    "        if (n_samp > 700) and (model_key == \"svm-linearSVC\"):\n",
    "            continue\n",
    "\n",
    "        model = model_dict[model_key]\n",
    "\n",
    "        # we take the min of the repeats because variation is likely due not to python execution\n",
    "        # but other operations https://docs.python.org/3/library/timeit.html#timeit.Timer.repeat\n",
    "        train_time = np.min(\n",
    "            timeit.repeat(\n",
    "                f'model.fit(x[:n_samp,:max_features],y[:n_samp])',\n",
    "                globals=globals(),\n",
    "                repeat=num_repeats,\n",
    "                number=num_loops))\n",
    "\n",
    "        test_time = np.min(\n",
    "            timeit.repeat(\n",
    "                f'model.predict(x[:n_samp,:max_features])',\n",
    "                globals=globals(),\n",
    "                repeat=num_repeats,\n",
    "                number=num_loops))\n",
    "\n",
    "        cv_acc = mean(\n",
    "            model_selection.cross_val_score(\n",
    "                model, x[:n_samp, :max_features], y[:n_samp], cv=3))\n",
    "\n",
    "        complexity_lod.append({\n",
    "            \"model\": model_key,\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\": test_time,\n",
    "            \"n_samples\": n_samp,\n",
    "            \"cv_acc\": cv_acc\n",
    "        })\n",
    "    print(f\"Time: {(arrow.now() -lap_time).seconds} seconds.\")\n",
    "    if lap_time.shift(seconds=max_sec) < arrow.now():\n",
    "        print(\"breaking\")\n",
    "        break\n",
    "\n",
    "complexity_df = pd.DataFrame(complexity_lod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:20:10.469582Z",
     "start_time": "2018-11-28T17:20:09.063772Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = subplots(nrows=3, sharex=True, figsize=(10,10))\n",
    "\n",
    "sns.lineplot(x=\"n_samples\", y=\"train_time\", hue=\"model\", data=complexity_df, ax=axes[0])\n",
    "sns.lineplot(x=\"n_samples\", y=\"test_time\", hue=\"model\", data=complexity_df, ax=axes[1])\n",
    "sns.lineplot(x=\"n_samples\", y=\"cv_acc\", hue=\"model\", data=complexity_df, ax=axes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "What do these results tell us about which models would be feasible to use in practice?\n",
    "\n",
    "* When would selecting a model with **low train complexity** be a priority?\n",
    "* When would selecting a model with **low test complexity** be a priority?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now without the slowpokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:36:16.227533Z",
     "start_time": "2018-11-28T17:33:57.000725Z"
    }
   },
   "outputs": [],
   "source": [
    "import timeit  # We'll use this package to run each simulation multiple times and take the fastest runtime\n",
    "\n",
    "num_loops = 4\n",
    "num_repeats = 1\n",
    "num_tests = 10\n",
    "max_samples = 4000\n",
    "max_features = 784\n",
    "max_sec = 30\n",
    "\n",
    "# my favorite way to build a datafrom on the fly is\n",
    "# to start with a list of dictionaries\n",
    "complexity_lod = []\n",
    "\n",
    "# Each time we run our algorithm, we'll want to use a larger sample size to test the effect of sample size on runtime\n",
    "# Set up the list of sample sizes we'll use\n",
    "samples = np.linspace(100, max_samples, num=num_tests, dtype=int)\n",
    "\n",
    "# approx rbf svm see: https://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation\n",
    "model_dict = {\n",
    "#     \"svm-linearSVC\":\n",
    "#     svm.LinearSVC(),\n",
    "#     \"svm-rbf\":\n",
    "#     svm.SVC(gamma=\"auto\"),\n",
    "#     \"svm-poly3\":\n",
    "#     svm.SVC(kernel=\"poly\", gamma=\"auto\"),\n",
    "    \"svm-rbf-sgd\":\n",
    "    approx_rbf_svm,\n",
    "    \"svm-poly-sgd\":\n",
    "    approx_poly_svm,\n",
    "    \"random-forest\":\n",
    "    ensemble.RandomForestClassifier(n_estimators=100),\n",
    "    \"logistic\":\n",
    "    linear_model.LogisticRegression(\n",
    "        solver=\"lbfgs\",\n",
    "        multi_class=\"auto\",\n",
    "    ),\n",
    "#     \"knn\":\n",
    "#     neighbors.KNeighborsClassifier(),\n",
    "#     \"nb\":\n",
    "#     naive_bayes.GaussianNB(),\n",
    "}\n",
    "\n",
    "# Run all of our models, increasing the sample size each time (from samples list we made above)\n",
    "for n_samp in samples:\n",
    "    print(f\"Running for {n_samp} samples. \", end=\"\")\n",
    "    lap_time = arrow.now()\n",
    "\n",
    "    # Loop through each algorithm\n",
    "    for model_key in model_dict:\n",
    "        \n",
    "        if (n_samp > 2000) and (model_key==\"logistic\"):\n",
    "            continue\n",
    "\n",
    "        model = model_dict[model_key]\n",
    "\n",
    "        # we take the min of the repeats because variation is likely due not to python execution\n",
    "        # but other operations https://docs.python.org/3/library/timeit.html#timeit.Timer.repeat\n",
    "        train_time = np.min(\n",
    "            timeit.repeat(\n",
    "                f'model.fit(x[:n_samp,:max_features],y[:n_samp])',\n",
    "                globals=globals(),\n",
    "                repeat=num_repeats,\n",
    "                number=num_loops))\n",
    "\n",
    "        test_time = np.min(\n",
    "            timeit.repeat(\n",
    "                f'model.predict(x[:n_samp,:max_features])',\n",
    "                globals=globals(),\n",
    "                repeat=num_repeats,\n",
    "                number=num_loops))\n",
    "        \n",
    "        cv_acc = mean(\n",
    "            model_selection.cross_val_score(\n",
    "                model, x[:n_samp, :max_features], y[:n_samp], cv=3))\n",
    "\n",
    "        complexity_lod.append({\n",
    "            \"model\": model_key,\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\": test_time,\n",
    "            \"n_samples\": n_samp,\n",
    "            \"cv_acc\": cv_acc \n",
    "        })\n",
    "    print(f\"Time: {(arrow.now() -lap_time).seconds} seconds.\")\n",
    "    if lap_time.shift(seconds=max_sec) < arrow.now():\n",
    "        print(\"breaking\")\n",
    "        break\n",
    "\n",
    "complexity_df = pd.DataFrame(complexity_lod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T17:37:39.577453Z",
     "start_time": "2018-11-28T17:37:38.115515Z"
    },
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "fig, axes = subplots(nrows=3, sharex=True, figsize=(10,10), )\n",
    "\n",
    "sns.lineplot(x=\"n_samples\", y=\"train_time\", hue=\"model\", data=complexity_df, ax=axes[0])\n",
    "sns.lineplot(x=\"n_samples\", y=\"test_time\", hue=\"model\", data=complexity_df, ax=axes[1])\n",
    "sns.lineplot(x=\"n_samples\", y=\"cv_acc\", hue=\"model\", data=complexity_df, ax=axes[2])\n",
    "axes[0].set_title(\"Model time and performance on MNIST dataset\")\n",
    "savefig(\"bakeoff.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "What does this tell us? These analyses don't give us hard-and-fast answers. The instructors at Metis have decided to teach these models because every one of them is either important to understand or likely to be used in industry, and often both. \n",
    "\n",
    "Two of these in particular are worth noting: KNN and Random Forests. Both of these models are strong performers on benchmark tasks, where we have a fixed dataset and we want to see which model performs best (these are often derisively referred to as \"bake offs\").\n",
    "\n",
    "But (as they are implemented in sklearn) they both have a weakness: Prediction time scales linearly with the number of examples in our training set. This is important because often *the only or easiest way to improve a model is to add more and better training data*. For these two, more data might lead to better predictions, but also leads to slower performance. You'll find that this is always true of KNN but you might be able to tweak the hyperparameters of Random Forests to get improved time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "\n",
    "Modify the above code block to benchmark models in terms of the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "num_loops = 5\n",
    "num_repeats = 1\n",
    "num_tests = 20\n",
    "max_samples = 500\n",
    "max_features = 784\n",
    "max_sec = 30\n",
    "\n",
    "# my favorite way to build a datafrom on the fly is \n",
    "# to start with a list of dictionaries\n",
    "complexity_lod = []\n",
    "\n",
    "\n",
    "# set up the list of the different sizes we want to try\n",
    "features = np.linspace(50,max_features, num=num_tests, dtype=int)\n",
    "\n",
    "model_dict = {\n",
    "    \"svm-linearSVC\": svm.LinearSVC(),\n",
    "    \"svm-rbf\": svm.SVC(gamma=\"auto\"),\n",
    "    \"svm-poly3\": svm.SVC(kernel=\"poly\", gamma=\"auto\"),\n",
    "    \"random-forest\": ensemble.RandomForestClassifier(n_estimators=100),\n",
    "    \"logistic\": linear_model.LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "    \"knn\": neighbors.KNeighborsClassifier(),\n",
    "    \"nb\": naive_bayes.GaussianNB()\n",
    "}\n",
    "\n",
    "\n",
    "# loop through each training/test set size\n",
    "for n_feat in features:\n",
    "    print(f\"Running for {n_feat} features. \", end=\"\")\n",
    "    lap_time = arrow.now()\n",
    "    for model_key in model_dict:\n",
    "        \n",
    "        model = model_dict[model_key]\n",
    "    \n",
    "        # we take the min of the repeats because variation is likely due not to python execution \n",
    "        # but other operations https://docs.python.org/3/library/timeit.html#timeit.Timer.repeat\n",
    "        train_time = np.min(\n",
    "            timeit.repeat(\n",
    "                f'model.fit(x[:max_samples,:n_feat],y[:max_samples])',\n",
    "                globals=globals(),\n",
    "                repeat=num_repeats,\n",
    "                number=num_loops))\n",
    "\n",
    "        test_time = np.min(\n",
    "            timeit.repeat(\n",
    "                f'model.predict(x[:max_samples,:n_feat])',\n",
    "                globals=globals(),\n",
    "                repeat=num_repeats,\n",
    "                number=num_loops))\n",
    "\n",
    "        complexity_lod.append({\n",
    "                \"model\": model_key,\n",
    "                \"train_time\": train_time,\n",
    "                \"test_time\": test_time,\n",
    "                \"n_features\": n_feat\n",
    "            })\n",
    "    print(f\"Time: {(arrow.now() -lap_time).seconds} seconds.\")\n",
    "    if lap_time.shift(seconds=max_sec) < arrow.now():\n",
    "        print(\"breaking\")\n",
    "        break\n",
    "complexity_df = pd.DataFrame(complexity_lod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"n_features\", y=\"train_time\", hue=\"model\", data=complexity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"n_features\", y=\"test_time\", hue=\"model\", data=complexity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
