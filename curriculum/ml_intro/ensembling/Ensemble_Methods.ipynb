{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install mlxtend if you haven't already\n",
    "\n",
    "# ! conda install -y mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from mlxtend.classifier import StackingClassifier # <-- note: this is not from sklearn!\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, \n",
    "                              AdaBoostClassifier, BaggingRegressor)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "\n",
    "# change margin size of jupyter notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap aggregating**, more commonly referred to as **bagging**, is a type of machine learning algorithm that is designed to improve the accuracy and stability of the model. We will discuss bootstrapping and aggregating below, but we'll first outline the algorithm itself.\n",
    "\n",
    "The Bagging algorithm is fairly simple:\n",
    "\n",
    "Given a training set $T = \\{(x_1, y_1), \\ldots, (x_n, y_n)\\}$\n",
    "\n",
    "- Sample $k$ sets of $n$ elements from $T$ (with replacement), giving sets $T_1, \\ldots, T_k$\n",
    "- Train an algorithm on each $T_i, i=1, \\ldots, k$ and obtain a sequence of $k$ outputs $f_1(\\mathbf{x}), \\ldots, f_k(\\mathbf{x})$\n",
    "\n",
    "\n",
    "The final aggregate classifier is:\n",
    "\n",
    "**for Regression** - take the average of all of the predictions\n",
    "$$\\bar{f}(\\mathbf{x}) = \\frac{1}{k} \\sum_{i=1}^k f_i(\\mathbf{x})$$\n",
    "\n",
    "**for Classification** there are a few options, which are covered in more depth later in this lesson including:\n",
    "- Average Voting\n",
    "- Max Voting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here is a nice visual representation of the bagging algorithm\n",
    "\n",
    "![](images/bagging.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "Bootstrapping is a sampling technique. Out of the $n$ samples in our dataset, $k$ samples are chosen **with replacement**. \n",
    "\n",
    "We'll use income data to see how this works in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/incomes.csv')\n",
    "incomes = df['income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_indices(num):\n",
    "    return np.random.randint(0, num, num)\n",
    "\n",
    "def get_random_sample(X):\n",
    "    indices = get_random_indices(len(X))\n",
    "    return X[indices]\n",
    "\n",
    "def get_sample_median(X):\n",
    "    sample = get_random_sample(X)\n",
    "    return np.median(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = [get_sample_median(incomes) for _ in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.hist(medians)\n",
    "plt.title('Distribution of Medians')\n",
    "plt.xlabel('Median Income of Samples')\n",
    "plt.ylabel('Number of Samples')\n",
    "\n",
    "# Plot real median value\n",
    "whole_sample_median = np.median(incomes)\n",
    "plt.gca().axvline(whole_sample_median, ls='--', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get a dataset to build a machine learning model, we can only say what the sample median is for this dataset, but not necessarily what the true population median is. In other words, our sample median may be different from the true population median and we need a way to look at many different subsets to generalize. This is what we calculated in the figure above. \n",
    "\n",
    "What happens if we don't bootstrap? Well, it's probable that we can get a biased sample that will fail to generalize, i.e., we will overfit our model. \n",
    "\n",
    "As with any statistical sampling method, we prefer to look at the *distribution* of our feature and find a 95% likelihood cut-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the percentiles? This gives us a range of values \n",
    "# that the sample median lies in 95% of the samples\n",
    "np.percentile(medians,2.5), np.percentile(medians, 97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.hist(medians)\n",
    "plt.title('Distribution of Medians')\n",
    "plt.xlabel('Median income of Samples')\n",
    "plt.ylabel('Number of Samples')\n",
    "\n",
    "lower, upper = np.percentile(medians, 2.5), np.percentile(medians, 97.5)\n",
    "plt.gca().axvline(whole_sample_median, ls='--', c='k')\n",
    "plt.gca().axvline(lower, ls='--', c='r')\n",
    "plt.gca().axvline(upper, ls='--', c='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique samples do we have from our original dataset?\n",
    "\n",
    "unique_samples = []\n",
    "for i in range(500):\n",
    "    uniques = len(np.unique(get_random_indices(len(incomes))))\n",
    "    unique_samples.append(uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of observations in our dataset increases, our bootstrapping method will select $\\approx (1 - 1/e)$ unique samples. The derivation is beyond the scope of this lesson, but can be found in the resources at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify statement above\n",
    "(1 - 1/np.exp(1))*len(incomes), np.mean(unique_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from an earlier lesson that Decision Trees are prone to high variance. In other words, they will find complex & nonlinear ways to get the highest accuracy from the data, but it comes at the cost of overfitting to the training set. This is why decision trees aren't popular in practice and instead data scientists use an *ensemble* of decision trees instead to address this. \n",
    "\n",
    "**Note:** an ensemble of decision trees is called a Random Forest.\n",
    "\n",
    "\n",
    "## Why ensemble?\n",
    "\n",
    "Because **bagging reduces variance**\n",
    "\n",
    "The intuition is the following:\n",
    "\n",
    "If each single classifier is unstable - that is, it has **high\n",
    "variance** like decision trees - the aggregated classifier $\\bar f$ has a smaller variance than a single original classifier.\n",
    "\n",
    "This is because the aggregated classifier $\\bar f$ can be thought of as an approximation to the true (i.e. population) average obtained by replacing the probability distribution $p$ in the data with the bootstrapped approximations to $p$ obtained by concentrating mass $1/n$ at each point $(x_i, y_i)$.\n",
    "\n",
    "In short, looking at bootstrapped samples and running several different models on the data allows the classifier to find the best aggregate model that will generalize best to our data.\n",
    "\n",
    "Let's see this in action using the California Housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm our model is overfitting\n",
    "dt_model = DecisionTreeRegressor(random_state=123)\n",
    "dt_model.fit(X_train, y_train)\n",
    "print(f'Train R^2 Score: {np.round(dt_model.score(X_train, y_train), 4)}')\n",
    "print(f'Test R^2 Score: {np.round(dt_model.score(X_test, y_test), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_metric(y_true, y_pred):\n",
    "    return np.mean((np.mean(y_pred) - y_true)**2)\n",
    "\n",
    "def variance_metric(y_true, y_pred):\n",
    "    return np.var(y_true - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bagging classifier through cross validation\n",
    "# ~45s to run\n",
    "# instructor: pull up the bagging classifier docs if useful\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html\n",
    "\n",
    "variance = []\n",
    "bias = []\n",
    "test_range = np.arange(1, 30, 1)\n",
    "\n",
    "for i in test_range:\n",
    "    cv_out = cross_validate(\n",
    "        estimator=BaggingRegressor(DecisionTreeRegressor(random_state=123), \n",
    "                                   n_estimators=i),\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=3,\n",
    "        return_train_score=True,\n",
    "        scoring={\n",
    "            \"variance\": make_scorer(variance_metric),\n",
    "            \"bias\": make_scorer(bias_metric)\n",
    "        },\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    variance.append(np.mean(cv_out['test_variance']))\n",
    "    bias.append(np.mean(cv_out['test_bias']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(test_range, variance, label='variance')\n",
    "plt.ylim((0, 0.65))\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Variance Score');\n",
    "plt.title('Variance Score vs No. of Estimators');\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.plot(test_range, bias, label='bias', c='r')\n",
    "plt.ylim((1.3, 1.4))\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Bias Score');\n",
    "plt.title('Bias Score vs No. of Estimators');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see there is a clear reduction in the variance when increasing the number of estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we talked about Bagging and how it reduces variance. But there are many more types of ensemble models. The Bagging classifier, allows us to look at the same data through the lens of many bootstrapped samples... But, we're only able to use one classifier to train each bootstrapped sample.\n",
    "\n",
    "This begs the question: **Can we apply the same logic from bagging to use not just one model, but many different kinds of models as well?**\n",
    "\n",
    "Fortunately, the answer is **yes** and this section will provide how we can use this \"best-of-breed\" approach to build different ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "\n",
    "Before we dive into the nitty-gritty we'll use an analogy to drive what we want to achieve from our ensemble model.\n",
    "\n",
    "There is an old parable, called \"The Blind Men and the Elephant\" [wikipedia](https://en.wikipedia.org/wiki/Blind_men_and_an_elephant).\n",
    "\n",
    "The gist is the following:\n",
    "\n",
    "> A group of blind men have never seen an elephant before. They all are curious to learn what the elephant is like, so each blind man feels a different part of the elephant's body, but only one part, such as the side or the tusk. They then describe the elephant based on their limited experience and their descriptions of the elephant are different from each other.\n",
    "\n",
    "Sounds pretty similar to what data scientists do!\n",
    "\n",
    "![](images/elephant.jpeg)\n",
    "\n",
    "We can think of each blind man as an individual \"model\" that comes with certain biases and different decision as to what the elephant is like. Fortunately, we don't have to take the recommendation of one model only, we can take all of those recommendations ourselves and decide what makes up an elephant.\n",
    "\n",
    "This is, in essence, what any ensemble model is trying to do. It will look at all of the recommendations from different kinds of models and then make a decision based on the aggregate of all of the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common ensemble models is the Voting Classifier. It does what its name implies, looks at the votes of the pre-trained algorithms and gives an output.\n",
    "\n",
    "<img src=\"images/ensemble.png\" alt=\"\" width=\"500\" height=\"600\"/>\n",
    "\n",
    "There are 3 different flavors for voting classifier which we will discuss.\n",
    "\n",
    "- Max Voting\n",
    "- Average Voting\n",
    "- Weighted Voting\n",
    "\n",
    "But, before we get into all the fun stuff, let's load some data and pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = dict(enumerate(sns.color_palette(\"colorblind\", 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot('column1', 'column2', data=df, hue='label',\n",
    "           palette=pal, alpha=.2, edgecolor=None)\n",
    "plt.gcf().set_size_inches(10,6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), df.label, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained/tuned models\n",
    "\n",
    "model_names = [\"lr_model\", \"nb_model\", \"knn_model\", \"svc_model\", \"rf_model\", \"et_model\", \"ada_model\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    with open(f\"models/{model_name}.pickle\", \"rb\") as pfile:\n",
    "        exec(f\"{model_name} = pickle.load(pfile)\")\n",
    "\n",
    "model_vars = [eval(n) for n in model_names]\n",
    "model_list = list(zip(model_names, model_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek at each model performance\n",
    "\n",
    "for model_name in model_names:\n",
    "    curr_model = eval(model_name)\n",
    "    print(f'{model_name} score: {curr_model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic assumption for the max voting algorithm is easy: Take the class that has the largest number of predictions for each model.\n",
    "\n",
    "**Classifier 1 -> Class 0**\n",
    "\n",
    "**Classifier 2 -> Class 1**\n",
    "\n",
    "**Classifier 3 -> Class 1**\n",
    "\n",
    "In the example above, our max voting method would choose class 1.\n",
    "\n",
    "One item to be aware of is what happens with ties. Say we had the following outputs:\n",
    "\n",
    "**Classifier 1 -> Class 0**\n",
    "\n",
    "**Classifier 2 -> Class 1**\n",
    "\n",
    "**Classifier 3 -> Class 1**\n",
    "\n",
    "**Classifier 4 -> Class 0**\n",
    "\n",
    "\n",
    "In this scenario, the max voting method chooses class 0 since the algorithm breaks the tie by looking at the output from the **last** classifier (sorted alphabetically by name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize output from each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at decision regions will help us solidify how the algorithm is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision region for each model.\n",
    "gs = GridSpec(3, 3)\n",
    "for model_name, grd_i in zip(model_names, itertools.product([0, 1, 2], repeat=2)):\n",
    "\n",
    "    clf = eval(model_name)\n",
    "\n",
    "    ax = plt.subplot(gs[grd_i[0], grd_i[1]])\n",
    "    fig = plot_decision_regions(\n",
    "        X=X_train.values,\n",
    "        y=y_train.values,\n",
    "        clf=clf,\n",
    "        legend=2,\n",
    "        markers=\"o\",\n",
    "        scatter_kwargs=dict(c=list(pal.values()), alpha=.2, edgecolors=\"none\"),\n",
    "        contourf_kwargs=dict(colors=list(pal.values())))\n",
    "    ax.set_title(model_name.replace('_', ' ').upper())\n",
    "\n",
    "plt.gcf().set_size_inches(12, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our knowledge of the max voting method to use. \n",
    "\n",
    "**Focusing only in the square in the lower right corner, what prediction do we expect our model to give us for the ensemble classifier?**\n",
    "\n",
    "**Answer**: Looking at the output from each of the models, we have two models predicting class 0 (Log Reg and Naive Bayes) and the other five models predicting class 1. \n",
    "\n",
    "Thus, our model should **predict class 1** as it has more votes.\n",
    "\n",
    "Let's verify that this is indeed the case by running our ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create voting classifier\n",
    "voting_classifer = VotingClassifier(estimators=model_list,\n",
    "                                    voting='hard', #<-- sklearn calls this hard voting\n",
    "                                    n_jobs=-1)\n",
    "voting_classifer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "plot_decision_regions(X_train.values, y_train.values, voting_classifer)\n",
    "plt.title('Max voting classifier')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.gcf().set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Et voilà!**\n",
    "\n",
    "Our ensemble classifier chose class 1 for our lower right corner, just like we expected.\n",
    "\n",
    "\n",
    "Like any other model in `sklearn` we can see how our ensemble model performing on the test data. Ideally, we'd like to beat our best performing individual model, but this is **not** always guaranteed to happen. (No Free Lunch Theorem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy (model to beat: SVC with 0.8456 accuracy)\n",
    "y_pred = voting_classifer.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not an improvement, but our ensemble classifier would be tied for 2nd place out of all of our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In average voting (aka soft voting), we predict the class labels based on the predicted probabilities $p$ for classifier.\n",
    "\n",
    "Ultimately, the algorithm will decide which class has the highest sum of probabilities, i.e.,\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_i \\sum_{j=1}^k p_{ij}$$\n",
    "\n",
    "where $i$ is the number of classes and $k$ is the number of models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example we are looking at a binary classification task with class labels $i \\in \\{0,1\\}$. Imagine our ensemble is making the following predictions:\n",
    "\n",
    "- $\\mathbf{C}_1(\\mathbf{x}) \\rightarrow [0.9, 0.1]$ \n",
    "- $\\mathbf{C}_2(\\mathbf{x}) \\rightarrow [0.8, 0.2]$\n",
    "- $\\mathbf{C}_3(\\mathbf{x}) \\rightarrow [0.4, 0.6]$ \n",
    "\n",
    "\n",
    "\n",
    "Using uniform weights, the average probabilities are the following:\n",
    "\n",
    "$$p(i_0 | \\mathbf{x}) = \\frac{0.9 + 0.8 + 0.4}{3} = 0.7 $$\n",
    "\n",
    "$$p(i_1 | \\mathbf{x}) = \\frac{0.1 + 0.2 + 0.6}{3} = 0.3 $$\n",
    "\n",
    "$$\\hat y = \\arg \\max_i [p(i_0 | \\mathbf{x}), p(i_1 | \\mathbf{x})] = 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create voting classifier\n",
    "voting_classifer = VotingClassifier(estimators=model_list,\n",
    "                                    voting='soft', #<-- sklearn calls this soft voting\n",
    "                                    n_jobs=-1)\n",
    "voting_classifer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "plot_decision_regions(X_train.values, y_train.values, voting_classifer)\n",
    "plt.title('Average voting classifier')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.gcf().set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our decision regions have changed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy (model to beat: SVC with 0.8456 accuracy)\n",
    "y_pred = voting_classifer.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no improvement..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, we'll be assigning weights to each model to adjust its contribution to the final prediction. There will be times when you want to give additional weight to models that are performing better to increase the optimization metric.\n",
    "\n",
    "Mathematically speaking, there is very little change to the Average Voting method we just saw. All we do is add a weight to each probability, i.e.,\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_i \\sum_{j=1}^k w_j p_{ij}$$\n",
    "\n",
    "where $i$ is the number of classes, $w_j$ is the model weight, and $k$ is the number of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine our ensemble is making the same predictions as before:\n",
    "\n",
    "- $\\mathbf{C}_1(\\mathbf{x}) \\rightarrow [0.9, 0.1]$ \n",
    "- $\\mathbf{C}_2(\\mathbf{x}) \\rightarrow [0.8, 0.2]$\n",
    "- $\\mathbf{C}_3(\\mathbf{x}) \\rightarrow [0.4, 0.6]$ \n",
    "\n",
    "\n",
    "\n",
    "Using weights for each model with value $\\{0.1, 0.1, 0.8\\}$, the results are the following:\n",
    "\n",
    "$$p(i_0 | \\mathbf{x}) = 0.1*0.9 + 0.1*0.8 + 0.8*0.4 = 0.49 $$\n",
    "\n",
    "$$p(i_1 | \\mathbf{x}) = 0.1*0.1 + 0.1*0.2 + 0.8*0.6 = 0.51 $$\n",
    "\n",
    "$$\\hat y = \\arg \\max_i [p(i_0 | \\mathbf{x}), p(i_1 | \\mathbf{x})] = 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create voting classifier\n",
    "weights = [1.5,2.3,3.8,4.2,4.2,2.2,1.1]\n",
    "voting_model = VotingClassifier(estimators=model_list,\n",
    "                                    voting='soft', \n",
    "                                    weights = weights,\n",
    "                                    n_jobs=-1)\n",
    "voting_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "plot_decision_regions(X_train.values, y_train.values, voting_model)\n",
    "plt.title('Weighted voting classifier')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.gcf().set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy (model to beat: SVC with 0.8456 accuracy)\n",
    "y_pred = voting_model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Weighted Voting System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class probabilities for all classifiers\n",
    "probas = [c.predict_proba(X_train) for n,c in model_list]\n",
    "probas += [voting_model.predict_proba(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class probabilities for the first sample in the dataset\n",
    "class1_1 = [pr[0, 0] for pr in probas]\n",
    "class2_1 = [pr[0, 1] for pr in probas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "N = 8  # number of groups\n",
    "ind = np.arange(N)  # group positions\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# bars for classifier 1-7\n",
    "p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), \n",
    "            width,\n",
    "            color='green', \n",
    "            edgecolor='k')\n",
    "p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), \n",
    "            width,\n",
    "            color='lightgreen', \n",
    "            edgecolor='k')\n",
    "\n",
    "# bars for VotingClassifier\n",
    "p3 = ax.bar(ind, [0,0,0,0,0,0,0, class1_1[-1]], \n",
    "            width,\n",
    "            color='blue', \n",
    "            edgecolor='k')\n",
    "p4 = ax.bar(ind + width, [0,0,0,0,0,0,0, class2_1[-1]], \n",
    "            width,\n",
    "            color='dodgerblue', \n",
    "            edgecolor='k')\n",
    "\n",
    "# plot annotations\n",
    "plt.axvline(N - 1.3, color='k', linestyle='dashed')\n",
    "ax.set_xticks(ind + width)\n",
    "x_labels = [f\"{n}\\nweight {weights[i]}\" for i, n in enumerate(model_names)]\n",
    "ax.set_xticklabels(\n",
    "    x_labels + ['VotingClassifier\\n(weighted probabilities)'],\n",
    "    rotation=40,\n",
    "    ha='right')\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Class probabilities for sample 1 by different classifiers')\n",
    "plt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning weights\n",
    "\n",
    "In general, tuning the weights for the ensemble classifier is not easy. There is no built-in method to tune or even an educated guess as to where we begin our search. If this were a regression problem, we can take all of the outputs from each of the classifiers and pass them through an OLS model to tune each weight.\n",
    "\n",
    "In our case, we have a classification problem which makes determining the right coefficients a little more difficult to do. One method is to use a **stacked classifier** which we will cover in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. \n",
    "\n",
    "The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. \n",
    "\n",
    "The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n",
    "\n",
    "The major difference between the Stacked Classifier and the Voting classifier, is that it uses the outputs of the previous models as features in the meta-classifier to make new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stacked.png\" alt=\"\" width=\"500\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = StackingClassifier(\n",
    "    classifiers=model_vars, meta_classifier=BernoulliNB(), use_probas=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model\n",
    "plot_decision_regions(X_train.values, y_train.values, stacked)\n",
    "plt.title('Stacked classifier')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.gcf().set_size_inches(12,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacked.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "![](images/ensemble-all-the-things.jpg)\n",
    "\n",
    "In this lesson we covered the different techniques available to ensemble models in sklearn. \n",
    "\n",
    "Things to note:\n",
    "- Max Voting returns the most common class out of the classifiers (similar to how KNN works)\n",
    "- Average Voting returns the most likely class given all of the probabilities (similar to how Naive Bayes works)\n",
    "- Weighted Voting returns the most likely class given the weighted sum of probabilities\n",
    "- Finding the optimal weights to Weighted Voting can be difficult to achieve\n",
    "- Stacked Classifiers use the outputs of the models as features into the meta-classifier\n",
    "\n",
    "## Pros/Cons of Ensemble Methods\n",
    "\n",
    "**Pros:**\n",
    "- Reduces variance\n",
    "- (generally) better model performance\n",
    "\n",
    "**Cons:**\n",
    "- Loss of model interpretability\n",
    "- Possibility of high bias if data is not modeled properly\n",
    "- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- http://people.csail.mit.edu/rivest/pubs/APR07.pdf\n",
    "- http://www.mit.edu/~9.520/spring06/Classes/class10.pdf\n",
    "- http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
