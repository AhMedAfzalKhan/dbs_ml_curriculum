{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a reference for different activation and loss functions common in Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T20:06:40.137756Z",
     "start_time": "2018-12-11T20:06:40.094296Z"
    }
   },
   "source": [
    "# Activation functions for hidden  layers\n",
    "\n",
    "\n",
    "\n",
    "| Activation function | Function | Plot | Notes | \n",
    "|:--------------------|:------|:------|:-----|\n",
    "| ReLU (Rectified Linear Unit) | $$f(x) = \\max(x, o)$$ | ![](activations/relu.svg) | General purpose. Great default. |\n",
    "| tanh | $$f(x) = \\tanh(x)$$ | ![](activations/tanh.svg) |  |\n",
    "| LeakyReLU | $$f(x) = \\begin{cases} \\alpha x & x < 0 \\\\ x & x \\geq 0 \\end{cases} $$ | ![](activations/Leakyrelu.svg) | Useful in Generative Adversarial Networks. Cases where gradient needs to flow even in off state. |\n",
    "| ELU (Exponential Linear Unit) | $$f(x) = \\begin{cases} \\alpha e^{(x-1)} & x < 0 \\\\ x & x \\geq 0 \\end{cases} $$ |  ![](activations/elu.svg) | Sometimes better for image recognition. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions for Output layers\n",
    "\n",
    "| Activation function | Function | Plot | Notes | \n",
    "|---------------------|-------|-------|------|\n",
    "| Linear | $f(x) = x $ | ![](activations/linear.svg) | Regression |\n",
    "| Sigmoid | $$f(x) = \\sigma(x) = \\frac{1}{1+e^{-x}}$$ | ![](activations/sigmoid.svg) | Binary (two class) classification |\n",
    "| SoftMax | $$f_i(\\mathbf{x}) = \\frac{e^{x_i}}{ \\sum_{x_j \\in \\mathbf{x}} e^{x_j}} $$ | - | Multi-class classification | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Loss Functions\n",
    "\n",
    "| Loss | Function | Tasks | Notes |\n",
    "|------|----------|-------|-------|\n",
    "| Categorical Cross Entropy | $$ f(y, \\hat{y}) = -\\sum_{i=0}^C y_i\\log(\\hat{y}_i)  $$ | Multi Class & Binary Classification | Output layer should be a probability (softmax for multiclass and sigmoid for binary)|\n",
    "| Hinge loss | $$ f(y, \\hat{y}) = \\max(0, 1-y\\hat{y}) $$ | Binary Classification | Output layer should be linear |\n",
    "| Mean Square Error | $$ f(y, \\hat{y}) = \\frac{1}{n}\\sum(y-\\hat{y})^2 $$ | Regression | Output layer should be linear |\n",
    "| Mean Absolute Error | $$ f(y, \\hat{y}) = \\frac{1}{n}\\sum \\lvert y-\\hat{y} \\rvert $$ | Regression | Output layer should be linear |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
