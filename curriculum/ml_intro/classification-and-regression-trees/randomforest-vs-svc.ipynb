{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees vs Support Vectors\n",
    "\n",
    "Both these classification algorithms are powerful. But one might outperform the other based on the nature of the input data. Here are two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We choose a 100 random points between (0,1) in two dimensions, and send the labels to be 0 and 1 based on if they are above or below the diagonal line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 2)\n",
    "y1 = [1 if i[0]>i[1] else 0 for i in X]\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def visualize(X, y):\n",
    "    c = cm.rainbow(np.linspace(0, 1, 2))\n",
    "    plt.scatter([i[0] for i in X], [i[1] for i in X], color=[c[i] for i in y], alpha=.5)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.grid(True)\n",
    "    \n",
    "\n",
    "visualize(X, y1)\n",
    "\n",
    "# based on code above, which labels do red and purple have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data above is diagonally separable. And just a linear SVC can nail that. As we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def quick_test(model, X, y):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3)\n",
    "    model.fit(xtrain, ytrain)\n",
    "    return model.score(xtest, ytest)\n",
    "\n",
    "def quick_test_afew_times(model, X, y, n=10):\n",
    "    return np.mean([quick_test(model, X, y) for j in range(n)])\n",
    "\n",
    "linearsvc = LinearSVC()\n",
    "# Do the test 10 times with a LinearSVC and get the average score\n",
    "quick_test_afew_times(linearsvc, X, y1)\n",
    "\n",
    "# why don't we get 100% accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a tough problem for Decision Tree. It will easily overfit the data and perform poorly. Random Forest corrects for the overfitting and improves the performance, but never gets as good as the Linear SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier(max_depth=2)\n",
    "quick_test_afew_times(decisiontree, X, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest = RandomForestClassifier()\n",
    "quick_test_afew_times(randomforest, X, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will try a different categorization. Put an orthogonal axis in the middle of x values and make two diagonal quadrants the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = [1 if (0.5-i[0])*(0.5-i[1])>0 else 0 for i in X]\n",
    "visualize(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC struggles, since the data isn't linearly seperable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearsvc = LinearSVC()\n",
    "quick_test_afew_times(linearsvc, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a polynomal kernel can do better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "quick_test_afew_times(svc, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree thrives on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisiontree = DecisionTreeClassifier()\n",
    "quick_test_afew_times(decisiontree, X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But something to keep in mind. SVC, especially non-linear ones perform best when the data is scaled between -1 and 1. Performance improves once we do that. But still not the perfect score of the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = SVC()\n",
    "X2 = (0.5-X) * 2\n",
    "quick_test_afew_times(s2, X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We looked at two simple problems. Linear SVC and Decision Tree solved a different one perfectly and struggled with the other. When they struggled, sophistications like non-linear kernels and Random Forests helped, but still couldn't reach perfections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's easy to throw your data into a whole bunch of algorithms and see which one does best. But gaining some intuition for your data and your models, is worth the time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": false,
   "nav_menu": {
    "height": "206px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": "3",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
