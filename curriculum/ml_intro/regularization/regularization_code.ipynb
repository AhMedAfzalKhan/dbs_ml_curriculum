{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pylab inline\n",
    "#% config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression Applied to the Ames Housing Data\n",
    "\n",
    "Using the Ames Housing Data:\n",
    "\n",
    "Dean De Cock\n",
    "Truman State University\n",
    "Journal of Statistics Education Volume 19, Number 3(2011), www.amstat.org/publications/jse/v19n3/decock.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build some linear regression models to predict housing prices from this data.  We will split our data into training, validation, and test sets, build various (regularized) models on the training/validation data and compare their results on the test set. We will examine metrics such as *r-squared* and *mean absolute error*.\n",
    "\n",
    "**Notebook Contents**\n",
    "\n",
    "> 1. Simple EDA and dataset review\n",
    "> 2. Exploring the behavior of LASSO vs. Ridge regularization\n",
    "> 3. Standard-scaling features (a must for regularization!)\n",
    "> 4. Tuning regularization strength via validation\n",
    "> 5. Automated regularization strength tuning via cross-validation\n",
    "> 6. _Bonus_: Using the LARS path to study feature importance \n",
    "\n",
    "## 1. Simple EDA and Dataset Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data, Examine and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the Ames Housing Data\n",
    "datafile = \"data/Ames_Housing_Data.tsv\"\n",
    "df=pd.read_csv(datafile, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the columns, look at missing data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is recommended by the data set author to remove a few outliers\n",
    "\n",
    "df = df.loc[df['Gr Liv Area'] <= 4000,:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are a *lot* of variables, many of which have a lot of missing values.  Let's pick out just a few columns and start building models using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There appears to be one NA in Garage Cars - fill with 0\n",
    "smaller_df = smaller_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a nice, filtered dataset, let's generate visuals to better understand the target and feature-target relationships: pairplot is great for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(smaller_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**Data Exploration Exercises**: \n",
    "\n",
    "1. What do these plots tell us about the distribution of the target?   \n",
    "\n",
    "2. What do these plots tell us about the relationship between the features and the target? Do you think that linear regression is well-suited to this problem? Do any feature transformations come to mind?\n",
    "\n",
    "3. What do these plots tell us about the relationship between various pairs of features? Do you think there may be any problems here? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up for modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate our features from our target\n",
    "\n",
    "X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars']]\n",
    "\n",
    "y = smaller_df['SalePrice']\n",
    "\n",
    "# create overall quality squared term, which we expect to \n",
    "# help based on the relationship we see in the pair plot \n",
    "X['OQ2'] = X['Overall Qual'] ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data 60 - 20 - 20 train/val/test\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=.25, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Behavior of LASSO vs. Ridge Regularization\n",
    "\n",
    "We have been playing around with adding in or removing variables (or transformations of variables), and then seeing if the model improves.  However, this can be a tedious process.\n",
    "\n",
    "Regularized Linear Regression tries to circumvent this by changing the *cost function*.  In \"vanilla\" linear regression, the coefficients are chosen purely to minimize the Sum of Squared Errors.  In Regularized Linear Regression, there is an additional component of the cost function that penalizes the \"size\" of the coefficients.  \n",
    "\n",
    "Why penalize a coefficient?  At the simplest level, it forces a variable to be \"worth it\" in order to have a particularly extreme coefficient or even one that's greater than zero.  The intuition is that it is a \"simpler model\" to have smaller coefficients (in absolute value) than larger ones - regularization means that the coefficients are constrained to lie within a narrower region, making model coefficients more stable and less extreme.\n",
    "\n",
    "Regularized Linear Regression introduces a **regularization strength parameter** that says how strongly we want to penalize the coefficients.  At one extreme there is no penalty, and we revert back to \"vanilla\" Linear Regression.  At the other extreme, the penalty is so onerous that we set all of the coefficients to zero.  In between these two extremes are a continuous set of models.  We will see how to choose the best value with *validation* or *cross-validation*.\n",
    "\n",
    "There are two main \"flavors\" of Regularized Linear Regression.  In the **LASSO**, we penalize the sum of the absolute values of the coefficients and in **Ridge Regression** we penalize the sum of the squares of the coefficients. Which one works better depends on the data and the business needs of your model (e.g., strong inclination toward interpretability with small set of variables suggests use of LASSO). \n",
    "\n",
    "Let's see some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_columns = ['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                    'Year Built','Year Remod/Add', 'Gr Liv Area', \n",
    "                    'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                    'Garage Cars', 'OQ2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha = 1000000) # this is a VERY HIGH regularization strength!, wouldn't usually be used\n",
    "lasso_model.fit(X_train.loc[:,selected_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(selected_columns, lasso_model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how LASSO sets many variables to 0 (with a high enough alpha parameter). This is its ** feature selection ** property. To help remember that LASSO does this and not ridge, look no further than the name: **Least Absolute Shrinkage and Selection Operator** -- the shrinkage part means making the coefficients smaller by penalizing their size in the cost function, and the selection part is zeroing out coefficients. Remember the double SS! \n",
    "\n",
    "To highlight a notable difference between LASSO and ridge, we're going to add an additional column that's Lot Area + noise, so that we have two **highly collinear columns**. Then we'll fit a ridge and LASSO model and compare their coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "\n",
    "X_train_collinear = X_train.loc[:,selected_columns]\n",
    "X_train_collinear['Lot Area Clone'] = (X_train_collinear['Lot Area'] + \n",
    "                                      2500 * np.random.randn(X_train.shape[0]))\n",
    "\n",
    "X_train_collinear.corr() #notice .95 correlation b/w Lot Area and its \"clone\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick aside, let's understand what happens with p-values when there is a lot of collinearity! We are much less sure about our relationships being meaningful. In this case the model does detect the right variable as having a significant relationship, but this need not be the case in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train_collinear))\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare and contrast Ridge vs. Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_ridge = Ridge(alpha = 1000000000000)\n",
    "lr_model_ridge.fit(X_train_collinear, y_train)\n",
    "\n",
    "list(zip(X_train_collinear.columns, lr_model_ridge.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge** smoothed out all of the coefficients, **bringing them closer to 0 but not discarding any of them**. Also, it gave **roughly equal weight to the two highly collinear features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_lasso = Lasso(alpha = 100000)\n",
    "lr_model_lasso.fit(X_train_collinear, y_train)\n",
    "\n",
    "list(zip(X_train_collinear.columns, lr_model_lasso.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, **Lasso zeroed out most of the coefficients**, and **dropped the noisy collinear clone**, performing feature selection to keep the features we really wanted. \n",
    "\n",
    "What are the Pros/Cons of either behavior?\n",
    "\n",
    "**LASSO**:\n",
    "* _Pro_: great for trimming features and focusing interpretation on a few key ones\n",
    "* _Con_: risk of discarding features that are actually useful\n",
    "\n",
    "**Ridge**:\n",
    "* _Pro_: great for smoothly handling multicollinearity, very nice when working with sparse features \n",
    "* _Con_: will never fully discard features\n",
    "\n",
    "As always, you have to validate to choose between the two. If the mapping from features to target truly depends on only a few key features, LASSO should outperform. If instead the target actually depends on many features (even if only a little dependent), Ridge should work better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Back to the original LASSO model: diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pred = lasso_model.predict(X_test.loc[:,selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_set_pred, y_test, alpha=.1)\n",
    "plt.plot(np.linspace(0,600000,1000), np.linspace(0,600000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#r-squared\n",
    "r2_score(y_test, test_set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Absolute Error (MAE)\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_pred - y_true)) \n",
    "\n",
    "mae(y_test, test_set_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standard-scaling Features (a must for regularization!)\n",
    "\n",
    "One issue with Regularized Linear Regression is that the \"size\" of a coefficient may be more reflective of the units or scale of the associated variable than the actual power of the relationship.  For example, if a distance is measured in millimeters it will have a larger coefficient than if it is measured in miles.  For this reason, best practice is to \"standardize\" the variables prior to running a regularized regression.  Standardizing means subtracting off each feature column's mean and then dividing by its standard deviation so that the resulting variable has mean 0 and standard deviation 1. This ensures that the variables are penalized fairly with respect to one another.\n",
    "\n",
    "We demonstrate how to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This step fits the Standard Scaler to the training data\n",
    "## Essentially it finds the mean and standard deviation of each variable in the training set\n",
    "\n",
    "std = StandardScaler()\n",
    "std.fit(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This step applies the scaler to the train set.\n",
    "## It subtracts the mean it learned in the previous step and then divides by the standard deviation\n",
    "\n",
    "X_tr = std.transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the scaler to the test set\n",
    "\n",
    "X_te = std.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that even though we put a Pandas Dataframe into the scalar, what comes out is a numpy array\n",
    "## In general, sklearn works on numpy.  It will accept pandas objects by trying to coerce them to numpy arrays\n",
    "## But it will not usually output pandas objects\n",
    "\n",
    "type(X_train), type(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we can plot histograms of the transformed variables\n",
    "## Note that they seem to have means of 0 and stddevs of 1\n",
    "## (though they are not necessarily normally distributed)\n",
    "\n",
    "plt.hist(X_tr[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have appropriately scaled our variables, we can apply the LASSO as before.\n",
    "\n",
    "What we did before was technically not good practice since the variables were on different scales.  Certain variables would be (unfairly) penalized more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a LASSO model on the standardized data\n",
    "\n",
    "lasso_model = Lasso(alpha = 10000)\n",
    "lasso_model.fit(X_tr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that now we can meaningful compare the importance of\n",
    "## different features, since they're on the same scale\n",
    "\n",
    "## But it's now difficult to interpret the coefficients\n",
    "## We would need to translate back to the original feature scales by dividing\n",
    "## each coefficient by the original column's standard deviation\n",
    "\n",
    "list(zip(X_train.columns, lasso_model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Tuning Regularization Strength via Validation\n",
    "\n",
    "Here we will attempt to find the \"best\" value of the regularization strength alpha for this feature and target set and the LASSO model. We'll use simple validation (single train/valid split) as our model selection method.\n",
    "\n",
    "We will first decide on a vector of \"candidate\" alpha values.  Then, for each candidate value, we run the following steps:\n",
    "\n",
    "> 1. Fit a LASSO model on the training data\n",
    "> 2. Using the newly trained model, make predictions on the validation data\n",
    "> 3. Run evaluation metrics on validation\n",
    "\n",
    "Then we plot how the errors change for the different values of alpha, and see where alpha minimizes our error metric on the validation data. This value of alpha is the one we would select for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphalist = 10**(np.linspace(-2,2,200))\n",
    "err_vec_val = np.zeros(len(alphalist))\n",
    "err_vec_train = np.zeros(len(alphalist))\n",
    "\n",
    "for i,curr_alpha in enumerate(alphalist):\n",
    "\n",
    "    # note the use of a new sklearn utility: Pipeline to pack\n",
    "    # multiple modeling steps into one fitting process \n",
    "    steps = [('standardize', StandardScaler()), \n",
    "             ('lasso', Lasso(alpha = curr_alpha))]\n",
    "\n",
    "    pipe = Pipeline(steps)\n",
    "    pipe.fit(X_train.loc[:,selected_columns].values, y_train)\n",
    "    \n",
    "    val_set_pred = pipe.predict(X_val.loc[:,selected_columns].values)\n",
    "    err_vec_val[i] = mae(y_val, val_set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the curve of validation error as alpha changes\n",
    "\n",
    "plt.plot(np.log10(alphalist), err_vec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the minimum error achieved on the validation set \n",
    "## across the different alpha values we tried\n",
    "\n",
    "np.min(err_vec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the value of alpha that gave us the lowest error\n",
    "alphalist[np.argmin(err_vec_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Regularization Tuning Exercise**: \n",
    "\n",
    "Repeat the model selection workflow above (simple validation), but using a ridge model instead of a LASSO model. Based on the evidence you gather, do you think that a ridge or LASSO model has better predictive power on this dataset?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automated Regularization Strength Tuning via Cross-validation \n",
    "\n",
    "### Using LassoCV to find the best alpha via Cross-Validation\n",
    "In the previous section, we found the best alpha value by comparing the performance on a single validation set.  An even better, though more computationally intensive method, is to do a full cross-validation when comparing the different alphas. Fortunately, the `LassoCV` in sklearn handles this \"under the hood\". You pass the `LassoCV` the list of alphas and the number of folds to use for Cross-Validation.  \n",
    "\n",
    "It will do the following for each candidate value of alpha:\n",
    "\n",
    "> 1. Run cross-validation and score the result\n",
    "> 2. Find the value of alpha that gave the best CV score\n",
    "> 3. Fit a final model on all the data using the best value of alpha it just found\n",
    "\n",
    "Then you can use the `predict` method of the model just as with all of our previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the data as before\n",
    "std = StandardScaler()\n",
    "std.fit(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the Predictors on both the train and test set\n",
    "X_tr = std.transform(X_train.values)\n",
    "X_te = std.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cross validation, find the best alpha, refit the model on all the data with that alpha\n",
    "\n",
    "alphavec = 10**np.linspace(-2,2,200)\n",
    "\n",
    "lasso_model = LassoCV(alphas = alphavec, cv=5)\n",
    "lasso_model.fit(X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the best alpha value it found - not far from the value\n",
    "# selected using simple validation\n",
    "lasso_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the (standardized) coefficients found\n",
    "# when it refit using that best alpha\n",
    "list(zip(X_train.columns, lasso_model.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the new model\n",
    "test_set_pred = lasso_model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the MAE and R^2 on the test set using this model\n",
    "mae(y_test, test_set_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, test_set_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Regularization Tuning with CV Exercise**: \n",
    "\n",
    "Repeat the model selection workflow above (CV), but using the RidgeCV model instead of the LassoCV model. Based on the evidence you gather, do you think that a ridge or LASSO model has better predictive power on this dataset?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. _Bonus_: Using the LARS Path to Study Feature Importance \n",
    "\n",
    "This is a tool used to visualize *all* of the LASSO models across a range of different alpha values.  At the far left is the value of alpha where the penalty on coefficients is *so* onerous, that it just sets all of the coefficients to zero.  At the far right is when there is no penalty, and corresponds to the values of the coefficients that you would get from a \"vanilla\" linear regression.\n",
    "\n",
    "So each vertical slice corresponds to the coefficients you would get at a particular setting of alpha.  The black dotted lines indicate where a new variable \"enters\" the model (that is, its coefficient changes from 0 to non-zero).\n",
    "\n",
    "This is a good way to see which variables are most influential and how their strengths change as you change the value of alpha. Intuitively, the features that enter the model (nonzero coefficients) earliest in the path are the ones that the model treats as most essential, that it doesn't want to live without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale the variables\n",
    "std = StandardScaler()\n",
    "std.fit(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = std.transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Note: lars_path takes numpy matrices, not pandas dataframes\n",
    "\n",
    "print(\"Computing regularization path using the LARS ...\")\n",
    "alphas, _, coefs = lars_path(X_tr, y_train.values, method='lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the LARS path\n",
    "\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.legend(X_train.columns)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
