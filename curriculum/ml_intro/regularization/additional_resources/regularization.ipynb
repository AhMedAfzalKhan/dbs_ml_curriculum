{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 2 & 3 Compatibility\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression with sklearn\n",
    "As we've discussed, **regularization** is basically a blanket term for any technique that alters a model to prevent **overfitting**.  In linear regression, this is generally done by adding a term to the **Cost function** that is increasing in the absolute value of the coefficients.  The upshot of this is that models with large coefficients are implicitly punished by increasing their cost function, and the underlying mathematics of regression seek to choose the model that minimizes the value of the cost function.\n",
    "\n",
    "There are 2 very common regularization techniques for regression, called **\"Ridge Regression\"** and **\"Lasso Regression\"**\n",
    "\n",
    "##### Ridge Regression\n",
    "[Ridge Regression](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) adds the L2 norm (square root of the sum of squares) of the coefficients to the cost function.\n",
    "\n",
    "##### Lasso Regression\n",
    "[Lasso Regression](http://scikit-learn.org/stable/modules/linear_model.html#lasso) adds the L1 norm (sum of absolute values) of the coefficients to the cost function.\n",
    "\n",
    "##### ElasticNet Regression\n",
    "[ElasticNet Regression](http://scikit-learn.org/stable/modules/linear_model.html#elastic-net) simply combines both by adding **both** the **L1** and **L2** norm seeking the best of both worlds.\n",
    "\n",
    "sklearn has specific model types for both of the above regressions which we will see some of below.  Essentially, there is a \"punishment parameter\" alpha that these models try several different values of with cross-validation and then they return the value and model that yields the best generalization.  The beauty of this is that it all (optionally) happens completely under the covers!  We will use **RidgeCV** later in the exercises to perform regression with automatic Regularization and Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression Revisited\n",
    "To see how to use Regularized Regression we're going to revisit our generated data which we were using to fit polynomial regressions.\n",
    "\n",
    "#### Reload the Data\n",
    "Let's reload the `X` and `y` numpy arrays as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "npz = np.load('../../week02-luther1/03-regression_statsmodels/data/poly_data.npz')\n",
    "# Retrieve each array\n",
    "X = npz['arr_0']\n",
    "y = npz['arr_1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Overfitting: The need for Regularization\n",
    "Now that we have our data again we're going to train some increasingly complex (increasing degree) polynomials and watch what happens to the training and test errors as complexity increases.  Let's take a look at the data as we did before except this time we'll split into a train and test set first via `train_test_split`.\n",
    "\n",
    "Don't worry about the `sklearn` details of this right now, the **focus is on regularization**.  Essentially, we're training our model against one set and then testing it against some previously unknown set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to see what happens as we increase the degree of our polynomials and fit each one.  The below code will generate a plot of the error as a function of polynomial degree for degree 1-9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step through degrees from 0 to 9 and store the training and test (generalization) error.\n",
    "train_error = np.empty(10)\n",
    "test_error = np.empty(10)\n",
    "for degree in range(10):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    train_error[degree] = mean_squared_error(y_train, est.predict(X_train))\n",
    "    test_error[degree] = mean_squared_error(y_test, est.predict(X_test))\n",
    "\n",
    "# Plot the training and test errors against degree\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(10), train_error, color='green', label='train')\n",
    "plt.plot(np.arange(10), test_error, color='red', label='test')\n",
    "plt.ylim((0.0, 1e0))\n",
    "plt.ylabel('log(mean squared error)')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice?**\n",
    "\n",
    "The higher the degree of the polynomial (our proxy for model complexity), the lower the training error. The testing error decreases too, but it eventually reaches its minimum at a degree of three and then starts increasing at a degree of seven. \n",
    "\n",
    "This is a visual demonstration of ***overfitting***: the model is already so complex that it fits the idiosyncrasies of our training data, idiosyncrasies which limit the model's ability to generalize (as measured by the testing error).\n",
    "\n",
    "In the above example, the optimal choice for the degree of the polynomial approximation would be between three and six. So when we get some data, we could fit a bunch of polynomials and then choose the one that minimizes MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Regularized Regression\n",
    "\n",
    "#### Hand picking polynomials is hard work, and data scientists are lazy so....\n",
    "...we would like a method that eliminates the need to manually select the degree of the polynomial: we can add a constraint to our linear regression model that constrains the magnitude of the coefficients in the regression model. This constraint is called the regularization term and the technique is often called shrinkage in the statistical community because it shrinks the coefficients towards zero. In the context of polynomial regression, constraining the magnitude of the regression coefficients effectively is a smoothness assumption: by constraining the L2 norm of the regression coefficients we express our preference for smooth functions rather than wiggly functions.\n",
    "\n",
    "A popular regularized linear regression model is Ridge Regression. This adds the L2 norm of the coefficients to the ordinary least squares objective:\n",
    "\n",
    "  $J(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\beta^T \\mathbf{x}_i')^2 + \\alpha \\|\\boldsymbol\\beta\\|_2$\n",
    "\n",
    "where $\\boldsymbol\\beta$ is the vector of coefficients including the intercept term and $\\mathbf{x}_i'$ is the i-th feature fector including a dummy feature for the intercept. The L2 norm term is weighted by a regularization parameter ``alpha``: if ``alpha=0`` then you recover the Ordinary Least Squares regression model. The larger the value of ``alpha`` the higher the smoothness constraint.\n",
    "\n",
    "Below you can see the approximation of a [``sklearn.linear_model.Ridge``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) estimator fitting a polynomial of degree nine for various values of ``alpha`` (left) and the corresponding coefficient loadings (right). The smaller the value of ``alpha`` the higher the magnitude of the coefficients, so the functions we can model can be more and more wiggly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing in our helper functions\n",
    "\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "def plot_approximation(est, ax, label=None):\n",
    "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
    "    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "    ax.scatter(X, y, s=100)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
    "    ax.set_ylim((-2, 2))\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(loc='upper right',frameon=True)\n",
    "\n",
    "x_plot = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Set up a figure and axes for 8 plots, 2 per row for 4 rows\n",
    "fig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n",
    "\n",
    "# A helper function to plot the absolute value of the coefficients on the right-hand column plot\n",
    "def plot_coefficients(est, ax, label=None, yscale='log'):\n",
    "    coef = est.steps[-1][1].coef_.ravel()\n",
    "    if yscale == 'log':\n",
    "        ax.semilogy(np.abs(coef), marker='o', label=label)\n",
    "        ax.set_ylim((1e-1, 1e8))\n",
    "    else:\n",
    "        ax.plot(np.abs(coef), marker='o', label=label)\n",
    "    ax.set_ylabel('abs(coefficient)')\n",
    "    ax.set_xlabel('coefficients')\n",
    "    ax.set_xlim((1, 9))\n",
    "\n",
    "# Try out 4 different values of the RidgeRegression parameter alpha and watch how the resulting models change\n",
    "# With higher values of alpha, more complex (more wiggly) models will be more punished and thus less likely\n",
    "degree = 9\n",
    "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now with the LASSO...\n",
    "\n",
    "In the above example we used Ridge Regression, a regularized linear regression technique that puts an [L2 norm](http://mathworld.wolfram.com/L2-Norm.html) penalty on the regression coefficients. As mentioned above, another popular regularization technique is the LASSO, a technique which puts an [L1 norm](http://mathworld.wolfram.com/L1-Norm.html) penalty instead. The difference between the two is that the LASSO leads to sparse solutions, driving most coefficients to zero, whereas Ridge Regression leads to dense solutions, in which most coefficients are non-zero.\n",
    "\n",
    "Let's check out the same process using the LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create only 2 plot rows, only trying 2 alphas\n",
    "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot the results next to the coefficient values for each of hte 2 alphas\n",
    "degree = 9\n",
    "alphas = [1e-3, 1e-2]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Lasso(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for the Lasso many of the coefficients are zero.  When you choose an sklearn model that encapsulates both Regularization and Cross-Validation (like [**RidgeCV**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to be used below for instance) sklearn will choose the best value from several options of ***alpha*** by cross-validating in whatever way you decide.\n",
    "\n",
    "##### And with ElasticNet...\n",
    "Remember this tries to find the best of both worlds, with both L1 and L2 regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create only 2 plot rows, only trying 2 alphas\n",
    "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot the results next to the coefficient values for each of hte 2 alphas\n",
    "degree = 9\n",
    "alphas = [1e-3, 1e-2]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), ElasticNet(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='ElasticNet(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is some sort of hybrid of the look/feel of the Lasso and Ridge!  \n",
    "\n",
    "Congratulations, you understand regularization!\n",
    "\n",
    "**Remember:** regularization is a general concept.  It's any adjustment to the actual modeling step that tries to inherently control for overfitting, most often by augmenting the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's get back to the cars ~ Import the car dataframe via the picklefile again\n",
    "\n",
    "cars=pd.read_pickle('../../week02-luther1/03-regression_statsmodels/cars2frame.pkl')\n",
    "cars.head(1)\n",
    "\n",
    "y=cars['log_price']\n",
    "X=cars.drop(['log_price','price'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we saw before, here is our baseline r^2 score\n",
    "lr=LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Automatic Cross-Validation with sklearn\n",
    "So far we've just been using train/test splits for cross-validation and we haven't let sklearn do the work for us.  We also haven't even touched regularization.  So here we're going to do both, and we're going to do it in just 3 lines!\n",
    "\n",
    "First, import sklearn.linear_model.RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a `RidgeCV` object with the parameter `cv` set to 10.  \n",
    "\n",
    "What this will do is perform [10-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation).  The CV in RidgeCV means that is has this capability built in.  Because this is a Ridge Regressor, it also has regularization built in.  You can check out the [RidgeCV Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to see the default values it uses for `alpha` (they are configurable as a parameter of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv = RidgeCV(cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now \n",
    "# Is there an easier way to do train/test/validation and Ridge Regression altogether?  Of course there is!\n",
    "\n",
    "rcv.fit(X, y)\n",
    "rcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  We've already improved our baseline model and with cross-validation and regularization we have an estimate of our model's future performance that we should feel reasonably comfortable reporting to our boss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STUDENT SECTION: \n",
    "\n",
    "# Using Ridge, Lasso and/or ElasticNet together with cross-validation, \n",
    "# Tune your alpha parameter to determine which alpha minimizes Mean-squared Error\n",
    "# Plot your results \n",
    "\n",
    "## Hint (before you go too far): see the note here about L1 & L2 \n",
    "# http://scikit-learn.org/stable/modules/preprocessing.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
