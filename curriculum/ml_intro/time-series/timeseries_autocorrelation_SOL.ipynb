{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrelation: Exploring Rossmann Drug Store Sales Data\n",
    "### Jonathan Balaban\n",
    "\n",
    "This lab sets the foundation for ARIMA modeling. We will get to know our dataset, cover basic theory, and explore Python methods for managing time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "- Explore Rossmann data\n",
    "- Introduce technical concepts related to time series and ARIMA modeling \n",
    "- Calculate moving averages\n",
    "- Analyze Autocorrelations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context:\n",
    "\n",
    "> Key Definition: Time series data is any data that is captured across time.\n",
    "\n",
    "We have captured daily sales data for [Rossmann](https://en.wikipedia.org/wiki/Rossmann_(company)) (German mart/pharmacy, like CVS). If we can use past sales data to predict future sales, we have autocorrelation among the feature \"Sales\". This is a solid foundation for modeling and we can determine how far back our model should look. The past two days might be insightful, but if we're predicting Monday sales, so might the previous two Mondays, etc.\n",
    "\n",
    "However, if we go back too far, we complicate our model, and as expected, long ago data will not be very correlated with current sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data usually contains more than meets the eye, and can often be decomposed into trend, seasonal, and random fluctuation components.\n",
    "\n",
    "![Decomposition](http://rstatistics.net/wp-content/uploads/2014/09/Multiplicative-Decomposition-of-Time-series.png)\n",
    "\n",
    "- Trends\n",
    "    - Up\n",
    "    - Down\n",
    "    - Flat\n",
    "    - Larger trends can be made up of smaller trends\n",
    "    - There is no defined timeframe for what constitutes a trend; it depends on your goals\n",
    "- Seasonal Effects\n",
    "    - Weekend retail sales spikes\n",
    "    - Holiday shopping\n",
    "    - Energy requirement changes with annual weather patterns\n",
    "    - Note: twitter spikes when news happens are not seasonal; they aren't regular and predictable\n",
    "- Random Fluctuations\n",
    "    - The human element\n",
    "    - Aggregations of small influencers\n",
    "    - Observation errors\n",
    "    - The smaller this is in relation to Trend and Seasonal, the better we can predict the future\n",
    "    \n",
    "Time series models fall into [two camps](http://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics#HOW%20DO%20I%20KNOW%20WHICH%20DECOMPOSITION):\n",
    "- Additive\n",
    "    - Data = Trend + Seasonal + Random\n",
    "    - What we will be using for our modeling\n",
    "- Multiplicative\n",
    "    - Data = Trend x Seasonal x Random\n",
    "    - As easy to fit as Additive if we take the log\n",
    "    - log(Data) = log(Trend x Seasonal x Random)\n",
    "\n",
    "We should use multiplicative models when the percentage change of our data is more important than the absolute value change (e.g. stocks, commodities); as the trend rises and our values grow, we see amplitude growth in seasonal and random fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Modeling Process\n",
    "Time series model selection is driven by the Trend and Seasonal components of our raw data. The general approach for analysis looks like this:\n",
    "\n",
    "1. Plot the data and determine Trends and Seasonality\n",
    "    1. Difference the data (multiple times if needed) to remove trends for [certain model applications](https://en.wikipedia.org/wiki/Stationary_process)\n",
    "    1. Stationairity is needed for ARMA models\n",
    "1. Determine if we have additive or multiplicative data patterns\n",
    "1. Select the appropriate algorithm based on the chart below\n",
    "1. Determine if model selection is correct with these tools\n",
    "    - Ljung-Box Test\n",
    "    - Residual Errors (Normal Distribution with zero mean and constant variance-homoskedastic)\n",
    "    - Autocorrelation Function (ACF)\n",
    "    - Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Algorithm | Trend | Seasonal | Correlations\n",
    "---|---|---|---\n",
    "ARIMA | X |X|X\n",
    "SMA Smoothing |X||\n",
    "Simple Exponential Smoothing |X||\n",
    "Seasonal Adjustment |X|X|\n",
    "Holt's Exponential Smoothing |X||\n",
    "Holt-Winters |X|X|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages and data\n",
    "import pandas as pd, numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('data/rossmann.csv', skipinitialspace=True, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are most interested in `Date` column that contains date of sales per store; convert to `DateTime` and set as index\n",
    "# pull year and month as features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "data['Year'] = data.index.year\n",
    "data['Month'] = data.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dates to ascending and view\n",
    "data.sort_index(inplace=True)\n",
    "data.head()\n",
    "# notice Monday is DayOfWeek 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check index for missing days\n",
    "\n",
    "# first find the number of unique indices\n",
    "len(data.index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then calculate the delta in time plus one (inclusive of first day)\n",
    "data.index.max() - data.index.min()\n",
    "\n",
    "# counting the first day, we have a match: no missing dates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe and EDA\n",
    "data.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data to open dates\n",
    "df = data[data.Open==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table of average sales with rows=month and cols as promotion or not\n",
    "pt = df.pivot_table(index='Month', columns='Promo', values='Sales')\n",
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average sales by month and promo\n",
    "pt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average customers by month and promo\n",
    "df.pivot_table(index='Month', columns='Promo', values='Customers').plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df of store 1 open day sales\n",
    "store1 = df[df.Store == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare sales on holidays, we can compare the sales using box-plots, which allows us to compare the distribution of sales on holidays against all other days. On state holidays the store is closed (and as a nice sanity check there are 0 sales), and on school holidays the sales are relatively similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do school holidays affect sales?\n",
    "sns.catplot(\n",
    "    x='SchoolHoliday',\n",
    "    y='Sales',\n",
    "    data=store1,\n",
    "    kind='box');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does day of week affect sales?\n",
    "sns.catplot(\n",
    "    hue='SchoolHoliday',\n",
    "    x='DayOfWeek',\n",
    "    y='Sales',\n",
    "    data=store1,\n",
    "    kind='box',\n",
    "    height=8);\n",
    "# notice closed on Sundays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to identify larger-scale trends in our data. How did sales change from 2014 to 2015? Were there any particularly interesting outliers in terms of sales or customer visits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot store 1 sales when open\n",
    "store1.Sales.plot(figsize=(18,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot store 1 customer count when open\n",
    "store1.Customers.plot(figsize=(18,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure how much the sales are correlated with each other, we want to compute the _autocorrelation_ of the 'Sales' column. In pandas, we do this we with the `autocorr` function.\n",
    "\n",
    "`autocorr` takes one argument, the `lag` - which is how many prior data points should be used to compute the correlation. If we set the `lag` to 1, we compute the correlation between every point and the point directly preceding it, while setting `lag` to 10, computes the correlation between every point and the point 10 days earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample all store data to average daily sales\n",
    "daily_average_sales = df.Sales.resample('D').mean()\n",
    "\n",
    "daily_average_sales.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check autocorrelation for previous two weeks\n",
    "for i in range(1,16):\n",
    "    print(i, daily_average_sales.autocorr(lag=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many days after counts as a year, since some days are closed\n",
    "len(data.index.unique()) - len(daily_average_sales.index.unique())\n",
    "\n",
    "# across all stores, some are always open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check yearly autocorr\n",
    "for i in range(360,371):\n",
    "    print(i, daily_average_sales.autocorr(lag=i))\n",
    "    \n",
    "# note the spike around 364-365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot autocorrelation for different lags using pandas\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "autocorrelation_plot(daily_average_sales)\n",
    "plt.xlim(0,35);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving/Rolling Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to investigate trends over time in sales, as always, we will start by computing simple aggregates.  We want to know what the mean and median sales were for each month and year.\n",
    "\n",
    "In Pandas, this is performed using the `resample` command, which is very similar to the `groupby` command. It allows us to group over different time intervals.\n",
    "\n",
    "We can use `data.resample` and provide as arguments:\n",
    "    - The level on which to roll-up to, 'D' for day, 'W' for week, 'M' for month, 'A' for year\n",
    "    - What aggregation to perform: 'mean', 'median', 'sum', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample original sales data total by week\n",
    "weekly_average_sales = df.Sales.resample('W').mean()\n",
    "\n",
    "weekly_average_sales.plot(figsize=(20,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While identifying the weekly averages are useful, we often want to compare the sales data of a date to a smaller window. To understand holidays sales, we don't want to compare late December with the entire month, but perhaps a few days surrounding it. We can do this using rolling averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find rolling daily mean\n",
    "\n",
    "daily_average_sales.rolling(window=3, center=True).mean().plot(figsize=(20,7));\n",
    "\n",
    "# this gives us a bit of smoothing to exclude extreme events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rolling` has these important parameters:\n",
    "- the first is the series to aggregate\n",
    "- `window` is the number of days to include in the average\n",
    "- `center` is whether the window should be centered on the date or use data prior to that date\n",
    "- `freq` level to roll-up averages to (as in `resample`). `D` for day, `M` for month or `A` for year, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of plotting the full timeseries, we can plot the rolling mean instead, which smooths random changes in sales as well as removing outliers, helping us identify larger trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 5-week centered rolling mean\n",
    "\n",
    "weekly_average_sales.rolling(window=5, center=True).mean().plot(figsize=(18,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Window functions\n",
    "Pandas `rolling` is an example of Pandas window function capabilities. Window functions operate on a set of N consecutive rows (a window) and produce an output: mean, median, min, max, sum, etc.\n",
    "\n",
    "Another common one is `diff`, which takes the difference over time. `pd.diff` takes one arugment, `periods`, which is how many prior rows to use for the difference. This is *extremely* useful for getting our data stationary!\n",
    "\n",
    "In addition to `rolling` functions, Pandas provides a similar collection of `expanding` functions, which instead of a window, use all values up until that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate diff for open store 1 data\n",
    "store1['Diff'] = store1['Sales'].diff(periods=1)\n",
    "store1.Diff.plot(figsize=(20,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 30-day rolling mean\n",
    "store1.Sales.rolling(30).mean().plot(figsize=(20,6));\n",
    "# notice the smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average daily expanding sales\n",
    "daily_average_sales.expanding(min_periods=1).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does expanding sales at the last row work as assumed?\n",
    "print(daily_average_sales.expanding(min_periods=1).mean().iloc[-1])\n",
    "print\n",
    "print(daily_average_sales.mean())\n",
    "\n",
    "# yes, minus rounding errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of sales by month and compare the effect of promotions\n",
    "sns.catplot(\n",
    "    col='Open',\n",
    "    hue='Promo',\n",
    "    x='Month',\n",
    "    y='Sales',\n",
    "    data=store1, \n",
    "    kind='box');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are sales more correlated with the prior day, day of week, last month, or last year?\n",
    "\n",
    "# remake \"daily_average_sales\" but include Open data\n",
    "average_daily_sales = df[['Sales', 'Open']].resample('D').mean()\n",
    "\n",
    "print('Correlation with last day: {}'.format(average_daily_sales['Sales'].autocorr(lag=1)))\n",
    "print('Correlation with last week: {}'.format(average_daily_sales['Sales'].autocorr(lag=7)))\n",
    "print('Correlation with last month: {}'.format(average_daily_sales['Sales'].autocorr(lag=30)))\n",
    "print('Correlation with last year: {}'.format(average_daily_sales['Sales'].autocorr(lag=365)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 15 day rolling mean of customers in the stores\n",
    "average_daily_sales.Sales.rolling(window=15).mean().plot(figsize=(18,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the date with largest drop in average sales from previous cycles: daily, weekly, etc.\n",
    "total_daily = df[['Sales', 'Open']].resample('D').sum()\n",
    "total_daily['Diff'] = total_daily.Sales.diff(periods=1)\n",
    "\n",
    "total_daily.sort_values(by='Diff').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the total sales up until Dec. 2014\n",
    "total_daily_sales = df.Sales.resample('D').sum()\n",
    "total_daily_sales.expanding().sum()['2014-12'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When were the largest differences between 15-day moving/rolling averages?\n",
    "x = total_daily_sales.rolling(window=15).mean().diff(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values\n",
    "x.sort_values(ascending=True).head(10)\n",
    "\n",
    "# Unsurprisingly, they occur at the beginning of every year after the holiday season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Differencing Works\n",
    "\n",
    "Let's see how we can standardize our data (remove trends and changes in variance) via different levels of differencing. This is important, as standardized data is a requirement for ARIMA modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a play dataframe from 1-10, regular and squared to test differencing works\n",
    "play = pd.DataFrame([[x for x in range(1,11)], [x**2 for x in range(1,11)]]).T\n",
    "play.columns = ['original', 'squared']\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take diffs of our two series until they're stationary (mean doesn't change for sub-windows)\n",
    "play.original.diff()\n",
    "# this is similar to taking a first-order derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play.squared.diff().diff()\n",
    "# notice we need to difference twice on an exponential trend, and every time we do, we lose a bit of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "\n",
    "- Explore Rossmann data\n",
    "- Introduce technical concepts related to time series and ARIMA modeling \n",
    "- Calculate moving averages\n",
    "- Analyze Autocorrelations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
