{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Lab\n",
    "\n",
    "Run in `py36` environment:\n",
    "```\n",
    "conda create -n py36 python=3.6 pandas numpy seaborn matplotlib scipy patsy statsmodels jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Video Guide to this Lab](https://youtu.be/3Gw1E_BJU58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Explain time series decomposition\n",
    "1. Describe additive and multiplicative data\n",
    "1. Stationarize data\n",
    "1. Fit AR, MA, and ARIMA models on prepared data\n",
    "1. Interpret model parameters and performance\n",
    "1. Visualize trends and forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "We often refer to our input features in machine learning as \"dimensions\". On that note, there's a dimension that pervades almost everything we do and observe as humans. It's the fourth dimension we experience every waking moment: time. But time is quite unlike other data we capture, and often requires unique machine learning approaches. These models and approaches are fairly established in  R and a few other languages, but have more recently immigrated to Python.\n",
    "\n",
    "In regession and classification, we use features (collected during a cross-sectional study/survey/measurement) to predict an outcome. The model and parameters represent part of the underlying relationship between features and outcome. But what if we run out of funds to cross-section (it's possible), or need to predict future outcomes for which the features aren't measurable or don't yet exist?\n",
    "\n",
    "### Examples of time series data and modeling (constant time interval):\n",
    "- Hedge fund prediction of stock and index movements\n",
    "- Long and short-term weather forecasting\n",
    "- Business budgeting and trend analysis\n",
    "- Health vitals monitoring\n",
    "- Traffic flows and logistic optimization modeling\n",
    "- Can you think of others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series data usually contains more than meets the eye, and can often be decomposed into trend, seasonal, and random fluctuation components.\n",
    "\n",
    "![Decomposition](http://rstatistics.net/wp-content/uploads/2014/09/Multiplicative-Decomposition-of-Time-series.png)\n",
    "\n",
    "- Trends\n",
    "    - Up\n",
    "    - Down\n",
    "    - Flat\n",
    "    - Larger trends can be made up of smaller trends\n",
    "    - There is no defined timeframe for what constitutes a trend; it depends on your goals\n",
    "- Seasonal Effects\n",
    "    - Weekend retail sales spikes\n",
    "    - Holiday shopping\n",
    "    - Energy requirement changes with annual weather patterns\n",
    "    - Note: twitter spikes when news happens are not seasonal; they aren't regular and predictable\n",
    "- Random Fluctuations\n",
    "    - The human element\n",
    "    - Aggregations of small influencers\n",
    "    - Observation errors\n",
    "    - The smaller this is in relation to Trend and Seasonal, the better we can predict the future\n",
    "    \n",
    "Time series models fall into [two camps](http://www.abs.gov.au/websitedbs/D3310114.nsf/home/Time+Series+Analysis:+The+Basics#HOW%20DO%20I%20KNOW%20WHICH%20DECOMPOSITION):\n",
    "- Additive\n",
    "    - Data = Trend + Seasonal + Random\n",
    "    - What we will be using for our modeling\n",
    "- Multiplicative\n",
    "    - Data = Trend x Seasonal x Random\n",
    "    - As easy to fit as Additive if we take the log\n",
    "        - log(Data) = log(Trend x Seasonal x Random)\n",
    "\n",
    "We should use multiplicative models when the percentage change of our data is more important than the absolute value change (e.g. stocks, commodities); as the trend rises and our values grow, we see amplitude growth in seasonal and random fluctuations. If our seasonality and fluctuations are stable, we likely have an additive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Modeling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series model selection is driven by the Trend and Seasonal components of our raw data. The general approach for analysis looks like this:\n",
    "\n",
    "1. Plot the data and determine Trends and Seasonality\n",
    "    1. Difference or take the log of the data (multiple times if needed) to remove trends for [certain model applications](https://en.wikipedia.org/wiki/Stationary_process)\n",
    "    1. Stationairity is needed for ARMA models\n",
    "1. Determine if we have additive or multiplicative data patterns\n",
    "1. Select the appropriate algorithm based on the chart below\n",
    "1. Determine if model selection is correct with these tools\n",
    "    - Ljung-Box Test\n",
    "    - Residual Errors (Normal Distribution with zero mean and constant variance-homoskedastic, i.i.d)\n",
    "    - Autocorrelation Function (ACF)\n",
    "    - Partial Autocorrelation Function (PACF)\n",
    "\n",
    "Algorithm | Trend | Seasonal | Correlations\n",
    "---|---|---|---\n",
    "ARIMA | X |X|X\n",
    "SMA Smoothing |X||\n",
    "Simple Exponential Smoothing |X||\n",
    "Seasonal Adjustment |X|X|\n",
    "Holt's Exponential Smoothing |X||\n",
    "Holt-Winters |X|X|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to achieve and test for stationarity"
   ]
  },
  {
   "attachments": {
    "Mean_nonstationary.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean of the series is not a function of time:\n",
    "!![Mean_nonstationary.png](attachment:Mean_nonstationary.png)\n",
    "\n",
    "- The variance of the series is not a function of time (homoscedasticity):\n",
    "![](data/Var_nonstationary.png)\n",
    "\n",
    "- The covariance at different lags is not a function of time:\n",
    "![](data/Cov_nonstationary.png)\n",
    "\n",
    "[From A Complete Tutorial on Time Series Modeling in R](https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/)\n",
    "\n",
    "- [Info on stationarity](http://www.investopedia.com/articles/trading/07/stationary.asp)\n",
    "- Plotting Rolling Statistics\n",
    "    - Plot the moving average/variance and see if it changes with time. This visual technique can be done on different windows, but isn't as rigorously defensible as the test below.\n",
    "- Dickey-Fuller Test\n",
    "    - Statistical tests for checking stationarity; the null hypothesis is that the TS is non-stationary. If our test statistic is below an `alpha` value, we _can_ reject the null hypothesis and say that the series is stationary.\n",
    "    \n",
    "    $$ Y_t = \\rho * Y_{t-1} + \\epsilon_t $$\n",
    "    \n",
    "    $$ Y_t - Y_{t-1} = (\\rho - 1) Y_{t - 1} + \\epsilon_t $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Differencing Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a play dataframe from 1-10 (linear and squared) to test how differencing works\n",
    "play = pd.DataFrame([[x for x in range(1,11)], [x**2 for x in range(1,11)]]).T\n",
    "play.columns = ['original', 'squared']\n",
    "play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationarize linear series (mean and variance doesn't change for sub-windows)\n",
    "play.original.diff()\n",
    "# this is similar to taking a first-order derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationarize squared series\n",
    "play.squared.diff().diff()\n",
    "# notice we need to difference twice on an exponential trend, and every time we do, we lose a bit of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationarize squared with log\n",
    "np.log(play.squared)\n",
    "# somewhat works, not as dramatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep and EDA\n",
    "\n",
    "We'll be looking at [monthly average temperatures between 1907-1972](https://datamarket.com/data/set/22o4/mean-monthly-temperature-1907-1972#!ds=22o4&display=line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, recast columns if needed, convert to datetime\n",
    "monthly_temp = pd.read_csv('./data/mean-monthly-temperature-1907-19.csv', skipfooter=2, \n",
    "                           infer_datetime_format=True, header=0, index_col=0, names=['month', 'temp'], engine='python')\n",
    "#monthly_temp.temp = monthly_temp.temp.astype(float)\n",
    "monthly_temp.index = pd.to_datetime(monthly_temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_temp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe\n",
    "monthly_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to annual and plot each\n",
    "annual_temp = monthly_temp.resample('A').mean()\n",
    "monthly_temp.plot();\n",
    "annual_temp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both on same figure\n",
    "plt.plot(monthly_temp)\n",
    "plt.plot(annual_temp);\n",
    "# note, easier to see trends on resampled or moving average charts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# plot with plotly (optional): might need a plotly account and key\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "data = [go.Scatter(x=annual_temp.index, y=annual_temp.temp)]\n",
    "py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot binned yearly segments using resample method\n",
    "monthly_temp.resample('A').temp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violinplot months to determine variance and range\n",
    "#sns.set()\n",
    "sns.violinplot(x=monthly_temp.index.month, y=monthly_temp.temp)\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Farenheit Temperature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these datasets stationary? We can look at a few things per the list above, including a visual check (there seems to be a small upward trend in the annual, too hard to tell for monthly), a standard deviation check on various differences (smallest one is usually most stationary), and the formal Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check montly deviations for various diffs\n",
    "print(monthly_temp.temp.std())\n",
    "print(monthly_temp.temp.diff().std())\n",
    "print(monthly_temp.temp.diff().diff().std()) # theoretically lowest, but one above is close enough\n",
    "print(monthly_temp.temp.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check annual deviations for various diffs\n",
    "print(annual_temp.temp.std()) # looks stationary as is\n",
    "print(annual_temp.temp.diff().std())\n",
    "print(annual_temp.temp.diff().diff().std())\n",
    "print(annual_temp.temp.diff().diff().diff().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Dickey-Fuller Test (DFT) function\n",
    "import statsmodels.tsa.stattools as ts\n",
    "def dftest(timeseries):\n",
    "    dftest = ts.adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','Lags Used','Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    #Determine rolling statistics\n",
    "    rolmean = timeseries.rolling(window=12).mean()\n",
    "    rolstd = timeseries.rolling(window=12).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DFT on monthly\n",
    "dftest(monthly_temp.temp)\n",
    "# p-value allows us to reject a unit root: data is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DFT on annual\n",
    "dftest(annual_temp.temp)\n",
    "# p-value allows us to reject a unit root: data is stationary\n",
    "\n",
    "# here's an example of non-stationary with DFT results\n",
    "# dftest(np.exp(annual_temp.temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key trading concepts in the quantitative toolbox is that of mean reversion. This process refers to a time series that displays a tendency to revert to its historical mean value. Mathematically, such a (continuous) time series is referred to as an Ornstein-Uhlenbeck process. This is in contrast to a random walk (Brownian motion: discuss this on chess board!), which has no \"memory\" of where it has been at each particular instance of time. The mean-reverting property of a time series can be exploited in order to produce better predictions.\n",
    "\n",
    "A continuous mean-reverting time series can be represented by an Ornstein-Uhlenbeck stochastic differential equation:\n",
    "\n",
    "$$ dx_t=\\theta(\\mu - x_t)dt+\\sigma dW_t $$\n",
    " \n",
    "Where $\\theta$\n",
    " is the rate of reversion to the mean, $\\mu$\n",
    " is the mean value of the process, $\\sigma$\n",
    " is the variance of the process and $W_t$\n",
    " is a Wiener Process or Brownian Motion.\n",
    "\n",
    "In a discrete setting the equation states that the change of the price series in the next time period is proportional to the difference between the mean price and the current price, with the addition of Gaussian noise.\n",
    "\n",
    "https://www.quantstart.com/articles/Basics-of-Statistical-Mean-Reversion-Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA with Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter [Autoregressive Integrated Moving Average (ARIMA)](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) modeling. When we have autocorrelation between outcomes and their ancestors, we will see a theme, or relationship in the outcome plot. This relationship can be modeled in its own way, allowing us to predict the future with a confidence level commensurate to the strength of the relationship and the proximity to known values (prediction weakens the further out we go).\n",
    "\n",
    "- [ARIMA in R](https://www.otexts.org/fpp/8/5)\n",
    "- [Duke ARIMA Guide](https://people.duke.edu/~rnau/411arim2.htm)\n",
    "- [Great explanation on MA in practice](http://stats.stackexchange.com/questions/164824/moving-average-ma-process-numerical-intuition)\n",
    "\n",
    "### Autoregressive Models\n",
    "\n",
    "![AR Model](./data/ar.svg)\n",
    "\n",
    "Autocorrelation: a variable's correlation with itself at different lags.\n",
    "\n",
    "For second-order stationary (both mean and variance: $\\mu_t = \\mu$ and $\\sigma_t^2 = \\sigma^2$ for all $t$) data, autocovariance is expressed as a function only of the time lag $k$:\n",
    "\n",
    "$$ \\gamma_k = E[(x_t-\\mu)(x_{t+k} - \\mu)] $$\n",
    "  \n",
    "Therefore, the autocorrelation function is defined as:\n",
    "\n",
    "$$ \\rho_k = \\frac{\\gamma_k}{\\sigma^2} $$\n",
    "  \n",
    "We use the plot of these values at different lags to determine optimal ARIMA parameters. Notice how `phi` changes the process.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/ArTimeSeries.svg/685px-ArTimeSeries.svg.png)\n",
    "By Tomaschwutz - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=14740378\n",
    "\n",
    "\n",
    "Some things to note:\n",
    "1. AR models propagate shocks infinitely\n",
    "1. Current random error is the `epsilon` error term\n",
    "1. If a process depends on previous values of itself then it is an AR process. If it depends on previous errors than it is an MA process.\n",
    "1. AR processes will exhibit exponential decay in ACR and a cut-off in PACR\n",
    "\n",
    "### Moving Average Models (This is NOT a Simple/Weighted/Exponential Moving Average)\n",
    "\n",
    "![MA Model](./data/ma.svg)\n",
    "\n",
    "Some things to note:\n",
    "1. MA models do not propagate shocks infinitely; they die after `q` lags\n",
    "1. All previous errors up to a lag are rolled into the `epsilon` error term for that period\n",
    "1. If a process depends on previous values of itself then it is an AR process. If it depends on previous errors than it is an MA process.\n",
    "1. MA processes will exhibit exponential decay in PACR and a cut-off in ACR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a moving-average model is conceptually a linear regression of the current value of the series against current and previous (unobserved) white noise error terms or random shocks. The random shocks at each point are assumed to be mutually independent and to come from the same distribution, typically a normal distribution, with location at zero and constant scale.\n",
    "\n",
    "Interpretation\n",
    "The moving-average model is essentially a finite impulse response filter applied to white noise, with some additional interpretation placed on it. The role of the random shocks in the MA model differs from their role in the autoregressive (AR) model in two ways. First, they are propagated to future values of the time series directly: for example, \n",
    "ε\n",
    "t\n",
    "−\n",
    "1\n",
    "\\varepsilon _{{t-1}} appears directly on the right side of the equation for \n",
    "X\n",
    "t\n",
    "X_{t}. In contrast, in an AR model \n",
    "ε\n",
    "t\n",
    "−\n",
    "1\n",
    "\\varepsilon _{{t-1}} does not appear on the right side of the \n",
    "X\n",
    "t\n",
    "X_{t} equation, but it does appear on the right side of the \n",
    "X\n",
    "t\n",
    "−\n",
    "1\n",
    "X_{{t-1}} equation, and \n",
    "X\n",
    "t\n",
    "−\n",
    "1\n",
    "X_{{t-1}} appears on the right side of the \n",
    "X\n",
    "t\n",
    "X_{t} equation, giving only an indirect effect of \n",
    "ε\n",
    "t\n",
    "−\n",
    "1\n",
    "\\varepsilon _{t-1} on \n",
    "X\n",
    "t\n",
    "X_{t}. Second, in the MA model a shock affects \n",
    "X\n",
    " X values only for the current period and q periods into the future; in contrast, in the AR model a shock affects \n",
    "X\n",
    " X values infinitely far into the future, because \n",
    "ε\n",
    "t\n",
    "\\varepsilon _{t} affects \n",
    "X\n",
    "t\n",
    "X_{t}, which affects \n",
    "X\n",
    "t\n",
    "+\n",
    "1\n",
    "X_{{t+1}},etc.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Moving-average_model\n",
    "\n",
    "-----------\n",
    "Consider a series consisting of the closing price (adjusted for splits and dividends) of a stock on consecutive days. Each day's closing price is derived from a trend (e.g., linear in time) plus the weighted effects of the daily shocks from prior days. \n",
    "\n",
    "Presumably, the effect of the shock at day t-1 will have a stronger influence on the price at day t than will the shock at day t-2, etc. Thus, logically, the stock's closing price at day t will reflect the trend value on day t plus a constant (less than 1) times the weighted sum of the shocks up through day t-1 (i.e., the error term at day t-1)(MA1), possibly plus a constant (less than 1) times the weighted sum of the shocks up through day t-2 (i.e., the error term at day t-2)(MA2), ..., plus the novel shock at day t (white noise).\n",
    "\n",
    "This kind of model seems appropriate for modelling series like the stock market, where the error term at day t represents the weighted sum of prior and current shocks, and defines an MA process.\n",
    "\n",
    "https://stats.stackexchange.com/questions/107834/under-what-circumstances-is-an-ma-process-or-ar-process-appropriate\n",
    "\n",
    "Many ways to predict future TS data, but SARIMA is performant. Other methods that we won't go into:\n",
    "- Vector autoregressions (VARs)\n",
    "- Gaussian state space models – often called structural time series or unobserved component models\n",
    "- GARCH\n",
    "- Generalized Autoregressive Score (GAS)\n",
    "- Kalman Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARIMA\n",
    "![SARIMA Form](./data/sarima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper plot function for visualization\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "def plots(data, lags=None):\n",
    "    layout = (1, 3)\n",
    "    raw  = plt.subplot2grid(layout, (0, 0))\n",
    "    acf  = plt.subplot2grid(layout, (0, 1))\n",
    "    pacf = plt.subplot2grid(layout, (0, 2))\n",
    "    \n",
    "    data.plot(ax=raw, figsize=(12, 6))\n",
    "    smt.graphics.plot_acf(data, lags=lags, ax=acf)\n",
    "    smt.graphics.plot_pacf(data, lags=lags, ax=pacf)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper plot for monthly temps\n",
    "plots(monthly_temp, lags=24);\n",
    "# open Duke guide for visual\n",
    "# we note a 12-period cycle (yearly) with suspension bridge design, so must use SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Box-Jenkins Method](https://en.wikipedia.org/wiki/Box–Jenkins_method)\n",
    "\n",
    "ACF Shape|Indicated Model\n",
    "---|---\n",
    "Exponential, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model.\n",
    "Alternating positive and negative, decaying to zero|Autoregressive model. Use the partial autocorrelation plot to help identify the order.\n",
    "One or more spikes, rest are essentially zero|Moving average model, order identified by where plot becomes zero.\n",
    "Decay, starting after a few lags|Mixed autoregressive and moving average (ARMA) model.\n",
    "All zero or close to zero|Data are essentially random.\n",
    "High values at fixed intervals|Include seasonal autoregressive term.\n",
    "No decay to zero|Series is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might need to install dev version for statespace functionality\n",
    "#!pip install git+https://github.com/statsmodels/statsmodels.git\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# fit SARIMA monthly based on helper plots\n",
    "sar = sm.tsa.statespace.SARIMAX(monthly_temp.temp, order=(1,1,0), seasonal_order=(0,1,0,12), trend='c').fit()\n",
    "sar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot resids\n",
    "plots(sar.resid, lags=40);\n",
    "\n",
    "# Thought process: (Reference: https://people.duke.edu/~rnau/arimrule.htm)\n",
    "# 010010 is overdiff by AIC and negative ACR, but 000010 is a big underdiff with better AIC\n",
    "# we pick 000010,12 and Trend='c' per rule4/5\n",
    "\n",
    "# now look at seasonal, notice negative ACR spike at 12: per rule 13, we add a SMA term\n",
    "# big drop to 4284 AIC\n",
    "# looks like ACR looks good at seasonal lags, so we move back to ARIMA portion\n",
    "\n",
    "# rule6 says we're a bit underdiff, so we add AR=3 based on PACF: 4261 AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual diagnostics\n",
    "sar.plot_diagnostics(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "monthly_temp['forecast'] = sar.predict(start = 750, end= 820, dynamic=30)  \n",
    "monthly_temp[730:][['temp', 'forecast']].plot();\n",
    "\n",
    "# per the documentation, we start predicting at period 750, and start a dynamic forecast at 750 + 30\n",
    "# this means we use our predictions as \"ground truth\" for following predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests\n",
    "\n",
    "\n",
    "- [Normality (Jarque-Bera)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_normality)\n",
    "    - Null hypothesis is normally distributed residuals (good, plays well with RMSE and similar error metrics)\n",
    "\n",
    "- [Serial correlation (Ljung-Box)](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_serial_correlation)\n",
    "    - Null hypothesis is no serial correlation in residuals (independent of each other)\n",
    "\n",
    "- [Heteroskedasticity](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity.html#statsmodels.tsa.statespace.sarimax.SARIMAXResults.test_heteroskedasticity)\n",
    "    - Tests for change in variance between residuals.\n",
    "    - The null hypothesis is of no heteroskedasticity. That means different things depending on which alternative is selected:\n",
    "        - Increasing: Null hypothesis is that the variance is not increasing throughout the sample; that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "        - Decreasing: Null hypothesis is that the variance is not decreasing throughout the sample; that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample.\n",
    "        - Two-sided (default): Null hypothesis is that the variance is not changing throughout the sample. Both that the sum-of-squares in the earlier subsample is not greater than the sum-of-squares in the later subsample and that the sum-of-squares in the later subsample is not greater than the sum-of-squares in the earlier subsample.\n",
    "\n",
    "- [Durbin Watson](https://en.wikipedia.org/wiki/Durbin–Watson_statistic)\n",
    "    - Tests autocorrelation of residuals: we want between 1-3, 2 is ideal (no serial correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and run statistical tests on model\n",
    "norm_val, norm_p, skew, kurtosis = sar.test_normality('jarquebera')[0]\n",
    "lb_val, lb_p = sar.test_serial_correlation(method='ljungbox')[0]\n",
    "het_val, het_p = sar.test_heteroskedasticity('breakvar')[0]\n",
    "# we want to look at largest lag for Ljung-Box, so take largest number in series\n",
    "# there's intelligence in the method to determine how many lags back to calculate this stat\n",
    "lb_val = lb_val[-1]\n",
    "lb_p = lb_p[-1]\n",
    "durbin_watson = sm.stats.stattools.durbin_watson(sar.filter_results.standardized_forecasts_error[0, sar.loglikelihood_burn:])\n",
    "\n",
    "print('Normality: val={:.3f}, p={:.3f}'.format(norm_val, norm_p));\n",
    "print('Ljung-Box: val={:.3f}, p={:.3f}'.format(lb_val, lb_p));\n",
    "print('Heteroskedasticity: val={:.3f}, p={:.3f}'.format(het_val, het_p));\n",
    "print('Durbin-Watson: d={:.2f}'.format(durbin_watson))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on autofit methods\n",
    "R has an autoARIMA function (and other automagic methods) that gridsearches/optimizes our model hyperparameters for us. Over time, more of these goodies are porting to Python (e.g. statsmodels.tsa.x13 import x13_arima_select_order). While there's nothing wrong with utilizing these resources, the _human makes the final determination!_ Don't become over-reliant on these methods, especially early on when you are grasping the underlying mechanics and theory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Explain time series decomposition\n",
    "1. Describe additive and multiplicative data\n",
    "1. Stationarize data\n",
    "1. Fit AR, MA, and ARIMA models on prepared data\n",
    "1. Interpret model parameters and performance\n",
    "1. Visualize trends and forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing\n",
    "\n",
    "- Which areas of time series modeling are most interesting?\n",
    "- Which concepts are most challenging?\n",
    "- How will you apply these concepts to your work or personal projects?\n",
    "- Have you used other packages or tools that you've found helpful?\n",
    "\n",
    "For more time series packages, check out the [Facebook Prophet](https://github.com/facebookincubator/prophet) and [Pyflux](http://www.pyflux.com) projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
