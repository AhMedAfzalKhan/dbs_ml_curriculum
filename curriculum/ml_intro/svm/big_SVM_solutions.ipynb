{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:02:28.093294Z",
     "start_time": "2018-12-06T22:02:25.221112Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import sklearn\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One last thing\n",
    "\n",
    "- SVMs have an interesting history that we haven't quite had the time to discuss in detail. \n",
    "- In a nutshell, \n",
    "    - SVMs were one of the most popular off-the-shelf models during the late 1990s and early 2000s.\n",
    "    - Two recent trends have led SVMs to be less used:\n",
    "        - The success of Deep Learning for nonlinear classification problems.\n",
    "        - The use of ever-bigger datasets.\n",
    "        \n",
    "Let's zero in on this last point and then discuss the how and why SVMs can remain useful today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T20:28:09.153381Z",
     "start_time": "2018-12-05T20:28:09.118744Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Big Data Woes\n",
    "\n",
    "SVMs run into two problems with big data\n",
    "\n",
    "\n",
    "- First, **Test complexity**\n",
    "    - SVM test complexity depends on the kernel but is generally $O(n_\\text{sv}d)$ where $n_\\text{sv}$ is the number of support vectors and $d$ is the number of dimensions. <sup> [1](https://arxiv.org/pdf/1403.0736.pdf) </sup>\n",
    "    - In the worst case, all training examples are chosen as support vectors and SVM has the same test complexity as other non-parametric models like KNN: $O(nd)$. \n",
    "    - This is bad. Ideally more training data will always make our model better, but in this case it slows SVMs down.\n",
    "\n",
    "- Second, the **train complexity**\n",
    "    - For linear SVMs, the train complexity is between $O(n^2)$ and $O(n^3)$. <sup>[2](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)</sup>\n",
    "    - For kernels-SVMs, train complexity depends on many factors but shares a lower bound at $O(n^2)$ and can often be much worse in practice. <sup>[2](https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf)</sup>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:16:28.576309Z",
     "start_time": "2018-12-06T22:16:28.522306Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pal = dict(enumerate(sns.color_palette(\"husl\", 4)))\n",
    "\n",
    "def plot_decision_boundary(pred_func, x, y, ax=None, points=1e3, pal=pal, margin_func=None):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = subplots()\n",
    "    \n",
    "    y_pred = pred_func(x)\n",
    "    score = metrics.accuracy_score(y_pred.flatten(), y.flatten())\n",
    "\n",
    "    sns.scatterplot(x=x[:, 0], y=x[:, 1], hue=y, alpha=.5, edgecolor=None, palette=pal, ax=ax)\n",
    "\n",
    "    side_pts = int(sqrt(points))\n",
    "\n",
    "    x0_min, x0_max = ax.get_xlim()\n",
    "    x1_min, x1_max = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x0_min, x0_max, num=side_pts),\n",
    "        np.linspace(x1_min, x1_max, num=side_pts))\n",
    "\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    ax.text(\n",
    "        (x0_min + x0_max) / 2,\n",
    "        (x1_min + x1_max) / 2,\n",
    "        f\"acc: {score:.1%}\",\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"black\"))\n",
    "    \n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.2, colors=list(pal.values()), zorder=-1)\n",
    "    \n",
    "    if not (margin_func is None): \n",
    "        Z = margin_func(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "        \n",
    "        # plot decision boundary and margins\n",
    "        ax.contour(xx, yy, Z, colors='k', levels=[-1, 1], alpha=0.5,\n",
    "                   linestyles=['--','--'], zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:16:29.179030Z",
     "start_time": "2018-12-06T22:16:29.174349Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, HBox, VBox, interactive_output, widgets\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:16:29.823639Z",
     "start_time": "2018-12-06T22:16:29.783496Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# dataset generation from https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py\n",
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 500\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=.5, noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=0)\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=0)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    centers=2,\n",
    "    cluster_std=[.7, 2.5],\n",
    "    random_state=random_state)\n",
    "\n",
    "datasets_str = [\"noisy_circles\", \"noisy_moons\", \"blobs\", \"aniso\", \"varied\"]\n",
    "kernels = ['linear', 'poly', 'rbf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:16:30.561683Z",
     "start_time": "2018-12-06T22:16:30.235830Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def kernel_widget(d_name, k_name, degree=3, c=1.0, gamma=.5):\n",
    "\n",
    "    x, y = eval(d_name)\n",
    "\n",
    "    svm_model = svm.SVC(\n",
    "        kernel=k_name, gamma=gamma, degree=degree, C=c, cache_size=1000, max_iter=1000)\n",
    "    svm_model.fit(x, y)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        svm_model.predict,\n",
    "        x,\n",
    "        y,\n",
    "        points=1e4,\n",
    "        margin_func=svm_model.decision_function)\n",
    "\n",
    "    sv_scatter = plt.scatter(\n",
    "        svm_model.support_vectors_[:, 0],\n",
    "        svm_model.support_vectors_[:, 1],\n",
    "        marker=\"x\",\n",
    "        c=\"black\",\n",
    "        zorder=-1,\n",
    "        label=\"sv\")\n",
    "    \n",
    "    n_sv = svm_model.support_vectors_.shape[0]\n",
    "    \n",
    "    plt.title(f\"{k_name}-svm on {d_name}. d={degree}, c={c:g}, gamma={gamma:g}. {n_sv} support vectors\" )\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:16:31.059644Z",
     "start_time": "2018-12-06T22:16:30.913572Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "d_name_w = widgets.RadioButtons(\n",
    "    options=datasets_str, description='Dataset:', disabled=False)\n",
    "k_name_w = widgets.RadioButtons(\n",
    "    options=kernels, description='Kernel:', disabled=False)\n",
    "degree_w = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Degree: ',\n",
    "    continuous_update=False)\n",
    "c_w = widgets.FloatLogSlider(\n",
    "    value=1,\n",
    "    base=10,\n",
    "    min=-2,  # max exponent of base\n",
    "    max=6,  # min exponent of base\n",
    "    step=0.5,  # exponent step\n",
    "    description='C: ',\n",
    "    continuous_update=False)\n",
    "gamma_w = widgets.FloatLogSlider(\n",
    "    value=.5,\n",
    "    base=2,\n",
    "    min=-5,  # max exponent of base\n",
    "    max=6,  # min exponent of base\n",
    "    step=0.5,  # exponent step\n",
    "    description='Gamma: ',\n",
    "    continuous_update=False)\n",
    "\n",
    "ui = HBox([\n",
    "    VBox([d_name_w]),\n",
    "    VBox([k_name_w]),\n",
    "    VBox([degree_w, c_w, gamma_w])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:16:32.047900Z",
     "start_time": "2018-12-06T22:16:31.519146Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "out = interactive_output(kernel_widget, {\"d_name\":d_name_w,\n",
    "                                   \"k_name\": k_name_w, \n",
    "                                   \"degree\":degree_w, \n",
    "                                   \"c\": c_w, \n",
    "                                   \"gamma\":gamma_w})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Addressing Test Complexity\n",
    "\n",
    "- If the main problem with test complexity is the number of support vectors, we can try to tackle that directly.\n",
    "    - One important strategy here is to tune the model so that it learns fewer support vectors.\n",
    "\n",
    "Note: You can also directly limit the number of support vectors with [`NuSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise: Tuning can reduce the number of support vectors\n",
    "\n",
    "Let's revisit the previous widget. This time, see if you can determine how to tune the model to reduce the number of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:26:45.460800Z",
     "start_time": "2018-12-06T22:26:45.407647Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f66e659eea498cb9ec2496cb069bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(RadioButtons(description='Dataset:', options=('noisy_circles', 'noisy_moons', 'b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af131c392135490f98b01e404a25d951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 432x288 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution:\n",
    "\n",
    "For all kernels: \n",
    "  - Increasing $C$ tends to reduce the number of support vectors.\n",
    "  - Reducing training error (by finding a kernel or parameters that better capture the data) tends to reduce the number of support vectors.\n",
    "    \n",
    "For poly and RBF:\n",
    "  - Increasing `gamma` tends to reduce the number of support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Addressing Train Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Gradient Descent for SVMs\n",
    "\n",
    "- Recall from the [SGD lesson](../../project-02/stochastic-gradient-descent) that training a model with SGD has an impressive time complexity: In the limit, the time to train a model with SGD is $O(1)$ in terms of the number of examples. 💁‍\n",
    "- When dealing with very large datasets, it's a good idea to train with SGD wherever possible. \n",
    "\n",
    "\n",
    "- Can we train SVMs with SGD?\n",
    "    - We *can* train SVMs with SGD,\n",
    "    - but there's *one* problem: this only works for **linear kernels**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Enter Kernel approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But first a kernel trick review\n",
    "\n",
    "As we discussed before, the kernel trick implicitly calculates a product $\\phi(\\mathbf{x})^\\top\\phi(\\mathbf{x}')$ between two transformed sets of examples.\n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\phi(\\mathbf{x})^\\top\\phi(\\mathbf{x}')$$\n",
    "\n",
    "Without the kernel trick, if we wish to transform our data into a high-degree polynomial space, we would need to explicitly transform each point using $\\phi(\\mathbf{x})$ and then we would need to multiply the transformed examples $\\phi(\\mathbf{x})^\\top\\phi(\\mathbf{x}')$. The kernel trick allows us to arrive at the result without first performing the transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Kernel approximation\n",
    "\n",
    "Kernel approximation is another method for efficiently using kernels. Instead we find an approximation $\\phi_a(\\mathbf{x}) \\sim \\phi(\\mathbf{x})$ and use that to *explicitly* transform the examples before we feed them to a linear SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, we reproduce the above widgets, showing a comparison between the kernel trick and kernel approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:27:37.384927Z",
     "start_time": "2018-12-06T22:27:37.343348Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import kernel_approximation, linear_model, pipeline\n",
    "\n",
    "def approx_widget(d_name, k_name, degree=3, c=1.0, gamma=.5, n_components=100):\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "\n",
    "    x, y = eval(d_name)\n",
    "\n",
    "    svm_model = svm.SVC(\n",
    "        kernel=k_name,\n",
    "        gamma=gamma,\n",
    "        degree=degree,\n",
    "        C=c,\n",
    "        cache_size=1000,\n",
    "        max_iter=1000)\n",
    "    svm_model.fit(x, y)\n",
    "\n",
    "    svm_sgd = linear_model.SGDClassifier(max_iter=500, tol=1e-3)\n",
    "    if d_name != \"linear\":\n",
    "        svm_sgd = pipeline.make_pipeline(\n",
    "            kernel_approximation.Nystroem(\n",
    "                kernel=k_name,\n",
    "                gamma=gamma,\n",
    "                degree=degree,\n",
    "                n_components=n_components), svm_sgd)\n",
    "        \n",
    "    svm_sgd.fit(x, y)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        svm_model.predict,\n",
    "        x,\n",
    "        y,\n",
    "        ax=axes[0],\n",
    "        points=1e4,\n",
    "        margin_func=svm_model.decision_function)\n",
    "    sv_scatter = axes[0].scatter(\n",
    "        svm_model.support_vectors_[:, 0],\n",
    "        svm_model.support_vectors_[:, 1],\n",
    "        marker=\"x\",\n",
    "        c=\"black\",\n",
    "        zorder=-1,\n",
    "        label=\"sv\")\n",
    "    n_sv = svm_model.support_vectors_.shape[0]\n",
    "    axes[0].set_title(f\"Exact SVM. {n_sv} support vectors\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        svm_sgd.predict,\n",
    "        x,\n",
    "        y,\n",
    "        ax=axes[1],\n",
    "        points=1e4,\n",
    "        margin_func=svm_sgd.decision_function)\n",
    "    axes[1].set_title(f\"SGD SVM. {n_components} components\")\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"{k_name}-svm on {d_name}. d={degree}, c={c:g}, gamma={gamma:g}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:27:41.606526Z",
     "start_time": "2018-12-06T22:27:41.459845Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "d_name_w = widgets.RadioButtons(\n",
    "    options=datasets_str, description='Dataset:', disabled=False)\n",
    "k_name_w = widgets.RadioButtons(\n",
    "    options=kernels, description='Kernel:', disabled=False)\n",
    "degree_w = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Degree: ',\n",
    "    continuous_update=False)\n",
    "c_w = widgets.FloatLogSlider(\n",
    "    value=1,\n",
    "    base=10,\n",
    "    min=-2,  # max exponent of base\n",
    "    max=6,  # min exponent of base\n",
    "    step=0.5,  # exponent step\n",
    "    description='C: ',\n",
    "    continuous_update=False)\n",
    "gamma_w = widgets.FloatLogSlider(\n",
    "    value=.5,\n",
    "    base=2,\n",
    "    min=-5,  # max exponent of base\n",
    "    max=6,  # min exponent of base\n",
    "    step=0.5,  # exponent step\n",
    "    description='Gamma: ',\n",
    "    continuous_update=False)\n",
    "n_components_w = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=50,  # max exponent of base\n",
    "    max=500,  # min exponent of base\n",
    "    step=50,  # exponent step\n",
    "    description='N Components: ',\n",
    "    continuous_update=False)\n",
    "ui = HBox([\n",
    "    VBox([d_name_w]),\n",
    "    VBox([k_name_w]),\n",
    "    VBox([degree_w, c_w, gamma_w, n_components_w])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T20:49:16.042933Z",
     "start_time": "2018-12-05T20:49:15.858852Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Confirm that SVMs with the with SGD and kernel approximation are similar to kernel SVMs. \n",
    "\n",
    "- What differences do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:27:48.957590Z",
     "start_time": "2018-12-06T22:27:47.990383Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36211b4b540946c491558233edc3be97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(RadioButtons(description='Dataset:', options=('noisy_circles', 'noisy_moons', 'b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fda6d3d62b4c7bbcfe3fa784260007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = interactive_output(approx_widget, {\"d_name\":d_name_w,\n",
    "                                   \"k_name\": k_name_w, \n",
    "                                   \"degree\":degree_w, \n",
    "                                   \"c\": c_w, \n",
    "                                   \"gamma\":gamma_w,\n",
    "                                   \"n_components\": n_components_w})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T20:49:16.042933Z",
     "start_time": "2018-12-05T20:49:15.858852Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "- What differences do you notice?\n",
    "    - There are no support vectors. `SGDClassifier` learns a weight matrix according to the weight-based linear function: $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + B$.\n",
    "    - The hyperparameter `C` doesn't affect the SGD SVM. This is because `SGDClassifier` uses a separate parameter `alpha` which, depending on the kernel, is roughly $\\alpha \\sim \\frac{1}{c}$.\n",
    "    - Accuracy tends to increase as the number of components increases. However, be careful because the time to train also increases with the number of components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T20:49:16.042933Z",
     "start_time": "2018-12-05T20:49:15.858852Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main points\n",
    "\n",
    " \n",
    "- When using SVMs on big data we may\n",
    "    - Tune hyperparameters to reduce the number of support vectors.\n",
    "    - Use SGD along with kernel approximation to efficiently train on large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
