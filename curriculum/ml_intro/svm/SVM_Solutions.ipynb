{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "# standard code block #\n",
    "#######################\n",
    "\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "%pylab inline\n",
    "\n",
    "# sets backend to render higher res images\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "\n",
    "#######################\n",
    "#       imports       #\n",
    "#######################\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# import sklearn\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"title\"> Support Vector Machines </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Concepts\n",
    "\n",
    "- Parametric Models\n",
    "- Model Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parametric models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<u>**Parametric Models**</u> learn a finite (and typically fixed) number of parameters.\n",
    "\n",
    "- Example: <u>logistic regression</u> uses the prediction function $f(\\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$\n",
    "    - The weight matrix $\\mathbf{w}$ is fixed by the number of features used.\n",
    "\n",
    "\n",
    "- The good\n",
    "    - Fewer parameters to store in memory\n",
    "    - Useful when a strong bias is merited\n",
    "    - More interpretable\n",
    "- The bad\n",
    "    - Limited in what can in theory be learned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<u>**Nonparametric Models**</u> have no fixed set of parameters to learn. Often nonparametric models learn a potentially infinite set of parameters.\n",
    "\n",
    "- Example: <u>K Nearest Neighbors</u> \n",
    "    - KNN memorizes all observed examples. While KNN has no fixed parameters like logistic regression, the examples memorized by KNN could be considered parameters.\n",
    "\n",
    "\n",
    "- The good\n",
    "    - Quite flexible.\n",
    "    - Often come with guarantees. With enough examples and a limit on noise, KNN is guaranteed to learn most classification functions. \n",
    "- The bad\n",
    "    - (Potentially) infinite parameters means (potentially) infinite memory storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Linearity\n",
    "\n",
    "- See [Model Linearity lesson](../model-linearity/Model_Linearity.ipynb)\n",
    "\n",
    "- Linear functions are made up of two operations: matrix multiplication and addition.\n",
    "    - Linear Models are made up of only linear operations are unable to learn non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img class=\"big\" src=../model-linearity/images/lin_models.png>\n",
    "\n",
    "\n",
    "<img class=\"big\" src=../model-linearity/images/nonlin_models.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVMs\n",
    "\n",
    "Support Vector Machines are a hybrid model that achieves nice trade-offs in a few ways:\n",
    "\n",
    "- SVMs are *non-parametric*.\n",
    "    - **However**: SVMs are designed to only remember the examples they need.\n",
    "- SVMs are built on the same linear equation we've seen elsewhere: $f(\\mathbf{x}) = \\mathbf{w}^\\top\\mathbf{x} + b$.\n",
    "    - **However**: SVMs are able to learn non-linear functions by using the \"kernel trick\" (more on this later!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do SVMs work?\n",
    "\n",
    "## 1) Example-based linear function\n",
    "\n",
    "<small>Note: below, we'll distinguish between $\\mathbf{x}_{\\text{train}}$, our training examples, and $\\mathbf{x}_{\\text{test}}$, our testing examples.</small>\n",
    "\n",
    "The first insight of SVMs is to write our traditional linear equation in terms of a dot product between examples. \n",
    "\n",
    "Our original linear equation now looks like $f(\\mathbf{x}_{\\text{test}}) = \\mathbf{w}^\\top\\mathbf{x}_{\\text{test}} + b$. A model learns a weight matrix $\\mathbf{w}$ and a bias $b$ from the training data and then applies those to test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SVMs re-write this using the inner product between examples instead of a weight matrix:\n",
    "\n",
    "$$\\mathbf{w}^\\top\\mathbf{x}_{\\text{test}} + b = b + \\sum^{m}_{i=1}\\alpha_i\\mathbf{x}_{\\text{test}}^\\top \\mathbf{x}_{\\text{train}}[i,:]$$\n",
    "\n",
    "Here, we're learning a set of coefficients $\\mathbf{\\alpha} = [\\alpha_0, \\alpha_1, \\dots \\alpha_n]$, one for each of the $n$ training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: the details of how this new equation are arrived at are going to be skipped over for now. If you wish to explore more details on your own, this is called the [\"dual problem\"](https://en.wikipedia.org/wiki/Support_vector_machine#Dual)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo\n",
    "\n",
    "\n",
    "Let's prove that this example-based function still works. First, we'll re-do an exercise from the Model Linearity lesson, then we'll build a similar exercise for the example-based formula.  \n",
    "\n",
    "First, we generate a small blob dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "x, y = datasets.make_blobs(\n",
    "    n_samples=6, n_features=2, centers=[[-2, -2], [2, 2]], random_state=0)\n",
    "\n",
    "blob_df = pd.DataFrame({\"x0\": x[:,0], \"x1\": x[:,1], \"y\":y})\n",
    "\n",
    "sns.scatterplot(x=\"x0\", y=\"x1\", hue=\"y\", data=blob_df, alpha=.3, edgecolor=None)\n",
    "title(\"Two random Gaussian clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pal = dict(enumerate(sns.color_palette(\"husl\", 4)))\n",
    "\n",
    "\n",
    "def plot_decision_boundary(pred_func,\n",
    "                           x,\n",
    "                           y,\n",
    "                           ax=None,\n",
    "                           points=1e3,\n",
    "                           pal=pal,\n",
    "                           margin_func=None):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = subplots()\n",
    "\n",
    "    y_pred = pred_func(x)\n",
    "    score = metrics.accuracy_score(y_pred.flatten(), y.flatten())\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=x[:, 0],\n",
    "        y=x[:, 1],\n",
    "        hue=y,\n",
    "        alpha=.5,\n",
    "        edgecolor=None,\n",
    "        palette=pal,\n",
    "        ax=ax)\n",
    "\n",
    "    side_pts = int(sqrt(points))\n",
    "\n",
    "    x0_min, x0_max = ax.get_xlim()\n",
    "    x1_min, x1_max = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x0_min, x0_max, num=side_pts),\n",
    "        np.linspace(x1_min, x1_max, num=side_pts))\n",
    "\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.text(\n",
    "        (x0_min + x0_max) / 2,\n",
    "        x1_min + (x1_max - x1_min) * .1,\n",
    "        f\"acc: {score:.1%}\",\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"black\"))\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.2, colors=list(pal.values()), zorder=-1)\n",
    "\n",
    "    if not (margin_func is None):\n",
    "        Z = margin_func(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "        # plot decision boundary and margins\n",
    "        ax.contour(\n",
    "            xx,\n",
    "            yy,\n",
    "            Z,\n",
    "            colors='k',\n",
    "            levels=[-1, 1],\n",
    "            alpha=0.5,\n",
    "            linestyles=['--', '--'],\n",
    "            zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below is a function that calculates a linear classifier using a weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def lin_weight_func(x_test, w, b):\n",
    "    f_out = w.T.dot(x_test.T) + b\n",
    "    return (f_out > 0).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def weight_widget(w0, w1, b):\n",
    "    w = array([[w0], [w1]])\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        lambda x: lin_weight_func(x, w, b),\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y\"].values,\n",
    "        points=1e4)\n",
    "    plt.title(f\"Accuracy for $f(x) = [{w0}, {w1}]^\\\\top \\\\mathbf{{x}}$ + {b}\")\n",
    "    return (w0, w1, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: \n",
    "\n",
    "We will use the weight-based function to solve a small problem with an SVM. The function has been incorporated into a widget that appears below.\n",
    "\n",
    "You will need to find values for $\\mathbf{w}$ and $b$ that give the best classifier.\n",
    "\n",
    "$$\\underbrace{\\mathbf{w}^\\top\\mathbf{x}_\\text{test} + b}_\\text{Weight-based function}\n",
    "    = b + \\sum^{m}_{i=1}\\alpha_i\\mathbf{x}_{\\text{test}}^\\top \\mathbf{x}_{\\text{train}}[i,:]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, HBox, VBox, interactive_output\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interact(weight_widget,w0=(-10,10,1),w1=(-10,10,1),b=(-10,10,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution:\n",
    "\n",
    "One solution is $\\mathbf{w} = [1,1]$, $b=-2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's use the example-based method.\n",
    "\n",
    "This time, the function below executes an equivalent linear function using a weighted sum of dot products from training examples instead of a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def lin_ex_func(x_test, x_train, a, b):\n",
    "    n_test = x_test.shape[0]\n",
    "    sum_out = np.zeros((n_test, 1))\n",
    "    for i in range(x_train.shape[0]):\n",
    "        x_i = x_train[[i],:]\n",
    "        sum_out += a[i]*(x_test.dot(x_i.T))\n",
    "    f_out = sum_out + b\n",
    "    \n",
    "    return (f_out > 0).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def ex_widget(a0, a1, a2, a3, a4, a5, b):\n",
    "    a = array([a0, a1, a2, a3, a4, a5])\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        lambda x_test: lin_ex_func(x_test, x, a, b),\n",
    "        x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "        y=blob_df[\"y\"].values,\n",
    "        points=1e4)\n",
    "    for i in range(6):\n",
    "        plt.text(blob_df.loc[i,\"x0\"], blob_df.loc[i,\"x1\"], str(i))\n",
    "    plt.title(f\"Accuracy for a = [{a}], b = {b}\")\n",
    "    return (a0, a1, a2, a3, a4, a5, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "a_widgets = {}\n",
    "for i in range(6):\n",
    "    a_widgets[f\"a{i}\"]= widgets.FloatText(\n",
    "        value=0.0,\n",
    "        description=f\"a{i}\",\n",
    "        disabled=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:\n",
    "\n",
    "Now, we'll solve the same problem with an SVM using the equivalent example-based function. The function has been incorporated into a widget that appears below.\n",
    "\n",
    "You will need to find values for $\\mathbf{a}$ and $b$ that give the best classifier.\n",
    "\n",
    "$$\\mathbf{w}^\\top\\mathbf{x}_\\text{test} + b\n",
    "    = \\underbrace{\n",
    "        b + \\sum^{m}_{i=1}\\alpha_i\\mathbf{x}_{\\text{test}}^\\top \\mathbf{x}_{\\text{train}}[i,:]\n",
    "       }_\\text{Example-based function}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hints\n",
    "\n",
    "- The index of each example corresponds to an $a_i$. These have been labeled for you.\n",
    "- Start with all zeros.\n",
    "    - What is the effect of turning one example from $0\\rightarrow1$?\n",
    "    - What is the effect of turning one example from $0\\rightarrow-1$?\n",
    "    - What is the effect of adjusting $b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(ex_widget,b=widgets.FloatText(\n",
    "        value=0.0,\n",
    "        description=f\"b\",\n",
    "        disabled=False\n",
    "    ), **a_widgets);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Solution\n",
    "\n",
    "One solution that works here is $\\mathbf{a} = [0,0,0,1,-1,0]$, $b = -4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "- Go back to the example-based widget.\n",
    "- Compare two solutions that both get 100% accuracy. Do you see any advantages to either solution?  \n",
    "    - Solution 1: $a=[1, -1, 1, 1, -1, -1]$\n",
    "    - Solution 2: $a=[0, 0, 0, 1, -1, 0]$  \n",
    "    \n",
    "    \n",
    "- If we keep $b=0$ our model is perfectly accurate. We could actually use several different values for $b$ and get the same score. How should decide between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion Answers\n",
    "\n",
    "- Recall from the regression lesson that we can use Lasso to do feature selection. Lasso regression pushes the coefficient for many features to zero. This is a clue that our model might be better off if we omitted those zero features.\n",
    "- SVMs use a similar process for examples. Instead of saving all of the training examples, SVMs only need to save the ones with a non-zero $a_i$.\n",
    "    - The saved examples are called <u>**support vectors**</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.dropbox.com/s/lsbse60wi31lhl0/2018-12-06_11-59-43.png?raw=1)\n",
    "- Let's think back to discussions of cross validation. While the widget above is displaying training accuracy, what we really care about is how accurate a model will be on new data. Errors on new data are often called **generalization error**.\n",
    "- SVMs chose a $b$ that puts the greatest distance between examples and the decision boundary.\n",
    "    - This strategy is what is called a <u>**maximum margin**</u> classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# sklearn version\n",
    "\n",
    "Now, let's confirm that the model in `sklearn` works the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC(kernel=\"linear\")\n",
    "svm_model.fit(x, y)\n",
    "\n",
    "plot_decision_boundary(\n",
    "    svm_model.predict,\n",
    "    x=blob_df[[\"x0\", \"x1\"]].values,\n",
    "    y=blob_df[\"y\"].values,\n",
    "    points=1e4, margin_func=svm_model.decision_function)\n",
    "\n",
    "sv_scatter = plt.scatter(\n",
    "    svm_model.support_vectors_[:, 0],\n",
    "    svm_model.support_vectors_[:, 1],\n",
    "    marker=\"x\",\n",
    "    label=\"sv\",\n",
    "    c=\"black\",\n",
    "    zorder=-1)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"support vectors: \", svm_model.support_vectors_)\n",
    "print(\"coefficients: \", svm_model.dual_coef_)\n",
    "print(\"intercept: \", svm_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we see:\n",
    "- `svm_model.support_vectors_` contains the support vectors. These are the same ones we found in our own solution above. \n",
    "- `svm_model.dual_coef_` is what we called $\\mathbf{a}$ and `svm_model.intercept_` is what we called $b$. These are different in scale, but otherwise quite similar to what we found above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Need for Improvement\n",
    "\n",
    "So far, SVMs sound great, but we'll run into a few problems when we start to apply the model to real data:\n",
    "\n",
    "- A **maximum margin** classifier sounds great when data is clearly separable, but what happens when we have noisy data that can't be cleanly divided?\n",
    "- We have not yet discussed how SVMs can be used to learn nonlinear functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improvement 1: Soft Margin\n",
    "\n",
    "Today, all major SVM implementations use what is called a **soft margin**. Under this approach SVMs balance two priorities:\n",
    "\n",
    "1. finding the boundary with the largest margin between classes and\n",
    "2. finding the boundary with the smallest training error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Soft margin works by modifying the loss function to include a term that penalizes classification error. For this lesson, we won't be going into the details of how this loss function is arrived at, but it is reproduced below:\n",
    "\n",
    "$$\n",
    "C\\underbrace{\n",
    "    \\left[\\frac 1 n \\sum_{i=1}^n \\max\\left(0, 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i - b)\\right) \\right]\n",
    "}_\\text{error penalty}\n",
    "    + \n",
    "\\underbrace{\n",
    "    \\lVert \\mathbf{w} \\rVert^2\n",
    "}_\\text{margin penalty}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The \"margin penalty\" is what the SVMs we've discussed so far are optimizing. This term roughly corresponds to generalization error: the larger the margin, the better our model will tend to generalize.\n",
    "\n",
    "The \"error penalty\" term is zero if all examples are correctly classified and increases with the number and severity of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Soft Margin Demo\n",
    "\n",
    "Here we'll pull in the [Iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) to demo the soft-margin SVM on data that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# pulling in our iris dataset for a quick viz\n",
    "iris_data = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n",
    "iris_df[\"species\"] = iris_data.target\n",
    "\n",
    "# only use the last two classes\n",
    "iris_df = iris_df.query(\"species > 0\")\n",
    "iris_df.species = iris_df.species - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's the two features we'll look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x=\"petal length (cm)\",\n",
    "    y=\"petal width (cm)\",\n",
    "    hue=\"species\",\n",
    "    alpha=.5, data=iris_df, palette=pal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = iris_df[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = iris_df.species.values\n",
    "\n",
    "\n",
    "def c_widget(c):\n",
    "    svm_model = svm.SVC(C=c, kernel=\"linear\")\n",
    "\n",
    "    svm_model.fit(x, y)\n",
    "\n",
    "    plot_decision_boundary(svm_model.predict, x=x, y=y, points=1e4, margin_func=svm_model.decision_function)\n",
    "\n",
    "    sv_scatter = plt.scatter(\n",
    "        svm_model.support_vectors_[:, 0],\n",
    "        svm_model.support_vectors_[:, 1],\n",
    "        marker=\"x\",\n",
    "        c=\"black\",\n",
    "        zorder=-1, label=\"sv\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    h,l = ax.get_legend_handles_labels()\n",
    "    plt.legend(h, [\"Virginica\", \"Versicolor\", \"Support Vectors\"])\n",
    "    \n",
    "    plt.title(f\"Soft margin SVM with C = {c:g}\")\n",
    "    plt.xlabel(\"petal length\")\n",
    "    plt.ylabel(\"petal width\")\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion:\n",
    "\n",
    "Adjust the $C$ parameter. Describe how the model and its decision boundary are changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interact(\n",
    "    c_widget,\n",
    "    c=widgets.FloatLogSlider(\n",
    "        value=10,\n",
    "        base=10,\n",
    "        min=-2,  # max exponent of base\n",
    "        max=8,  # min exponent of base\n",
    "        step=0.5,  # exponent step\n",
    "        description='C',\n",
    "        continuous_update=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "- Higher $C$: tends to care more about increasing accuracy on the training set.\n",
    "- Lower $C$: tends to care more about reducing generalization error.\n",
    "- Very low $C$: results in all points being support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Improvement 2: The kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's a kernel? 🤷‍\n",
    "\n",
    "A **kernel** maps a function from its original domain to another domain where a problem might be easier to solve.\n",
    "\n",
    "Some interesting domains:\n",
    "- *Polynomial* this is similar to using the `PolynomialFeatures` tool in preprocessing. It allows the model to make predictions based on higher order polynomial transformations of our input features.\n",
    "- *RBF-Radial Basis Function* this is similar to selecting examples as prototypes of a class. The radial basis function decreases as a test point gets farther away from this prototype in any direction (thus \"radial\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How does this work?**\n",
    "\n",
    "Recall that earlier we rewrote the linear function to use dot products between examples instead of a weight matrix:\n",
    "\n",
    "$$f(\\mathbf{x}_\\text{test})= b + \\sum^{m}_{i=1}\\alpha_i\\mathbf{x}_{\\text{test}}^\\top \\mathbf{x}_{\\text{train}}[i,:]$$\n",
    "\n",
    "A kernel allows us to make use of a different domain without transforming the individual examples first. If we have a transformation on $\\mathbf{x}$, $\\phi(\\mathbf{x})$, the kernel function *implicitly* finds the dot product between transformed inputs—that is, **without actually performing the transformation explicitly**. \n",
    "\n",
    "$$K(\\mathbf{x}, \\mathbf{x}') = \\phi(\\mathbf{x})^\\top\\phi(\\mathbf{x}')$$\n",
    "\n",
    "<small>While there are certainly more details, that is as deep as we need to go for now.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Key Points**\n",
    "\n",
    "- Using a kernel allows us to use different domains (including nonlinear ones) efficiently.\n",
    "- The kernel trick works by *implicitly* finding the dot product between transformed inputs.\n",
    "    - Allows us to use a linear classifier to efficiently leverage domains with large (or even infinite in the case of RBF) feature spaces.\n",
    "    - Allows SVMs to learn nonlinear decision boundaries. 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see it in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# dataset generation from https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html#sphx-glr-auto-examples-cluster-plot-linkage-comparison-py\n",
    "\n",
    "n_samples = 500\n",
    "noisy_circles = datasets.make_circles(\n",
    "    n_samples=n_samples, factor=.5, noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=0)\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, centers=2, random_state=0)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    centers=2,\n",
    "    cluster_std=[.7, 2.5],\n",
    "    random_state=random_state)\n",
    "\n",
    "datasets_str = [\"noisy_circles\", \"noisy_moons\", \"blobs\", \"aniso\", \"varied\"]\n",
    "kernels = ['linear', 'poly', 'rbf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def kernel_widget(d_name, k_name, degree=3, c=1.0, gamma=.5):\n",
    "\n",
    "    x, y = eval(d_name)\n",
    "\n",
    "    svm_model = svm.SVC(\n",
    "        kernel=k_name, gamma=gamma, degree=degree, C=c, cache_size=1000, max_iter=1000)\n",
    "    svm_model.fit(x, y)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        svm_model.predict,\n",
    "        x,\n",
    "        y,\n",
    "        points=1e4,\n",
    "        margin_func=svm_model.decision_function)\n",
    "\n",
    "    sv_scatter = plt.scatter(\n",
    "        svm_model.support_vectors_[:, 0],\n",
    "        svm_model.support_vectors_[:, 1],\n",
    "        marker=\"x\",\n",
    "        c=\"black\",\n",
    "        zorder=-1,\n",
    "        label=\"sv\")\n",
    "    \n",
    "    n_sv = svm_model.support_vectors_.shape[0]\n",
    "    \n",
    "    plt.title(f\"{k_name}-svm on {d_name}. d={degree}, c={c:g}, gamma={gamma:g}. {n_sv} support vectors\" )\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "d_name_w = widgets.RadioButtons(\n",
    "    options=datasets_str, description='Dataset:', disabled=False)\n",
    "k_name_w = widgets.RadioButtons(\n",
    "    options=kernels, description='Kernel:', disabled=False)\n",
    "degree_w = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Degree: ',\n",
    "    continuous_update=False)\n",
    "c_w = widgets.FloatLogSlider(\n",
    "    value=1,\n",
    "    base=10,\n",
    "    min=-2,  # max exponent of base\n",
    "    max=6,  # min exponent of base\n",
    "    step=0.5,  # exponent step\n",
    "    description='C: ',\n",
    "    continuous_update=False)\n",
    "gamma_w = widgets.FloatLogSlider(\n",
    "    value=.5,\n",
    "    base=2,\n",
    "    min=-5,  # max exponent of base\n",
    "    max=6,  # min exponent of base\n",
    "    step=0.5,  # exponent step\n",
    "    description='Gamma: ',\n",
    "    continuous_update=False)\n",
    "ui = HBox([\n",
    "    VBox([d_name_w]),\n",
    "    VBox([k_name_w]),\n",
    "    VBox([degree_w, c_w, gamma_w])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=exceptions.ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion\n",
    "\n",
    "Test out the different kernels and datasets. Get a feel for what each parameter does. \n",
    "\n",
    "Describe in your own words\n",
    "- What sorts of functions do the poly and rbf kernels tend to learn?\n",
    "- What does the `gamma` parameter do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "out = interactive_output(kernel_widget, {\"d_name\":d_name_w,\n",
    "                                   \"k_name\": k_name_w, \n",
    "                                   \"degree\":degree_w, \n",
    "                                   \"c\": c_w, \n",
    "                                   \"gamma\":gamma_w})\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion Answers\n",
    "\n",
    "- What sorts of functions do the poly and rbf kernels tend to learn?\n",
    "\n",
    "\n",
    "The **poly kernel** tends to learn continuous, curved boundaries.The function implemented by the poly kernel is: \n",
    "    \n",
    "$$K(\\mathbf{x},\\mathbf{x}') = \\left(\\gamma\\mathbf{x}^\\top\\mathbf{x} + r \\right)^d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What sorts of functions do the poly and rbf kernels tend to learn?\n",
    "\n",
    "\n",
    "The **RBF kernel** tends to learn continuous, curved boundaries. The function implemented by the RBF kernel is: \n",
    "    \n",
    "$$K(\\mathbf{x},\\mathbf{x}') = e^{-\\gamma\\|\\mathbf{x}-\\mathbf{x}' \\|^2} $$\n",
    "\n",
    "\n",
    "What's happening under the hood is that the RBF kernel is comparing test points to each of the support vectors. When the test point is close to a support vector, that support vector's coefficient is weighted highly. When a test point is farther away, the coefficient is weighted lower. \n",
    "\n",
    "Note: you might recognize that this function is similar to the PDF of the Normal distribution, \n",
    "    \n",
    "$$f(x \\mid \\mu, \\sigma^2) =\\frac{1}{ \\sqrt{2\\pi\\sigma^2}} e^{\\frac{ (x-x')^2}{2\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion Answers\n",
    "\n",
    "- What does the `gamma` parameter do?\n",
    "\n",
    "\n",
    "Note that in the RBF kernel, $\\gamma \\sim \\frac{1}{\\sigma^2}$, so if we think of the RBF as a Normal distribution, increasing `gamma` is analogous to decreasing the standard deviation. \n",
    "\n",
    "In more qualitative terms:\n",
    "- decreasing `gamma` leads to smoother boundaries\n",
    "- increasing `gamma` leads to boundaries that follow the training data more tightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main points\n",
    "\n",
    "- SVMs are \n",
    "    - Linear classifiers similar to other linear models we've covered. However, SVMs are able to learn nonlinear functions.\n",
    "    - Non-parametric models like KNN. However, rather than memorizing all training data, SVMs only store the examples they need, called support vectors.\n",
    "- SVMs use the kernel trick to learn nonlinear functions. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
