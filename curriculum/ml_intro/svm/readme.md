---
date: w6d2
duration: 90
maintainer: artificialsoph
order: 10
title: SVM
---

# Sample Lesson Plan
- (60m) [SVMs](SVM_Solutions.ipynb)
- (30m) [big SVMs](big_SVM_Solutions.ipynb)

# Learning Objectives

Students can

- Write, in pseudocode, the SVM prediction function given weights or example coefficients.
- Kernels
  - Explain what the kernel trick is and why it is important for SVM efficiency and improving the kinds of functions SVMs can learn.
  - Identify the right kernel for different classification problems.
  - Tune hyperparameters for common kernels.
- Support Vectors
  - Explain what support vectors are.
  - Find the support vectors for a trained SVM.
- Soft Margin
  - Explain what is meant by soft margin.
  - Tune the `c` hyperparameter for different classification problems.
- Big Data
  - Use kernel approximation and SGD to train an SVM on big data.

# Depends On

[Model Linearity](https://github.com/thisismetis/dscurriculum_gamma/tree/master/curriculum/project-03/model-linearity)

# Instructor Notes

This is a RISE notebook. To use it, install RISE according to the instructions [here](https://github.com/damianavila/RISE#installation) and then add the Metis styling according to the instructions [here](https://github.com/thisismetis/dscurriculum_gamma#custom-notebook-styling)

This lecture series is broken up into three parts:
1. Model Linearity (previous) has been broken into a separate smaller lecture and covers the concepts of linear functions and models.
2. The main SVM lesson discusses the theory behind SVMs and how they are used.
3. The final big SVM lesson focuses on modern approaches to using SVMs on bigger data.

# Additional Resources
