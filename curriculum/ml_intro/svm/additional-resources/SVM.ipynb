{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPPORT VECTOR MACHINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pylab inline\n",
    "% config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling in our iris dataset for a quick viz\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# lets grab 1st 2 classes first,\n",
    "set0 = X[y == 0, :2]\n",
    "set1 = X[y == 1, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would the ideal classifier split the classes of this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(set0[:, 0], set0[:, 1], c='r')\n",
    "plt.scatter(set1[:, 0], set1[:, 1], c='b', alpha=.8)\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would the ideal classifier split the classes of *this* dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets grab 2nd 2 classes,\n",
    "set1 = X[y == 1, :2]\n",
    "set2 = X[y == 2, :2]\n",
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(set1[:, 0], set1[:, 1], c='b')\n",
    "plt.scatter(set2[:, 0], set2[:, 1], c='r', alpha=.8)\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets take a step back and refresh our Logistic Regression Memory\n",
    "\n",
    "Recall that with logistic regression, we are leveraging the **sigmoid function + a decision rule**\n",
    "\n",
    "Our decision rule is a simple one.    \n",
    "If p > 0.5  → predict y =1 (positive)    \n",
    "If p < 0.5  → predict y = 0 (negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets view sigmoid + decision rule !\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Quick sigmoid function w/\n",
    "    input z: linear combination of features\n",
    "    '''\n",
    "    return(1/(1+np.exp(-z)))\n",
    "x = np.linspace(-10, 10, 150)\n",
    "y = [sigmoid(s) for s in x]\n",
    "y2 = [.5 for x in x]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2, c='r')\n",
    "plt.title('sigmoid + decision rule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for viewing the decision boundary\n",
    "def plot_estimator(estimator, X, y, annot=True, color=None):\n",
    "    \"\"\"\n",
    "    create a plot to view decision boundary of classifier\n",
    "    Parameters:\n",
    "    -----------\n",
    "    esimator: classifier used to create the model\n",
    "    X: test-data, first two dimensions of which will be used for plotting\n",
    "    y: test labels\n",
    "    annot: boolean, whether to annotate observations\n",
    "    color: define cmaps color\n",
    "    \"\"\"\n",
    "    estimator.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Put the result into a color plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=color, alpha=.7)\n",
    "    plt.xlim(4.5)\n",
    "    plt.ylim(2)\n",
    "    # Lets plot our sample points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    if annot:\n",
    "        predicts = estimator.predict_proba(X)\n",
    "        for i, txt in enumerate(predicts[:, 1]):\n",
    "            plt.annotate(np.round(txt, decimals=2),\n",
    "                         (X[:, 0][i]+.02, X[:, 1][i]+.02), fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Decision Boundary \n",
    "Here we developed a LR decision boundary (w/ linearly separable data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get iris data & labels,\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# grabbing 1st two classes,\n",
    "X_sep = X[y != 2, :2]\n",
    "y_sep = iris.target[y != 2]\n",
    "# instantiate model \n",
    "lr = LogisticRegression()\n",
    "# plot the LR decision boundary\n",
    "plot_estimator(lr, X_sep, y_sep, color='Blues_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice?\n",
    "\n",
    "We notice that the further away observations are from the decision boundary, the \n",
    "more confident the LR model is what its predictions.\n",
    "\n",
    "**SVMs take this notion one step further.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What about the SVM decision boundary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop a quick model via sklearn's SVC classifier class.\n",
    "svm = LinearSVC()\n",
    "# plot the LR decision boundary\n",
    "plot_estimator(svm, X_sep, y_sep, annot=False, color='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks similiar to Logistic Regression \n",
    "\n",
    "Does SVM behave similiarly to Logistic Regresson? \n",
    "\n",
    "Well, **it depends**. \n",
    "The answer to that question is directly *hinged* to which \n",
    "*cost function* you are using (no pun intended)\n",
    "\n",
    "**And SVMs have many of them!**\n",
    "\n",
    "<img src='data/losses.png'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVMS \n",
    "## HARD MARGIN\n",
    "\n",
    "Let’s begin with the notion of linearly separable data & Hard Margin cost function.\n",
    "\n",
    "Let's start by repeating the quesion we asked earlier.    \n",
    "\n",
    "**How would the ideal classifier split the dataset below?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(set0[:, 0], set0[:, 1], c='r')\n",
    "plt.scatter(set1[:, 0], set1[:, 1], c='b', alpha=.8)\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perhaps something like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8), dpi=80)\n",
    "plt.scatter(set0[:, 0], set0[:, 1], c='r')\n",
    "plt.scatter(set1[:, 0], set1[:, 1], c='b', alpha=.8)\n",
    "plt.plot([4.65, 6.25], [2.25, 4.25])\n",
    "plt.xlabel('sepal length')\n",
    "plt.ylabel('sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed!\n",
    "\n",
    "When dealing with SVM Hard Margin approach, the correct decision \n",
    "boundary is the one that maximizes the distance between both classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we find the optimal decision boundary (aka hyperplane)?\n",
    "\n",
    "Just like with other linear models: \n",
    "we solve for the weight vector that meets our objective     \n",
    "(recall with OLS the objective was to solve for the weights that minimize residual error).  Same idea.\n",
    "\n",
    "But with SVM (hard margin) : the objective is to simultaneously maximize the margin     \n",
    "while ensuring that positive   classes & negative classes remain on opposite sides of the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='data/svm_hardmargin.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets solve for the optimal hyperplane: \n",
    "\n",
    "**Step 1) Develop constraints:**  \n",
    "Let's start by forcing a separation between the different classes \n",
    "\n",
    "$f(x) = w_1x_1  + w_2x_2 +b$\n",
    "\n",
    "f(x) >= +1 (for pos. samples)   \n",
    "f(x) <= -1 (for negative samples)  \n",
    "\n",
    "** Step 2) Compute the margin geometrically**   \n",
    "\n",
    "$$ \\vec{w}\\cdot{{x_+}} + b = +1 $$\n",
    "$$ \\vec{w}\\cdot{{x_-}} + b = -1 $$\n",
    "$$ (\\vec{w}\\cdot{{x_+}}) -(\\vec{w}\\cdot{{x_-}})= 2 $$\n",
    "\n",
    "$$width =({x}_{+}-{x}_{-})\\cdot{{\\vec{w}\\above 1pt\\|w\\|}}={{2\\above 1pt\\|w\\|}}$$\n",
    "\n",
    "$$ Maximize {{2\\above 1pt\\|w\\|}}$$\n",
    "  \n",
    "$$ Minimize  {{\\|w\\|^2\\above 1pt 2}}$$\n",
    "\n",
    "**Combining Steps 1 & 2 **:   \n",
    "Solve for w: s.t. we are simulataneously  \n",
    "minimimizing ${{\\|w\\|^2\\above 1pt 2}}$ while using **n** equations (s.t)     \n",
    "\n",
    "$f(x) = w_1x_1  + w_2x_2 +b > 1 $(for pos samples)   \n",
    "$f(x) = w_1x_1  + w_2x_2 +b < 1 $(for neg samples)\n",
    "\n",
    "This loss function is referred to the Hard Margin primal:  \n",
    "and is a hefty constrained optimization problem.\n",
    "\n",
    "So much so that sklearn does not really bother with this implementation.   \n",
    "** Let's see how we can make this loss function a bit more managable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dual Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our optimization problem above:    \n",
    "(recall from your calculus classes)   \n",
    " \n",
    "#### (Q:) What can we do when:   \n",
    "\n",
    "1) We have an equation we want to find the minima of    \n",
    "\n",
    "2) But we also have constraints we need to honor? \n",
    "\n",
    "#### (A:) We want to **use Lagrangian Multipliers! ** \n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg', width=800,height=600>\n",
    "\n",
    "### YES !!  \n",
    "\n",
    "<img src='https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/120/google/146/clapping-hands-sign_1f44f.png', width=150,height=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's use a cost function**  using equations from steps (1) & (2).    \n",
    "\n",
    "We'll plug this into the Lagrange function: \n",
    "$L(x,y,\\alpha) = f(x,y) - \\alpha(g(x,y))$\n",
    "\n",
    "$$ L=  1/2{{\\|w\\|}^2} - \\sum \\alpha_i[1- (y_i(\\vec{w}\\cdot{x_i} +b)] $$    \n",
    "\n",
    "\n",
    "As usual we want to solve for the weight parameters that will minimize our loss function.     \n",
    "\n",
    "**(Q)** How do we find the minima of the function above ? \n",
    "\n",
    "\n",
    "**(A)**  Partial Derviatives.\n",
    "\n",
    "\n",
    "###  Right Again !! \n",
    "<img src='https://emojipedia-us.s3.dualstack.us-west-1.amazonaws.com/thumbs/120/google/146/clapping-hands-sign_1f44f.png', width=200,height=200>\n",
    "\n",
    "We will take the partial derivative of the loss with respect to w (dL/dw).   \n",
    "\n",
    "We can solve for w directly:    \n",
    "\n",
    "$$\\vec{w} = \\sum \\alpha_i({x_i})$$    \n",
    "\n",
    "We can also solve for b directly    \n",
    "\n",
    "$$ \\sum \\alpha_i(y_i)=0$$\n",
    "\n",
    "We also have a constraint on alpha (s.t.) if   \n",
    "\n",
    "$ 1- y_i(\\vec{w}\\cdot{x_i} +b) <= 0: $\n",
    "then $a_i =0$ \n",
    "\n",
    "$ 1- y_i(\\vec{w}\\cdot{x_i} +b) >>  0$ , then $a_i => inf $    \n",
    "\n",
    "Intuitively, this makes sense: if we meet the required constraint of the RH term of the loss function, \n",
    "then that term of the loss is 0,   \n",
    "otherwise we have a cost to pay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COOL SVM FACT #1 \n",
    "Any observtion $x_i$ that has a non-zero alpha (or point lying on the boundary )\n",
    "is a support vector!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrapping it up:**   \n",
    "Let's plug the w & b terms into our loss function: \n",
    "\n",
    "$$ L= \\sum \\alpha_i -1/2\\sum\\sum\\alpha_i\\alpha_j(y_iy_j)(x_i\\cdot{x_j})$$\n",
    "\n",
    "1) Note that the second term of the loss function ends up being the pairwise dot product of observations.\n",
    "\n",
    "What else??\n",
    "\n",
    "2) Our Loss is only being affected by the support vector observations\n",
    "\n",
    "\n",
    "**Hold these thoughts!  They are important, and we will come back to them shortly ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We code :  Let's visualize our support vectors \n",
    "\n",
    "Let's add a bit of functionality to our existing plot_estimator function, using inspiration from sklearn:      \n",
    "http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html\n",
    "\n",
    "But let's first break down the code we already have.\n",
    "Stare at the code for a second.  \n",
    "Be ready to explain what the code is doing. \n",
    "\n",
    "```\n",
    "# lets add some code for the decision boundary and margins\n",
    "xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "Z= estimator.decision_function(xy).reshape(xx.shape)\n",
    "plt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "           \n",
    "# plot support vectors\n",
    "plt.scatter(estimator.support_vectors_[:, 0], estimator.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='r')\n",
    "           \n",
    "```\n",
    " \n",
    "What is the decision_function doing?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimator_with_margin(estimator, X, y, annot=True, color=None):\n",
    "    \"\"\"\n",
    "    create a plot to view decision boundary of classifier\n",
    "    Parameters:\n",
    "    -----------\n",
    "    esimator: classifier used to create the model\n",
    "    X: test-data, first two dimensions of which will be used for plotting\n",
    "    y: test labels\n",
    "    annot: boolean, whether to annotate observations\n",
    "    color: define cmaps color\n",
    "    \"\"\"\n",
    "    estimator.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))         \n",
    "    # Z= estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    Z = estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ZZ = estimator.decision_function(xy).reshape(xx.shape)              \n",
    "    # Put the result into a color plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=color, alpha=.7)\n",
    "    plt.xlim(4)\n",
    "    plt.ylim(2)\n",
    "    # Lets plot our sample points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    # plot decision boundary & margins\n",
    "    plt.contour(xx, yy, ZZ, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "                linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    plt.scatter(estimator.support_vectors_[:, 0],\n",
    "                estimator.support_vectors_[:, 1], s=100,\n",
    "                linewidth=1, facecolors='r')\n",
    "    if annot:\n",
    "        predicts = estimator.predict_proba(x_train)\n",
    "        for i, txt in enumerate(predicts[:, 1]):\n",
    "            plt.annotate(np.round(txt, decimals=2), (X[:, 0][i] + .02,\n",
    "                                    X[:, 1][i] + .02), fontsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or for fun we can replace estimator.predict function with\n",
    "# the estimator.decison_function for some pretty cool results!\n",
    "\n",
    "\n",
    "def plot_estimator_with_margin_gradient(estimator, X, y, annot=True,\n",
    "                                        color=None):\n",
    "    \"\"\"\n",
    "    create a plot to view decision boundary of classifier\n",
    "    Parameters:\n",
    "    -----------\n",
    "    esimator: classifier used to create the model\n",
    "    X: test-data, first two dimensions of which will be used for plotting\n",
    "    y: test labels\n",
    "    annot: boolean, whether to annotate observations\n",
    "    color: define cmaps color\n",
    "    \"\"\"\n",
    "    estimator.fit(X, y)\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                            np.linspace(y_min, y_max, 100))\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    # Z= estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Z=Z.reshape (xx.shape)\n",
    "    Z = estimator.decision_function(xy).reshape(xx.shape)\n",
    "    # Put the result into a color plot\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=color, alpha=.7)\n",
    "    plt.xlim(4.5)\n",
    "    plt.ylim(2)\n",
    "    # Lets plot our sample points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    # plot decision boundary & margins\n",
    "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "                linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    plt.scatter(estimator.support_vectors_[:, 0],\n",
    "                estimator.support_vectors_[:, 1], s=100, linewidth=1, \n",
    "                facecolors='r')\n",
    "    if annot:\n",
    "        predicts = estimator.predict_proba(x_train)\n",
    "        for i, txt in enumerate(predicts[:, 1]):\n",
    "            plt.annotate(np.round(txt, decimals=2), (X[:, 0][i] + .02, \n",
    "                            X[:, 1][i] + .02), fontsize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You Code:\n",
    "\n",
    "Using the following sklearn doc as a guide, instantiate a quick SVM model using the same Iris Data\n",
    "and:    \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html.     \n",
    "Start off with using the first (2), separable classes \n",
    "\n",
    "Visualize the model decision boundary using our function above\n",
    "\n",
    "Hint:  Use hyperparameters: kernel='Linear' & C with large value >> 10000   (this will allow us to simulate the Hard Margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOFT MARGIN  (Hinge Loss) \n",
    "\n",
    "So far we've forced a separation between positive & negative classes with our margin.  \n",
    "\n",
    "But **sometimes data cannot be linearly separated** regardless of the w & b parameters that we \n",
    "solve for. \n",
    "\n",
    "This is where the soft margin approach comes in. \n",
    "\n",
    "How will we do it? \n",
    "\n",
    "Again.  It's all about the loss function\n",
    "\n",
    "we're going to change our loss function to the following:\n",
    "\n",
    "$L= \\sum_J w^2_j +C\\sum_i max[0,1-y^i(wx^i+b)]$\n",
    "\n",
    "### YOUR TURN\n",
    "\n",
    "(1) What do you notice about the cost function? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) As C decreases, what happens to the margin? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COOL SVM FACT #2 \n",
    "\n",
    "$L= \\sum_J w^2_j +C\\sum_i max[0,1-y^i(wx^i+b)]$\n",
    "\n",
    "We said our loss function is similar to Ridge.    \n",
    "Like Ridge, we have a term that is dedicated to 'fit' (RHS) and a term that is dedicated to reducing complexity (LHS) \n",
    "\n",
    "But wait!  isn't our LHS term just maximizing the margin ? \n",
    "Are we achieving the same goal (bias / variance trade-off), or aren't we ?\n",
    "\n",
    "** We are achieving the same goal.  Note that a model that has a larger margin is comparitively more 'generalized'**\n",
    "\n",
    "\n",
    "## COOL SVM FACT #3\n",
    "Recall the logistic regression loss function: \n",
    "\n",
    "<img src='data/logistic.png'>\n",
    "\n",
    "Look familiar ? \n",
    "\n",
    "\n",
    "<img src='data/hinge_log.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Hard Margin approach, we can express the Soft Margin/ Hinge Loss function in terms of the pairwise dot products \n",
    "of the support vectors \n",
    "\n",
    "$L =\\sum\\alpha_i - 1/2\\sum\\alpha_i\\alpha_jy^iy^j(x^i*x^j)$\n",
    "max \n",
    "$0<=\\alpha<=C$\n",
    "\n",
    "**The support vectors for the Hinge loss are any data points that are on the margin or on the wrong side \n",
    "  of the margin**\n",
    "\n",
    "## You code: \n",
    "\n",
    "Visualize the Support Vectors of linearly inseparable data using the 'plot_estimator_with_margin' function.\n",
    "Use different values of C to observe how the margin & # of support vectors change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Student section\n",
    "# starting out with the non-separable classes\n",
    "X_nonsep = X[y != 0, :2]\n",
    "y_nonsep = iris.target[y != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear SVM \n",
    "### The Kernel Trick  \n",
    "\n",
    "Sometimes we are dealing with data that is truly in seperable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n=300\n",
    "# sklearn function for making circles! \n",
    "X, Y = make_circles(n_samples=n, noise=0.07, factor=0.4)\n",
    "X0 = X[Y == 0]\n",
    "X1 = X[Y == 1]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter([a[0] for a in X0], [a[1] for a in X0])\n",
    "plt.scatter([b[0] for b in X1], [b[1] for b in X1], c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we separate this data linearly ??  \n",
    "\n",
    "We dont!    \n",
    "\n",
    "However, if we are stuck in a space, we can simply switch to a new dimensional space  .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(X):\n",
    "    '''\n",
    "    Quick function to generate radial coordinate\n",
    "    '''\n",
    "    return [np.sqrt((x[0])**2 + x[1]**2) for x in X]\n",
    "\n",
    "\n",
    "# create new dimension : polar coordinate!\n",
    "Z = np.array(kernel(X))\n",
    "\n",
    "# separate classes\n",
    "Z0 = Z[Y == 0]\n",
    "Z1 = Z[Y == 1]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter([a[0] for a in X0], [a[1] for a in X0], Z0)\n",
    "ax.scatter([a[0] for a in X1], [a[1] for a in X1], Z1, c='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool SVM FACT #4\n",
    "Remember we said that our optimization of the loss depended only on dot products?  \n",
    "These dot products can be replaced with a kernel function which compute a dot product in \n",
    "some higher dimensional space \n",
    "\n",
    "\n",
    "$L =\\sum\\alpha_i - 1/2\\sum\\alpha_i\\alpha_jy^iy^j(\\phi(x^i)*\\phi(x^j))$\n",
    "\n",
    "\n",
    "Let's get hands on practice with Kernels via sklearn! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, a quick note about sklearn kernel options: \n",
    "\n",
    "\n",
    "#1) linear : (no kernel trick)\n",
    "#2) poly kernel: we're increasing dimensionality (think PolynomialFeatures for support vectors) \n",
    "#2) [rbf](https://en.wikipedia.org/wiki/Radial_basis_function_kernel) : \n",
    "increases dimensionality that much more, and thus can allow for a much more complex model \n",
    "than the poly or linear kernels.  So much so that [SVM + rbf kernel is considered a non-parametric model](https://sebastianraschka.com/faq/docs/parametric_vs_nonparametric.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Code:\n",
    "\n",
    "Create a few models & visualize the difference with the plot_estimator function with the inseparable classes  \n",
    "If you have time use sklearn's GridSearch to determine which model provides the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn : \n",
    "    \n",
    "1) How do you think SVM handles outliers ? \n",
    "\n",
    "2) How do you think that SVM handles multi-class ? \n",
    "\n",
    "3) Do we have to standardize the features prior to training? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train / Test Complexity \n",
    "\n",
    "Perhaps most important of all are the details that surround complexity. \n",
    "\n",
    "**Train Complexity**     \n",
    "If you take a look at the sklearn [SVC doc](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "One of the first lines of text is :     \n",
    "*The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.*\n",
    "\n",
    "This is important to note & valid, and frankly can make many shy away from SVMs.\n",
    "\n",
    "\n",
    "**But before we shy away, let us consider:**    \n",
    "**Test Complexity** \n",
    "\n",
    "\n",
    "*What is the complexity of SVM testing?*  \n",
    "\n",
    "Thats right! Its quadratic with the number of support vectors.  This is **not bad at all** for a non-parametric model (compare this with KNN ! ) \n",
    "\n",
    "In the real world, we generally tend to worry more about test complexity than train complexity.   Why ??       \n",
    "In production, we could run the model for predictions many, many times a day whereas we might only need to retrain the model every few weeks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
